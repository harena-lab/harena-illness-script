{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../annotations')\n",
    "import annotation_metrics as am\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE, Isomap, MDS\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "dimensions = ['pathophysiology', 'epidemiology', 'etiology', 'history', 'physical', 'exams', 'differential', 'therapeutic']\n",
    "\n",
    "dimensions_mean = [dim + '_mean' for dim in dimensions]\n",
    "\n",
    "metrics = [dim + '_norm_mean' for dim in dimensions]\n",
    "\n",
    "cluster_methods = [\n",
    "    'kmeans 3 emb', 'kmeans 3 cat',\n",
    "    'agg 3 emb', 'agg 3 cat',\n",
    "    'gmm 3 emb', 'gmm 3 cat',\n",
    "    'birch 3 emb', 'birch 3 cat'\n",
    "]\n",
    "\n",
    "cluster_names = [\"Novice\", \"Developing\", \"Proficient\",\n",
    "                 \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\", \"20\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_sets = ['Human_84', 'Human_435', 'BioBERTpt', 'BioBERT_Llama', 'Llama']\n",
    "\n",
    "metrics_all = {'Human_84': pd.read_csv('medical_specialist/annotations-dpoc-medical_specialist_metrics_84.csv'),\n",
    "               'Human_435': pd.read_csv('medical_specialist/aligned_annotations-dpoc-medical_specialist_metrics_435.csv'),\n",
    "               'BioBERTpt': pd.read_csv('biobert_balanced/annotations-dpoc-biobert_metrics.csv'),\n",
    "               'BioBERT_Llama': pd.read_csv('biobert-llama_balanced/annotations-dpoc-biobert-llama_metrics.csv'),\n",
    "               'Llama': pd.read_csv('llama/annotations-dpoc-llm_3_static_shot_metrics.csv')}\n",
    "\n",
    "for metric_label in metrics_all:\n",
    "    metrics_all[metric_label] = metrics_all[metric_label].sort_values('annotation id')\n",
    "\n",
    "common_ids_435 = metrics_all['Human_435']['annotation id'].isin(metrics_all['Llama']['annotation id'])\n",
    "metrics_all['Human_435'] = metrics_all['Human_435'][common_ids_435]\n",
    "metrics_all['Llama'] = metrics_all['Llama'][common_ids_435]\n",
    "\n",
    "common_ids_86 = metrics_all['Human_84']['annotation id'].isin(metrics_all['BioBERTpt']['annotation id'])\n",
    "metrics_all['Human_84'] = metrics_all['Human_84'][common_ids_86]\n",
    "metrics_all['BioBERTpt'] = metrics_all['BioBERTpt'][common_ids_86]\n",
    "metrics_all['BioBERT_Llama'] = metrics_all['BioBERT_Llama'][common_ids_86]\n",
    "\n",
    "stats_all = {'Human_84': pd.read_csv('medical_specialist/annotations-dpoc-medical_specialist_stats_84.csv'),\n",
    "             'Human_435': pd.read_csv('medical_specialist/annotations-dpoc-medical_specialist_stats_435.csv'),\n",
    "             'BioBERTpt': pd.read_csv('biobert_balanced/annotations-dpoc-biobert_stats.csv'),\n",
    "             'BioBERT_Llama': pd.read_csv('biobert-llama_balanced/annotations-dpoc-biobert-llama_stats.csv'),\n",
    "             'Llama': pd.read_csv('llama/annotations-dpoc-llm_3_static_shot_stats.csv')}\n",
    "\n",
    "reduction_method = ['tsne', 'pca', 'isomap', 'mds']\n",
    "reduction_all = {}\n",
    "for ann_set in annotation_sets:\n",
    "    tsne = TSNE(n_components=2, random_state=42, init='pca')\n",
    "    pca = PCA(n_components=2)\n",
    "    isomap = Isomap(n_components=2)\n",
    "    mds = MDS(n_components=2, random_state=42)\n",
    "    reduction_all[ann_set] = {\n",
    "        'tsne': tsne.fit_transform(metrics_all[ann_set][am.cluster_labels].values),\n",
    "        'pca': pca.fit_transform(metrics_all[ann_set][am.cluster_labels].values),\n",
    "        'isomap': isomap.fit_transform(metrics_all[ann_set][am.cluster_labels].values),\n",
    "        'mds': mds.fit_transform(metrics_all[ann_set][am.cluster_labels].values)\n",
    "    }\n",
    "\n",
    "comparison = [['Human_435', 'Llama'], ['Human_84', 'BioBERTpt'], ['Human_84', 'BioBERT_Llama']]\n",
    "\n",
    "def create_radar_charts(method_data, method, stats_label, show_scores=True):\n",
    "    stats_label = stats_label.replace('_435', '').replace('_84', '').replace('BioBERT_Llama', 'BioBERT-Llama')\n",
    "    \"\"\"Create radar charts from DataFrame\"\"\"\n",
    "    metrics = ['pathophysiology_norm_mean', 'epidemiology_norm_mean', 'etiology_norm_mean', 'history_norm_mean',\n",
    "               'physical_norm_mean', 'exams_norm_mean', 'differential_norm_mean', 'therapeutic_norm_mean']\n",
    "    \n",
    "    clusters = method_data['cluster'].unique()\n",
    "    \n",
    "    # Calculate subplot layout\n",
    "    n_clusters = len(clusters)\n",
    "    n_rows = (n_clusters + 2) // 3  # Max 3 plots per row\n",
    "    n_cols = min(n_clusters, 3)\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, axs = plt.subplots(\n",
    "        n_rows, n_cols, \n",
    "        figsize=(5*n_cols, 5*n_rows), \n",
    "        subplot_kw={'projection': 'polar'}\n",
    "    )\n",
    "    \n",
    "    # Flatten axs for easier indexing if multiple rows\n",
    "    if n_clusters > 1:\n",
    "        axs = axs.flatten() if n_rows > 1 else axs\n",
    "    \n",
    "    # Color map for different clusters\n",
    "    colors = plt.cm.rainbow(np.linspace(0, 1, len(clusters)))\n",
    "    \n",
    "    # Plot data for each cluster\n",
    "    for i, (cluster, color) in enumerate(zip(clusters, colors)):\n",
    "        # Handle subplot indexing\n",
    "        ax = axs[i] if n_clusters > 1 else axs\n",
    "        \n",
    "        cluster_data = method_data[method_data['cluster'] == cluster]\n",
    "        values = cluster_data[metrics].values.flatten()\n",
    "        \n",
    "        # Compute angles for metrics\n",
    "        theta = np.linspace(0, 2*np.pi, len(metrics), endpoint=False)\n",
    "        \n",
    "        # Close the plot by repeating the first value\n",
    "        values = np.concatenate((values, [values[0]]))\n",
    "        theta = np.concatenate((theta, [theta[0]]))\n",
    "        \n",
    "        # Plot the radar chart\n",
    "        ax.plot(theta, values, color=color)\n",
    "        ax.fill(theta, values, color=color, alpha=0.25)\n",
    "        \n",
    "        # Set labels\n",
    "        ax.set_xticks(theta[:-1])\n",
    "        ax.set_xticklabels([\n",
    "            metric.replace('_norm_mean', '').replace('_', ' ').title() \n",
    "            for metric in metrics\n",
    "        ])\n",
    "        ax.set_ylim(0, 0.60)\n",
    "\n",
    "        cn = cluster_names[cluster] if show_scores else cluster\n",
    "        title = f'{cn} Cluster\\nannotations {cluster_data[\"annotations mean\"].values[0]:.1f} avg'\n",
    "        \n",
    "        if show_scores:\n",
    "            # Set title with cluster and year\n",
    "            ots_min = cluster_data['objective test score_min'].values[0]\n",
    "            ots_max = cluster_data['objective test score_max'].values[0]\n",
    "            ots_mean = cluster_data['objective test score_mean'].values[0]\n",
    "            ol_min = cluster_data['organization level_min'].values[0]\n",
    "            ol_max = cluster_data['organization level_max'].values[0]\n",
    "            ol_mean = cluster_data['organization level_mean'].values[0]\n",
    "            gs_min = cluster_data['global score_min'].values[0]\n",
    "            gs_max = cluster_data['global score_max'].values[0]\n",
    "            gs_mean = cluster_data['global score_mean'].values[0]\n",
    "\n",
    "            title += (f'\\nobjective test {ots_min}-{ots_max} / {ots_mean:.1f} avg\\n' +\n",
    "                      f'organization level {ol_min}-{ol_max} / {ol_mean:.1f} avg\\n' +\n",
    "                      f'global score {gs_min}-{gs_max} / {gs_mean:.1f} avg')\n",
    "            \n",
    "        ax.set_title(title)\n",
    "    \n",
    "    # Remove extra subplots if any\n",
    "    if n_clusters < len(axs.flatten()):\n",
    "        for j in range(n_clusters, len(axs.flatten())):\n",
    "            fig.delaxes(axs.flatten()[j])\n",
    "    \n",
    "    # Overall figure title\n",
    "    fig.suptitle(f'{stats_label}', fontsize=16, y=1.05)\n",
    "    \n",
    "    # Adjust layout and display\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for comparison_pair in comparison:\n",
    "\n",
    "    method = 'kmeans 3 cat'\n",
    "    # method = 'gmm 3 emb'\n",
    "    # Heatmap\n",
    "    # -------\n",
    "\n",
    "    column_name = f'cluster {method}'\n",
    "\n",
    "    cluster1_labels = metrics_all[comparison_pair[0]][column_name].values\n",
    "    cluster2_labels = metrics_all[comparison_pair[1]][column_name].values\n",
    "\n",
    "    # Create contingency matrix\n",
    "    contingency = confusion_matrix(cluster1_labels, cluster2_labels)\n",
    "\n",
    "    # Calculate proportion matrix\n",
    "    row_sums = contingency.sum(axis=1).reshape(-1, 1)\n",
    "    proportion_matrix = contingency / row_sums\n",
    "\n",
    "    # Plot contingency matrix heatmap\n",
    "    plt.subplot(212)\n",
    "    # Increase figure size to reduce overlap\n",
    "    plt.gcf().set_size_inches(9, 4.5)\n",
    "    sns.heatmap(proportion_matrix, annot=True, fmt='.2f', cmap='YlOrRd',\n",
    "                xticklabels=[f'{comparison_pair[1].replace(\"BioBERT_Llama\",\"BioBERT-Llama\")} {cluster_names[i]}' for i in range(len(np.unique(cluster2_labels)))],\n",
    "                yticklabels=[f'{comparison_pair[0].replace(\"_435\",\"\").replace(\"_84\",\"\").replace(\"_86\",\"\")} {cluster_names[i]}' for i in range(len(np.unique(cluster1_labels)))],\n",
    "                vmin=0, vmax=1)\n",
    "    plt.title(f'{comparison_pair[0].replace(\"_435\",\"\").replace(\"_84\",\"\").replace(\"_86\",\"\")} vs {comparison_pair[1].replace(\"BioBERT_Llama\",\"BioBERT-Llama\")}')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Radar Plot\n",
    "    # ----------\n",
    "\n",
    "    for stats_label in comparison_pair:\n",
    "        stats = stats_all[stats_label]\n",
    "        stats['annotations mean'] = stats[dimensions_mean].sum(axis=1)\n",
    "        method_data = stats[stats['method'] == method]\n",
    "        create_radar_charts(method_data, method, stats_label)\n",
    "\n",
    "        # Bar Chart\n",
    "        # ---------\n",
    "\n",
    "        # Extract year columns\n",
    "        year_columns = ['year_1', 'year_2', 'year_3', 'year_4', 'year_5', 'year_6']\n",
    "\n",
    "        # Plot bar chart for each cluster\n",
    "        clusters = method_data['cluster'].unique()\n",
    "        n_clusters = len(clusters)\n",
    "        print(clusters)\n",
    "        fig, axs = plt.subplots(1, n_clusters, figsize=(5 * n_clusters, 6), sharey=True)\n",
    "        \n",
    "        if n_clusters == 1:\n",
    "            axs = [axs]\n",
    "        year_cluster_normalized = {0:{1: 0, 2: 0, 3: 0, 4: 0, 5:0, 6:0},1:{1: 0, 2: 0, 3: 0, 4: 0, 5:0, 6:0},2:{1: 0, 2: 0, 3: 0, 4: 0, 5:0, 6:0}}\n",
    "        year_count = {1: 0, 2: 0, 3: 0, 4: 0, 5:0, 6:0}\n",
    "        for cluster in clusters:\n",
    "            cluster_data = method_data[method_data['cluster'] == cluster]\n",
    "            for year in range(1, 7):\n",
    "                year_count[year] += cluster_data[f'year_{year}'].sum()\n",
    "        for cluster in clusters:\n",
    "            for year in range(1, 7):\n",
    "                year_cluster_normalized[cluster][year] = method_data[method_data['cluster'] == cluster][f'year_{year}'].sum() / year_count[year]\n",
    "                print('attempting to normalize', cluster, year, year_count[year],method_data[method_data['cluster'] == cluster][f'year_{year}'].sum(), year_cluster_normalized[cluster][year])\n",
    "        for ax, cluster in zip(axs, clusters):\n",
    "            cluster_data = method_data[method_data['cluster'] == cluster]\n",
    "            ax.bar(year_columns, cluster_data[year_columns].values.flatten())\n",
    "            ax.set_title(f'{cluster_names[cluster]} Cluster')\n",
    "            ax.set_xlabel('Students per Year')\n",
    "            ax.set_ylabel('Values')\n",
    "            ax.legend()\n",
    "            for i, value in enumerate(cluster_data[year_columns].values.flatten()):\n",
    "                ax.text(i, value, str(int(value)), ha='center', va='bottom')\n",
    "\n",
    "        # Display the bar chart\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # Select elements labeled as 0 in cluster 1 that were labeled as 1 in cluster 2\n",
    "    intersect = [[0, 1], [1, 0]]\n",
    "    for cp in comparison_pair:\n",
    "        mean_dimensions_all = pd.DataFrame()\n",
    "        for inter in intersect:\n",
    "            selected_elements = metrics_all[cp][(cluster1_labels == inter[0]) & (cluster2_labels == inter[1])]\n",
    "            mean_dimensions = selected_elements[dimensions].mean()\n",
    "            annotations_mean = mean_dimensions.sum()\n",
    "            mean_dimensions = mean_dimensions / annotations_mean\n",
    "            mean_dimensions.index = [f'{dim}_norm_mean' for dim in mean_dimensions.index]\n",
    "            mean_dimensions['annotations mean'] = annotations_mean\n",
    "            mean_dimensions['cluster'] = f'{cluster_names[inter[0]]}/{cluster_names[inter[1]]}'\n",
    "            mean_dimensions_all = pd.concat([mean_dimensions_all, mean_dimensions.to_frame().T], ignore_index=True)\n",
    "        create_radar_charts(mean_dimensions_all, method, cp, False)\n",
    "\n",
    "    # Scatter Plot\n",
    "    # ------------\n",
    "\n",
    "    fig, axs = plt.subplots(4, 4, figsize=(20, 16))\n",
    "    axs = axs.flatten()\n",
    "\n",
    "    pos = 0\n",
    "\n",
    "    combination = [[0, 0], [1,1], [0,1], [1,0]]\n",
    "    for comb in combination:\n",
    "        for rm in reduction_method:\n",
    "            scatter = axs[pos].scatter(\n",
    "                reduction_all[comparison_pair[comb[1]]][rm][:, 0],\n",
    "                reduction_all[comparison_pair[comb[1]]][rm][:, 1],\n",
    "                c=metrics_all[comparison_pair[comb[0]]][column_name].values, cmap='viridis', alpha=0.6)\n",
    "            fig.colorbar(scatter, ax=axs[pos], label='Cluster')\n",
    "            axs[pos].set_title(\n",
    "                f'{comparison_pair[comb[0]]} ann/{comparison_pair[comb[1]]}')\n",
    "            axs[pos].set_xlabel(f'{rm} 1')\n",
    "            axs[pos].set_ylabel(f'{rm} 2')\n",
    "            pos += 1\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
