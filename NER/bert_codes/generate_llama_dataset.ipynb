{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir(os.path.dirname(os.path.abspath(__file__)) if '__file__' in globals() else os.getcwd())\n",
    "\n",
    "df = pd.read_csv('llama_raw_data_info_with_metrics.csv')\n",
    "df = df[['doc_id', 'text','augmented_annotation','labels']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def process_text_to_labels(text):\n",
    "    \"\"\"\n",
    "    Process input text with bracketed annotations and return BIO labels.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Input text with annotations in format '[text | label1, label2]'\n",
    "        \n",
    "    Returns:\n",
    "        list: List of BIO labels corresponding to each token in the text\n",
    "    \"\"\"\n",
    "\n",
    "    def tokenize(text):\n",
    "        \"\"\"\n",
    "        Split text into tokens using multiple delimiters\n",
    "        \"\"\"\n",
    "        # First, add spaces around delimiters\n",
    "        delimiters = r'[.,;:\\(\\)]'\n",
    "        text = re.sub(f'({delimiters})', r' \\1 ', text)\n",
    "        # Split by whitespace and filter out empty strings\n",
    "        return [token for token in text.split() if token]\n",
    "\n",
    "    # Initialize variables\n",
    "    labels = []\n",
    "    current_position = 0\n",
    "    text_length = len(text)\n",
    "    \n",
    "    while current_position < text_length:\n",
    "        # Handle bracketed sections\n",
    "        if text[current_position] == '[':\n",
    "            # Find closing bracket and separator\n",
    "            closing_bracket = text.find(']', current_position)\n",
    "            separator = text.find('|', current_position)\n",
    "            \n",
    "            # Extract the text and its labels\n",
    "            phrase = text[current_position + 1:separator].strip()\n",
    "            label_part = text[separator + 1:closing_bracket].strip()\n",
    "            labels_list = [label.strip() for label in label_part.split(',')]\n",
    "            \n",
    "            # Split the phrase into tokens\n",
    "            tokens = tokenize(phrase)\n",
    "\n",
    "            # for token in tokens:\n",
    "            #     labels.append(token)\n",
    "            \n",
    "            # Assign BIO labels to each token\n",
    "\n",
    "            for i, token in enumerate(tokens):\n",
    "                if i == 0:  # First token gets B- prefix\n",
    "                    if len(labels_list) == 1:\n",
    "                        labels.append(f\"B-{labels_list[0]}\")\n",
    "                    else:\n",
    "                        labels.append(f\"B-{'-'.join(labels_list)}\")\n",
    "                else:  # Subsequent tokens get I- prefix\n",
    "                    if len(labels_list) == 1:\n",
    "                        labels.append(f\"I-{labels_list[0]}\")\n",
    "                    else:\n",
    "                        labels.append(f\"I-{'-'.join(labels_list)}\")\n",
    "            \n",
    "            # Move position to after closing bracket\n",
    "            current_position = closing_bracket + 1\n",
    "            \n",
    "        else:\n",
    "            # Handle non-bracketed text\n",
    "            # Find next opening bracket or end of string\n",
    "            next_bracket = text.find('[', current_position)\n",
    "            if next_bracket == -1:\n",
    "                next_bracket = text_length\n",
    "            \n",
    "            # Split the text into tokens and assign O labels\n",
    "            plain_text = text[current_position:next_bracket]\n",
    "            tokens = [token for token in tokenize(plain_text) if token]\n",
    "            # for token in tokens:\n",
    "            #     labels.append(token)\n",
    "            labels.extend(['O'] * len(tokens))\n",
    "            \n",
    "            current_position = next_bracket\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def labels_to_tokens(string):\n",
    "    data = ast.literal_eval(string)\n",
    "    tokens = [ token[0] for token in data]\n",
    "    \n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['labels'].apply(labels_to_tokens)\n",
    "df['tags'] = df['augmented_annotation'].apply(process_text_to_labels)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(df['labels'][0])\n",
    "print(df['tokens'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('llama_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base2(n):\n",
    "    return 2**n\n",
    "\n",
    "tags = {'O': 0,\n",
    "            'B-pathophysiology': base2(0), \n",
    "            \"I-pathophysiology\": base2(1), \n",
    "            \"B-epidemiology\": base2(2), \n",
    "            \"I-epidemiology\": base2(3),\n",
    "            \"B-etiology\": base2(4),\n",
    "            \"I-etiology\": base2(5),\n",
    "            \"B-history\": base2(6),\n",
    "            \"I-history\": base2(7),\n",
    "            \"B-physical\": base2(8),\n",
    "            \"I-physical\": base2(9),\n",
    "            \"B-exams\": base2(10),\n",
    "            \"I-exams\": base2(11),\n",
    "            \"B-differential\": base2(12),\n",
    "            \"I-differential\": base2(13),\n",
    "            \"B-therapeutic\": base2(14),\n",
    "            \"I-therapeutic\": base2(15)\n",
    "           }\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags_aux = {'O': 0}\n",
    "\n",
    "cats = [\"Pathophysiology\", \n",
    "        \"Epidemiology\", \n",
    "        \"Etiology\", \n",
    "        \"History\", \n",
    "        \"Physical_examination\", \n",
    "        \"Complementary_exams\",\n",
    "        \"Differential_diagnosis\",\n",
    "        \"Therapeutic_plan\"\n",
    "       ]\n",
    "\n",
    "for i in range(len(cats)):\n",
    "    ner_tags_aux[\"B-\" + cats[i]] = base2(2*i)\n",
    "    ner_tags_aux[\"I-\" + cats[i]] = base2(2*i+1)\n",
    "    for j in range(i+1, len(cats)):\n",
    "        ner_tags_aux[\"B-\" + cats[i] + '-' + cats[j]] = base2(2*i) + base2(2*j)\n",
    "        ner_tags_aux[\"I-\" + cats[i] + '-' + cats[j]] = base2(2*i+1) + base2(2*j+1)\n",
    "        for k in range(j+1, len(cats)):\n",
    "            ner_tags_aux[\"B-\" + cats[i] + '-' + cats[j] + '-' + cats[k]] = base2(2*i) + base2(2*j) + base2(2*k)\n",
    "            ner_tags_aux[\"I-\" + cats[i] + '-' + cats[j] + '-' + cats[k]] = base2(2*i+1) + base2(2*j+1) + base2(2*k+1)\n",
    "            \n",
    "ner_tags_aux['B-Pathophysiology-Epidemiology-Etiology-History'] = 85\n",
    "ner_tags_aux['I-Pathophysiology-Epidemiology-Etiology-History'] = 170\n",
    "\n",
    "tags_ner = [k for k, v in ner_tags_aux.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_ner_aux = {value: key for key, value in ner_tags_aux.items()}\n",
    "ner_tags = {key: index for index, key in enumerate(ner_tags_aux.keys())}\n",
    "ner_tags_inverted = {value: key for key, value in ner_tags.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags_inverted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_to_id(string):\n",
    "    if string == 'O':\n",
    "        return 0\n",
    "    \n",
    "    words = string.split('-')\n",
    "    start = words[0]\n",
    "    labels = words[1:]\n",
    "    valor = 0\n",
    "    for label in labels:\n",
    "            valor += tags[start+\"-\"+label]\n",
    "    valor = ner_tags[tags_ner_aux[valor]]\n",
    "    return valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ner_ids'] = df['tags'].apply(lambda x: [labels_to_id(label) for label in x])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['check'] = df.apply(lambda row: [len(row['tokens']) == len(row['tags'])], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['check'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['augmented_annotation','check', 'labels', 'tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('teste.csv')\n",
    "treino = pd.read_csv('treino.csv')\n",
    "validacao = pd.read_csv('validacao.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = df[df['doc_id'].isin(test['doc_id'])]\n",
    "train_df = df[df['doc_id'].isin(treino['doc_id'])]\n",
    "validation_df = df[df['doc_id'].isin(validacao['doc_id'])]\n",
    "\n",
    "print(\n",
    "    f\"Train shape: {train_df.shape}\\n\"\n",
    "    f\"Test shape: {test_df.shape}\\n\"\n",
    "    f\"Validation shape: {validation_df.shape}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict, ClassLabel, Sequence\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "validation_dataset = Dataset.from_pandas(validation_df)\n",
    "test_dataset = Dataset.from_pandas(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_tags_aux = {'O': 0}\n",
    "\n",
    "cats = [\"Pathophysiology\", \n",
    "        \"Epidemiology\", \n",
    "        \"Etiology\", \n",
    "        \"History\", \n",
    "        \"Physical_examination\", \n",
    "        \"Complementary_exams\",\n",
    "        \"Differential_diagnosis\",\n",
    "        \"Therapeutic_plan\"\n",
    "       ]\n",
    "\n",
    "for i in range(len(cats)):\n",
    "    ner_tags_aux[\"B-\" + cats[i]] = base2(2*i)\n",
    "    ner_tags_aux[\"I-\" + cats[i]] = base2(2*i+1)\n",
    "    for j in range(i+1, len(cats)):\n",
    "        ner_tags_aux[\"B-\" + cats[i] + '-' + cats[j]] = base2(2*i) + base2(2*j)\n",
    "        ner_tags_aux[\"I-\" + cats[i] + '-' + cats[j]] = base2(2*i+1) + base2(2*j+1)\n",
    "        for k in range(j+1, len(cats)):\n",
    "            ner_tags_aux[\"B-\" + cats[i] + '-' + cats[j] + '-' + cats[k]] = base2(2*i) + base2(2*j) + base2(2*k)\n",
    "            ner_tags_aux[\"I-\" + cats[i] + '-' + cats[j] + '-' + cats[k]] = base2(2*i+1) + base2(2*j+1) + base2(2*k+1)\n",
    "            \n",
    "ner_tags_aux['B-Pathophysiology-Epidemiology-Etiology-History'] = 85\n",
    "ner_tags_aux['I-Pathophysiology-Epidemiology-Etiology-History'] = 170\n",
    "\n",
    "tags_ner = [k for k, v in ner_tags_aux.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_class_labels = ClassLabel(num_classes = len(tags_ner),names=tags_ner)\n",
    "\n",
    "train_dataset = train_dataset.cast_column(\"ner_ids\", Sequence(ner_class_labels))\n",
    "validation_dataset = validation_dataset.cast_column(\"ner_ids\", Sequence(ner_class_labels))\n",
    "test_dataset = test_dataset.cast_column(\"ner_ids\", Sequence(ner_class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset,\n",
    "    'test': test_dataset})\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "# Assuming `dataset` is your DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": dataset[\"train\"].remove_columns([\"__index_level_0__\"]),\n",
    "    \"validation\": dataset[\"validation\"].remove_columns([\"__index_level_0__\"]),\n",
    "    \"test\": dataset[\"test\"].remove_columns([\"__index_level_0__\"]),\n",
    "})\n",
    "\n",
    "# Check the modified dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.push_to_hub(\"harena-lab/bioberpt-llama-dpoc-is-multiple\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
