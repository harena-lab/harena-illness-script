{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "biobert = pd.read_csv('annotations-dpoc-biobert_groups_raw.csv')\n",
    "llama = pd.read_csv('annotations-dpoc-biobert-llama_groups_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biobert.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "\n",
    "def replace_tokens_with_entities(text, tokens, annotations):\n",
    "    \"\"\"\n",
    "    Replace tokens with their corresponding entity groups based on annotations.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Original text\n",
    "        tokens (list): List of tokens\n",
    "        annotations (list): List of annotation dictionaries\n",
    "\n",
    "    Returns:\n",
    "        list: List of tokens with entities replaced by their entity groups\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a mapping of character positions to entity groups\n",
    "    entity_map = {}\n",
    "    for annotation in annotations:\n",
    "        start = annotation['start']\n",
    "        end = annotation['end']\n",
    "        entity_group = annotation['entity_group']\n",
    "        \n",
    "        # Mark all character positions in this range with the entity group\n",
    "        for pos in range(start, end):\n",
    "            entity_map[pos] = entity_group\n",
    "    \n",
    "    # Track current position in the text\n",
    "    current_pos = 0\n",
    "    result_tokens = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Skip whitespace and newlines when matching\n",
    "        while current_pos < len(text) and text[current_pos] in ' \\n\\t':\n",
    "            current_pos += 1\n",
    "        \n",
    "        # Find the token in the text starting from current position\n",
    "        token_start = current_pos\n",
    "        token_end = current_pos + len(token)\n",
    "        \n",
    "        # Check if this token position overlaps with any entity\n",
    "        entity_found = None\n",
    "        for pos in range(token_start, min(token_end, len(text))):\n",
    "            if pos in entity_map:\n",
    "                entity_found = entity_map[pos]\n",
    "                break\n",
    "        \n",
    "        # If we found an entity, replace the token with the entity group\n",
    "        if entity_found is not None:\n",
    "            result_tokens.append(entity_found)\n",
    "        else:\n",
    "            result_tokens.append(token)\n",
    "\n",
    "        # Move position forward\n",
    "        current_pos = token_end\n",
    "    \n",
    "    return result_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decimal_to_bool(number):\n",
    "    if number != 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def decimal_to_binary_16(number):\n",
    "    binary = bin(number)[2:].zfill(16)\n",
    "    binary_list = [int(bit) for bit in binary]\n",
    "    binary_list.reverse()\n",
    "    return binary_list\n",
    "\n",
    "def binary_16_to_8_binary(bits_16):\n",
    "    lista_aux = []\n",
    "    for i in range(1, len(bits_16), 2):\n",
    "        if bits_16[i] == 1 or bits_16[i - 1] == 1:\n",
    "            lista_aux.append(1)\n",
    "        else:\n",
    "            lista_aux.append(0)  \n",
    "    return lista_aux\n",
    "\n",
    "def decimal_to_binary(n):\n",
    "\n",
    "    return binary_16_to_8_binary(decimal_to_binary_16(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels of a annotation to binary vector\n",
    "cats = {'Pathophysiology':0,\n",
    "        'Etiology':1,\n",
    "        'Epidemiology':2,\n",
    "        'History':3,\n",
    "        'Physical_examination':4,\n",
    "        'Complementary_exams':5,\n",
    "        'Differential_diagnosis':6,\n",
    "        'Therapeutic_plan':7}\n",
    "\n",
    "def label2binary(labels):\n",
    "    parts = labels.split('-')\n",
    "    vet = [0] * 8\n",
    "    for label in parts:\n",
    "        if label in cats:\n",
    "            vet[cats[label]] = 1\n",
    "    return vet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the text - divided in tokens - labels to binary vectors\n",
    "def preprocessClassification(tokens):\n",
    "    aux = []\n",
    "    for index in range(len(tokens)):\n",
    "        aux.append(label2binary(tokens[index]))\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the text - divided in tokens - labels to boolean\n",
    "def preprocessAnotation(tokens):\n",
    "    aux = []\n",
    "    for index in range(len(tokens)):\n",
    "        aux.append(1 if sum(label2binary(tokens[index])) >= 1 else 0)\n",
    "    return aux"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def add_predictions(df):\n",
    "\n",
    "    df_output = df.copy()[[\"annotation id\",\"text\"]]\n",
    "    annotations = []\n",
    "    annotations_by_label = []\n",
    "    annotations_prediction = []\n",
    "    annotations_prediction_by_label = []\n",
    "\n",
    "    for index in tqdm(range(len(df))):\n",
    "\n",
    "        ner_ids = ast.literal_eval(df['ner_ids'][index])\n",
    "        tokens = ast.literal_eval(df['tokens'][index])\n",
    "        annotations_str = ast.literal_eval(df['annotations'][index])\n",
    "\n",
    "        ## Converter ner_ids to binary\n",
    "        annotations.append([decimal_to_bool(x) for x in ner_ids])\n",
    "        annotations_by_label.append([decimal_to_binary(x) for x in ner_ids])\n",
    "\n",
    "        ## Create column for prediction\n",
    "        prediction = replace_tokens_with_entities(df['text'][index], tokens, annotations_str)\n",
    "        annotations_prediction.append(preprocessAnotation(prediction))\n",
    "        annotations_prediction_by_label.append(preprocessClassification(prediction))\n",
    "\n",
    "        pass\n",
    "\n",
    "    df_output['real annotation'] = annotations\n",
    "    df_output['real annotation by label'] = annotations_by_label\n",
    "    df_output['prediction annotation'] = annotations_prediction\n",
    "    df_output['prediction annotation by label'] = annotations_prediction_by_label\n",
    "\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biobert_output = add_predictions(biobert)\n",
    "llama_output = add_predictions(llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biobert_output.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "\n",
    "def metrics(df):\n",
    "\n",
    "    df_test = df.copy()\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    bad_data = []\n",
    "\n",
    "    df_test = df.copy()\n",
    "\n",
    "    for index in tqdm(range(len(df_test))):\n",
    "        if index not in bad_data:\n",
    "            \n",
    "            precision_now = precision_score(df_test[\"real annotation\"][index], df_test[\"prediction annotation\"][index],average='binary')\n",
    "            precision.append([precision_now,len(df_test[\"real annotation\"][index])])\n",
    "\n",
    "            recall_now = recall_score(df_test[\"real annotation\"][index], df_test[\"prediction annotation\"][index],average='binary')\n",
    "            recall.append([recall_now,len(df_test[\"real annotation\"][index])])\n",
    "\n",
    "            f1_now = f1_score(df_test[\"real annotation\"][index], df_test[\"prediction annotation\"][index],average='binary')\n",
    "            f1.append([f1_now,len(df_test[\"real annotation\"][index])])\n",
    "\n",
    "        else:\n",
    "            precision.append(np.nan)\n",
    "            recall.append(np.nan)\n",
    "            f1.append(np.nan)\n",
    "            \n",
    "    df_test[\"Precision\"] = precision\n",
    "    df_test[\"Recall\"] = recall\n",
    "    df_test[\"F1\"] = f1\n",
    "\n",
    "    return df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biobert_output = metrics(biobert_output)\n",
    "llama_output = metrics(llama_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "count = 0\n",
    "for value in biobert_output[\"Precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "Precision_Mean = score/count\n",
    "    \n",
    "print(f\"Biobert - Precision_Mean: {Precision_Mean}\")\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in llama_output[\"Precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "Precision_Mean = score/count\n",
    "\n",
    "print(f\"Biobert-LLama - Precision_Mean: {Precision_Mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "count = 0\n",
    "for value in biobert_output[\"Recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "Recall_Mean = score/count\n",
    "\n",
    "print(f\"Biobert - Recall_Mean: {Recall_Mean}\")\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in llama_output[\"Recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "Recall_Mean = score/count\n",
    "\n",
    "print(f\"Biobert-LLama - Recall_Mean: {Recall_Mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = 0\n",
    "count = 0\n",
    "for value in biobert_output[\"F1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "F1_Mean = score/count\n",
    "\n",
    "print(f\"Biobert - F1_Mean: {F1_Mean}\")\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in llama_output[\"F1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "F1_Mean = score/count\n",
    "\n",
    "print(f\"Biobert-LLama - F1_Mean: {F1_Mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification avaliation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "def jaccard(y_true, y_pred):\n",
    "    return jaccard_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annotation avaliation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jaccard Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import jaccard_score\n",
    "\n",
    "def jaccard(y_true, y_pred):\n",
    "    return jaccard_score(y_true, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    return precision_score(y_true, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    return recall_score(y_true, y_pred, average='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    return f1_score(y_true, y_pred, average='binary')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
