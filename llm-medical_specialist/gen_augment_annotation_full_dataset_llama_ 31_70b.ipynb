{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f82d9ea",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SageMaker JumpStart - invoke text generation endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c51bc6",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to attach a predictor to an existing endpoint name and invoke the endpoint with example payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f18dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker\n",
    "%pip install jsonlines\n",
    "%pip install tdqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6267543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import retrieve_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98dc72a",
   "metadata": {},
   "source": [
    "Retrieve a predictor from your deployed endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc092a63",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"jumpstart-dft-llama-3-1-70b-instruct\"\n",
    "predictor = retrieve_default(endpoint_name)\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# endpoint_name = \"jumpstart-dft-meta-textgeneration-llama-3-70b-instruct\"\n",
    "\n",
    "\n",
    "def query_endpoint(payload):\n",
    "    config = Config(\n",
    "    read_timeout=900,\n",
    "    connect_timeout=900,\n",
    "    tcp_keepalive=True,\n",
    "    retries={\"max_attempts\": 0})\n",
    "\n",
    "    client = boto3.client(\"sagemaker-runtime\",config=config)\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "    response = response[\"Body\"].read().decode(\"utf8\")\n",
    "    response = json.loads(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1130d-1026-43fa-bfa1-c698cf79cf63",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Medical Specialist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01410659-6b63-4060-a579-1d2fb40d3d64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format messages for Llama-3 chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    prompt: List[str] = []\n",
    "    # print(messages[0]['role'])\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        content = \"\".join([\"<|start_header_id|>system<|end_header_id|>\\n\\n\", messages[0][\"content\"], \"<|eot_id|>\", \"<|start_header_id|>user<|end_header_id|>\\n\\n\",messages[1][\"content\"],\"<|eot_id|>\"])\n",
    "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
    "\n",
    "    for user, answer in zip(messages[::2], messages[1::2]):\n",
    "        prompt.extend([\"<|start_header_id|>user<|end_header_id|>\", \"\\n\\n\", (user[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "\n",
    "    prompt.extend([\"<|begin_of_text|>\", (messages[0][\"content\"]).strip(), \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"])\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "llama_config = {\"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 8192\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ad602-5c13-4a62-9e93-47d2ac0479f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T16:09:20.312480Z",
     "iopub.status.busy": "2024-10-09T16:09:20.311707Z",
     "iopub.status.idle": "2024-10-09T16:09:20.408426Z",
     "shell.execute_reply": "2024-10-09T16:09:20.407380Z",
     "shell.execute_reply.started": "2024-10-09T16:09:20.312435Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "def get_static_shots():\n",
    "    file_path_example = 'static_examples.jsonl'\n",
    "    with jsonlines.open(file_path_example) as reader:\n",
    "        data = [line for line in reader]\n",
    "    return data\n",
    "\n",
    "static_shots = get_static_shots()\n",
    "print(static_shots[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2da384-7577-4c68-a3ce-4437db3043b0",
   "metadata": {},
   "source": [
    "====================================\n",
    "\n",
    "Augmented annotation format for NER (Encoder-decoder transformers) - Athiwaratkun et al, 2020\n",
    "\n",
    "Example of annotation format below:\n",
    "This is an example entity. = This is an [example entity | example].\n",
    "\n",
    "\"Given a passage, your task is to\n",
    "extract all entities and identify their\n",
    "entity types from this list: test,\n",
    "treatment, problem. The output\n",
    "should be in a list of tuples of the\n",
    "following format\"\n",
    "\n",
    "=====================================\n",
    "\n",
    "GPT-NER format of NER augmentation Wang et al, 2023\n",
    "This approach annotates categories individually \n",
    "\n",
    "\"The task is to label [Entity Type] entities in the\n",
    "given sentence. Here are some examples\"\n",
    "\n",
    "@@Columbus## is a city[...]\n",
    "\n",
    "===================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487de77a-896b-48ce-8e6b-723c6e667532",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "def get_text_example(text):\n",
    "    return str({\"text\":text})\n",
    "def get_annotation_example(text):\n",
    "    return str({\"annotation\":text})\n",
    "def get_examples(file_path):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        data = [line for line in reader]\n",
    "    return data\n",
    "\n",
    "def get_from_annotated_dataset(annotated_dataset,_id):\n",
    "    for doc in annotated_dataset:\n",
    "        if doc['doc_id'] == _id:\n",
    "            return doc\n",
    "\n",
    "system_ideas_augmented_athiwaratkun_zero_shot = \"\"\"You are a medical assistant with expertise in medical document processing.\\n\n",
    " Your task is to tag entities related to these classes ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic.\n",
    "You must be augment the text by tagging entity types for each word directly within the text. Follow this format: \"This is an [example | entityType], with more text\".\n",
    "All the texts will be in portuguese. DO NOT translate the classes. ONLY use JSON as the output format, starting with 'annotations' and the value being the annotated text. DO NOT write, only respond in JSON format.\\n\"\"\"\n",
    "\n",
    "system_ideas_augmented_athiwaratkun_1_shot = \"\"\" \"\"\"\n",
    "system_ideas_augmented_athiwaratkun_2_shot = \"\"\" \"\"\"\n",
    "\n",
    "system_ideas_augmented_athiwaratkun_shot_dynamic = \"\"\" You are a medical assistant with expertise in medical document processing.\\n\n",
    "Your task is to extract all entities and identify their entity types ONLY from this list: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic.\n",
    "You must be augment the text by tagging entity types for each word directly within the text. Follow this format: \"This is an example phrase, here is the [entity | entityType]\".\n",
    "All the texts will be in portuguese. ONLY use JSON as the output format, starting with 'annotations' and the value being the annotated text. \n",
    "DO NOT write, only respond in JSON format.\\n\n",
    "Examples of user input and assistant output:\\n {{shot}}\"\"\"\n",
    "\n",
    "# system_ideas_full_in_order = \"\"\"You are a medical assistant with expertise in medical document processing.\\n Your task is to tag entities related to these classes ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic. Each line must include: (1) word or phrase, (2) class or classes that the text is a part of. Maintain the correct format. Example: ['token', ['physical']]. All the texts will be in portuguese. ONLY use JSON as the output format, starting with 'annotations'. DO NOT write, only respond in JSON format. Examples of user input and assistant output:\\n \"user input\":\\n \"A Doença Pulmonar Obstrutiva Crônica é uma condição de insuficiência respiratória de padrão obstrutivo, que ocorre por lesão crônica do parêqnuima pulmonar, a qual culmina em diminuição da complacência pulmonar. A fisiopatogenia envolve um processo inflamatório crônico das vias aéreas e do parênquima que não permite a saída de ar dos pulmões e leva ao acúmulo de volume morto nos alvéolos, mantendo o tórax hiperinsuflado, com acúmulo de CO2 e dificultando as trocas respiratórias.\\nA causa mais comum para DPOC é o tabagismo, mas outras causas incluem a convivência com forno a lenha por longo tempo ou a deficiência genética de alfa-1-antitripsina.\\nO principal sintoma desses pacientes é a dispneia, que se incia em grandes esforços e pode chegar até ao repouso. O diagnóstico é feito pela clínica + espirometria. A principal complicação são as exacerbações de doença que pode ser associada a quadro infeccioso sobreposto.\\nEsses pacientes podem ser divididos segundo os critérios do GOLD entre pacientes muito sintomátisoc&nbsp; e com muitas exacerbações, e o tratamento utiliza LABA, SABA LAMA e CI a depender desses\\n', 'assistant output':\\n \"annotations\": [['tabagismo', ['epidemiology', 'etiology'], ['forno a lenha', ['epidemiology', 'etiology'], ['forno a lenha + por longo tempo', ['epidemiology'], ['deficiência genética de alfa-1-antitripsina', ['etiology'], ['diagnóstico é feito pela clínica + espirometria', ['exams', 'history'], ['dispneia', ['history'], ['dispneia + incia em grandes esforços', 693, 734, ['history'], ['dispneia + incia em grandes esforços + pode chegar até ao repouso', 693, 763, ['history'], ['exacerbações de doença', ['history', 'pathophysiology'], ['exacerbações de doença + quadro infeccioso', ['history', 'pathophysiology'], ['podem ser divididos segundo os critérios do GOLD',['history'], ['critérios do GOLD + muito sintomátiso + muitas exacerbações', ['history'], ['lesão crônica do parêqnuima pulmonar', ['pathophysiology'], ['diminuição da complacência pulmonar', ['pathophysiology'], ['processo inflamatório crônico', ['pathophysiology'], ['não permite a saída de ar dos pulmões', ['pathophysiology'], ['acúmulo de volume morto nos alvéolos', ['pathophysiology'], ['tórax hiperinsuflado', ['pathophysiology'], ['acúmulo de CO2', ['pathophysiology'], ['dificultando as trocas respiratórias', ['pathophysiology'], ['insuficiência respiratória', ['pathophysiology'], ['LABA', ['therapeutic']], ['SABA', ['therapeutic']], ['LAMA', ['therapeutic']], ['CI', ['therapeutic']]]\\n\"\"\"\n",
    "\n",
    "# system_ideas_full_in_order_2_shot = \"\"\"You are a medical assistant with expertise in medical document processing.\\n Your task is to tag entities related to these classes ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic. Each line must include: (1) word or phrase, (2) class or classes that the text is a part of. Maintain the correct format. Example: ['token', ['physical']]. All the texts will be in portuguese. ONLY use JSON as the output format, starting with 'annotations'. DO NOT write, only respond in JSON format. Examples of user input and assistant output:\\n \"user input\":\\n \"A Doença Pulmonar Obstrutiva Crônica é uma condição de insuficiência respiratória de padrão obstrutivo, que ocorre por lesão crônica do parêqnuima pulmonar, a qual culmina em diminuição da complacência pulmonar. A fisiopatogenia envolve um processo inflamatório crônico das vias aéreas e do parênquima que não permite a saída de ar dos pulmões e leva ao acúmulo de volume morto nos alvéolos, mantendo o tórax hiperinsuflado, com acúmulo de CO2 e dificultando as trocas respiratórias.\\nA causa mais comum para DPOC é o tabagismo, mas outras causas incluem a convivência com forno a lenha por longo tempo ou a deficiência genética de alfa-1-antitripsina.\\nO principal sintoma desses pacientes é a dispneia, que se incia em grandes esforços e pode chegar até ao repouso. O diagnóstico é feito pela clínica + espirometria. A principal complicação são as exacerbações de doença que pode ser associada a quadro infeccioso sobreposto.\\nEsses pacientes podem ser divididos segundo os critérios do GOLD entre pacientes muito sintomátisoc&nbsp; e com muitas exacerbações, e o tratamento utiliza LABA, SABA LAMA e CI a depender desses\\n', 'assistant output':\\n \"annotations\": [['tabagismo', ['epidemiology', 'etiology'], ['forno a lenha', ['epidemiology', 'etiology'], ['forno a lenha + por longo tempo', ['epidemiology'], ['deficiência genética de alfa-1-antitripsina', ['etiology'], ['diagnóstico é feito pela clínica + espirometria', ['exams', 'history'], ['dispneia', ['history'], ['dispneia + incia em grandes esforços', 693, 734, ['history'], ['dispneia + incia em grandes esforços + pode chegar até ao repouso', 693, 763, ['history'], ['exacerbações de doença', ['history', 'pathophysiology'], ['exacerbações de doença + quadro infeccioso', ['history', 'pathophysiology'], ['podem ser divididos segundo os critérios do GOLD',['history'], ['critérios do GOLD + muito sintomátiso + muitas exacerbações', ['history'], ['lesão crônica do parêqnuima pulmonar', ['pathophysiology'], ['diminuição da complacência pulmonar', ['pathophysiology'], ['processo inflamatório crônico', ['pathophysiology'], ['não permite a saída de ar dos pulmões', ['pathophysiology'], ['acúmulo de volume morto nos alvéolos', ['pathophysiology'], ['tórax hiperinsuflado', ['pathophysiology'], ['acúmulo de CO2', ['pathophysiology'], ['dificultando as trocas respiratórias', ['pathophysiology'], ['insuficiência respiratória', ['pathophysiology'], ['LABA', ['therapeutic']], ['SABA', ['therapeutic']], ['LAMA', ['therapeutic']], ['CI', ['therapeutic']]]\\n\"user input\":\\n \"DPOC é uma doença que costuma ocorrer em idosos e muito associada ao tabagismo e inalação de demais partículas tóxicas, com alta prevalência.\\nÉ caracterizada por enfisema e bronquite, havendo tanto o padrão clássico do paciente soprador rosado (magro, avermelhado, predomina enfisema) quanto do tossidor azul (cianótico, sobrepeso, predomina bronquite).\\nComo sintomas clássicos, a DPOC tem como sintomas tosse expectorante crônica, dispneia, infecções de repetição, edema. No exame físico, nota-se timpanismo, tórax aumentado em volume, respiração não enche plenamente a caixa torácica, por vezes uso de musculatura acessória, ruídos adventícios.\\n\",\"assistant output\":\\n\"annotations\":[['idosos', ['epidemiology']], ['muito associada ao tabagismo', ['epidemiology']], ['inalação de demais partículas tóxicas', ['epidemiology', 'etiology']], ['alta prevalência', ['epidemiology']], ['enfisema', ['physical', 'pathophysiology']], ['bronquite', ['physical', 'pathophysiology']], ['soprador rosado', ['pathophysiology', 'physical']], ['magro', ['physical']], ['avermelhado', ['physical']], ['predomina enfisema', ['pathophysiology']], ['tossidor azul', ['pathophysiology', 'physical']], ['cianótico', ['physical']], ['sobrepeso', ['physical']], ['predomina bronquite', ['pathophysiology']], ['tosse expectorante crônica', ['history']], ['dispneia', ['history']], ['infecções de repetição', ['history']], ['edema', ['physical']], ['timpanismo', ['physical']], ['tórax aumentado em volume', ['physical']], ['respiração não enche plenamente a caixa torácica', ['uso de musculatura acessória', ['physical']],['ruídos adventícios', ['physical']]]\"\"\"\n",
    "\n",
    "file_path = 'prompt-shots.jsonl'\n",
    "examples = get_examples(file_path)\n",
    "annotated_dataset = get_examples('teste-progresso/annotations-medical_specialist-dpoc-json.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2465d6-2ea4-4989-b2c4-fdefd24cb8e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "import sys\n",
    "def set_prompt(prompt_type,prompt_category,prompt_guide_level, prompt_pos, prompt_shot_amount,question, sys_prompt_name='default'):\n",
    "    # user_prompt = prompt_type[prompt_guide_level][prompt_pos]\n",
    "    ideas_shot_template = \"P. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR.<<{{studentResponse}}>>\\nSeparação e classificação de ideias:\\n<<{{annotationsResponse}}>>\"\n",
    "    \n",
    "    system_prompt = \"Você é um médico especialista com conhecimento médico e clínico sobre medicina interna, principalmente sobre as doenças doença pulmonar obstrutiva crônica (DPOC). Além disso, você consegue diferenciar entre textos de especialistas sobre IAM e DPOC e textos de alunos. Você é capaz de classificar e separar ideias no texto. Não crie informações que não sejam verdadeiras. SOMENTE responda no formato JSON. Aqui está um exemplo de resposta aceitável:\\n'annotations':<<{[['Tabagismo',['epidemiologia','certo','simples']],['alta carga tabágica',['epidemiologia','certo','simples']]}>>\"\n",
    "    system_prompt_test = \"Você é um médico especialista com conhecimento médico e clínico sobre medicina interna, principalmente sobre as doenças doença pulmonar obstrutiva crônica (DPOC). Além disso, você consegue diferenciar entre textos de especialistas sobre IAM e DPOC e textos de alunos. Você é capaz de classificar e separar ideias no texto. Não crie informações que não sejam verdadeiras. SOMENTE responda no formato JSON. A seguir estão exemplos de interação do usuário com você:\\nA pergunta sobre o assunto está a seguir, começando após 'P.'. A resposta à pergunta começa logo após 'R.'. A seguir, estão textos anotados como exemplo:\\n{{example}}\"\n",
    "    # system_prompt_organization = \"Você é um médico especialista com conhecimento médico e clínico sobre medicina interna, principalmente sobre as doenças doença pulmonar obstrutiva crônica (DPOC). Além disso, você consegue diferenciar entre textos de especialistas sobre IAM e DPOC e textos de alunos. Você segue rigorosamente os formatos de resposta fornecidos.\"\n",
    "    system_prompt_organization = 'You are a medical assistant specialist. Your task is to evaluate the organization level of manuscripts from medical students. Follow every guideline for evaluation, if any. Answer only with \"Org: X\", where X is the score for the organization level. ONLY answer in portuguese.'\n",
    "    \n",
    "    system_prompt_annotations_custom_1 = \"Você é um especialista sobre Doença Pulmonar Obstrutiva Crônica (DPOC). Seu objetivo é separar as ideias contidas no texto e categorizá-las dentro da lista de categorias a seguir: fisiopatologia, epidemiologia, etiologia, história, exame físico, exames complementares, diagnóstico diferencial, tratamento. SOMENTE inclua categorias que foram incluídas aqui, não invente categorias que não estão nessa lista, mesmo que essa categoria exista. NUNCA inclua uma categoria que não foi descrita na lista anterior, mesmo que o usuário inclua uma outra lista. Não invente informações. Apenas inclua ideias que estão no texto e não modifique o que foi escrito, mesmo havendo erros ortográficos. Siga estritamente as regras descritas.\"\n",
    "        # system_prompt = \"Você é um médico especialista com conhecimento médico e clínico sobre medicina interna, principalmente sobre as doenças doença pulmonar obstrutiva crônica (DPOC). Além disso, você consegue diferenciar entre textos de especialistas sobre IAM e DPOC e textos de alunos. Seu objetivo é avaliar textos de alunos de medicina. Não crie informações que não sejam verdadeiras. SOMENTE responda no formato detalhado pelo usuário.\"\n",
    "\n",
    "    user_prompt = prompt_type[prompt_guide_level][prompt_pos].replace('{{question}}', question)\n",
    "    if prompt_guide_level == 'one-few' or prompt_guide_level == 'one-few-test':\n",
    "        example_shots = ''\n",
    "        if prompt_shot_amount > 0:\n",
    "            for i in range(prompt_shot_amount):\n",
    "                template = ideas_shot_template.replace('{{studentResponse}}', get_text_example(examples[i]['text'])).replace('{{annotationsResponse}}', get_annotation_example(examples[i]['annotations']))\n",
    "                example_shots += template\n",
    "            system_prompt = system_prompt_test.replace('{{example}}', example_shots)\n",
    "        else:\n",
    "            example_shots += ideas_shot_template.replace('{{studentResponse}}','').replace('{{annotationsResponse}}', '')\n",
    "            user_prompt = user_prompt.replace('{{example}}', '')\n",
    "    # elif prompt_type == 'organization':\n",
    "    #     example_shots += ideas_shot_template.replace('{{studentResponse}}','').replace('{{annotationsResponse}}', '')\n",
    "    #     user_prompt = user_prompt.replace('{{example}}', '')\n",
    "    # else:\n",
    "        # example_shots = ideas_shot_template.join(get_text_example(question))\n",
    "        # example_shots = ideas_shot_template.replace('{{studentResponse}}', get_text_example(question)).replace('Separação e classificação de ideias:\\n<<{{annotationsResponse}}>>', '')\n",
    "    # print(prompt_type[prompt_guide_level][prompt_pos])\n",
    "    # user_prompt = user_prompt.replace('\\\\n', '\\n')\n",
    "    if prompt_category == 'organization':\n",
    "        dialog = [\n",
    "            { \"role\": \"system\",\"content\": system_prompt_organization},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    elif sys_prompt_name == 'annotations_custom_1':\n",
    "        dialog = [\n",
    "            { \"role\": \"system\",\"content\": system_prompt_annotations_custom_1},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    elif prompt_category == 'custom':\n",
    "        dialog = [\n",
    "            { \"role\": \"system\",\"content\": sys_prompt_name},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    else:\n",
    "        dialog = [\n",
    "            { \"role\": \"system\",\"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    input_prompt = format_messages(dialog)\n",
    "    return input_prompt\n",
    "\n",
    "def set_custom_prompt(system_prompt, user_prompt):\n",
    "    dialog = [\n",
    "            { \"role\": \"system\",\"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    input_prompt = format_messages(dialog)\n",
    "    return input_prompt\n",
    "\n",
    "def prompt_routine(input_prompt, num_replicas, top_p,temp,top_k,max_new_tokens):\n",
    "    prompt_list = []\n",
    "    llama_config[\"top_p\"] = top_p\n",
    "    llama_config[\"temperature\"] = temp\n",
    "    llama_config[\"top_k\"] = top_k\n",
    "    payload = {\n",
    "    \"inputs\":  input_prompt,\n",
    "       \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\":max_new_tokens,\n",
    "        \"top_p\": llama_config[\"top_p\"],\n",
    "        \"temperature\": llama_config[\"temperature\"],\n",
    "        # \"repetition_penalty\":llama_config['repetition_penalty'],\n",
    "        \"top_k\": llama_config[\"top_k\"],\n",
    "        \"stop\": \"<|eot_id|>\"}\n",
    "    }\n",
    "    for i in range(num_replicas):\n",
    "        predictior_output = query_endpoint(payload)\n",
    "        # sys.stdout.write('\\r')\n",
    "        # # the exact output you're looking for:\n",
    "        # sys.stdout.write(\"[%-20s] %d%%\" % ('='*num_replicas, 5*num_replicas))\n",
    "        # sys.stdout.flush()\n",
    "        # sleep(0.25)\n",
    "        prompt_list.append({\"generated_text\":predictior_output[\"generated_text\"] ,\"input\":input_prompt})\n",
    "        \n",
    "    return prompt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78222b5e-e5f0-409f-b8db-b07ffd92fa07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T16:09:20.586546Z",
     "iopub.status.busy": "2024-10-09T16:09:20.585937Z",
     "iopub.status.idle": "2024-10-09T16:09:20.700686Z",
     "shell.execute_reply": "2024-10-09T16:09:20.699787Z",
     "shell.execute_reply.started": "2024-10-09T16:09:20.586503Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/'\n",
    "file_names = os.listdir(path)\n",
    "used_data = [\"f6cb9773-9a81-499c-85c7-3cebe935b930\",\"207c237f-3bcd-4010-bcf1-c1e8b32de6be\"]\n",
    "exisiting_ids = []\n",
    "for name in file_names:\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6') and doc_id not in used_data:\n",
    "        used_data.append(doc_id)\n",
    "        exisiting_ids.append(doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c8a1f-5885-4392-ae3b-0bfb0e5cbe3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T16:09:20.705309Z",
     "iopub.status.busy": "2024-10-09T16:09:20.702111Z",
     "iopub.status.idle": "2024-10-09T16:09:20.790832Z",
     "shell.execute_reply": "2024-10-09T16:09:20.789936Z",
     "shell.execute_reply.started": "2024-10-09T16:09:20.705260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_new_examples (guided_lvl,prompt_category,shot_n):\n",
    "    random_annotated = random.choice(annotated_dataset)\n",
    "    while random_annotated['doc_id'] in used_data:\n",
    "        random_annotated = random.choice(annotated_dataset)\n",
    "    if prompt_category == 'organization':\n",
    "        #prompt_type,prompt_category,prompt_guide_level, prompt_pos, prompt_shot_amount,question\n",
    "            input_prompt = set_prompt(prompt_organization_lvl,'organization',guided_lvl, 0, shot_n,random_annotated['text'],'system_organization_lvl')\n",
    "    else:\n",
    "        input_prompt = set_prompt(prompt_ideas_full, guided_lvl, 0, shot_n,random_annotated['text'])\n",
    "    # prompt_category = 'ideas'\n",
    "    # guided_lvl = 'less'\n",
    "    output_batch = prompt_routine(input_prompt, 20,0.6,0.9,llama_config['top_k'],10)\n",
    "    dict_to_json = {\"doc_id\": random_annotated['doc_id'],\"text\": random_annotated['text'], \"response\": output_batch}\n",
    "    with open(f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}-{guided_lvl}-{random_annotated[\"doc_id\"]}.json', 'w') as file:\n",
    "        json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "    print('Complete',random_annotated['doc_id'])\n",
    "def run_examples_from_list (list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/'\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        requested_doc = get_from_annotated_dataset(_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_prompt(prompt_ideas_full, guided_lvl, 0, shot_n,requested_doc['text'])\n",
    "            output_batch = prompt_routine(input_prompt, 40,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'])\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'], \"response\": output_batch}\n",
    "            with open(f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def run_examples_from_list_all_guide_lvl (list_id,prompt_obj,guided_lvl_max,prompt_category,shot_n):\n",
    "    path = f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}'\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        requested_doc = get_from_annotated_dataset(_id)\n",
    "        max_guided = False\n",
    "        for guided_lvl in prompt_obj:\n",
    "           \n",
    "            if max_guided == False:\n",
    "                for prompt_pos in range(len(prompt_obj[guided_lvl])):\n",
    "                    if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "                        input_prompt = set_prompt(prompt_obj, prompt_category, guided_lvl, prompt_pos, shot_n,requested_doc['text'])\n",
    "                        output_batch = prompt_routine(input_prompt,20,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],llama_config['max_new_tokens'])\n",
    "                        dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'], \"response\": output_batch}\n",
    "                        with open(f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                            json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "                        print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)*3}')\n",
    "                    else:\n",
    "                        print('Already exists',requested_doc['doc_id'])\n",
    "                    i += 1\n",
    "            if guided_lvl == guided_lvl_max:\n",
    "                max_guided = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024c29f-abd3-43b2-921b-f29bb1920b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Annotating all categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44f20a-1846-446d-81dd-d69aeeeabe9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T16:53:21.662559Z",
     "iopub.status.busy": "2024-05-07T16:53:21.661517Z",
     "iopub.status.idle": "2024-05-07T16:53:21.667459Z",
     "shell.execute_reply": "2024-05-07T16:53:21.666135Z",
     "shell.execute_reply.started": "2024-05-07T16:53:21.662521Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment 1 - One prompt for all Categories (Same texts as Gabriel's Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e382b-d716-4032-b2d9-4a76e29266d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T20:07:26.487169Z",
     "iopub.status.busy": "2024-09-09T20:07:26.486348Z",
     "iopub.status.idle": "2024-09-09T20:07:26.612945Z",
     "shell.execute_reply": "2024-09-09T20:07:26.608313Z",
     "shell.execute_reply.started": "2024-09-09T20:07:26.487122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test_ids = list(pd.read_csv('test_data_info.csv')['doc_id'])\n",
    "test_ids_no_short = list(pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')['doc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38866a-e607-43e8-99e5-a67a5133ef72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T20:07:26.621055Z",
     "iopub.status.busy": "2024-09-09T20:07:26.617292Z",
     "iopub.status.idle": "2024-09-09T20:07:26.626419Z",
     "shell.execute_reply": "2024-09-09T20:07:26.625363Z",
     "shell.execute_reply.started": "2024-09-09T20:07:26.620999Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff = list(set(test_ids) - set(test_ids_no_short))\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6c556-bfb8-4c38-a091-63a930c8fbc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:17:41.460188Z",
     "iopub.status.busy": "2024-09-12T19:17:41.459463Z",
     "iopub.status.idle": "2024-09-12T19:17:41.487530Z",
     "shell.execute_reply": "2024-09-12T19:17:41.484507Z",
     "shell.execute_reply.started": "2024-09-12T19:17:41.460145Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_annotation_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def run_annotation_processed_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs-augmented/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dddae93-f551-4eb9-9aef-b745f22bbbc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1-shot (arbitrary examples) temp 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728ff91-80ca-4bba-b5ab-6f2e7f57de92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T20:07:26.722256Z",
     "iopub.status.busy": "2024-09-09T20:07:26.721955Z",
     "iopub.status.idle": "2024-09-09T20:37:47.021437Z",
     "shell.execute_reply": "2024-09-09T20:37:47.020375Z",
     "shell.execute_reply.started": "2024-09-09T20:07:26.722228Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_full_in_order,test_ids_no_short,'full','ideas',1)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3081929-342d-4098-b512-c728571095bd",
   "metadata": {},
   "source": [
    "#### 2-shot (arbitrary examples) temp 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f07b0-5cba-4f82-a39b-8827b638340d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T20:37:47.026734Z",
     "iopub.status.busy": "2024-09-09T20:37:47.024219Z",
     "iopub.status.idle": "2024-09-09T21:08:09.998262Z",
     "shell.execute_reply": "2024-09-09T21:08:09.997019Z",
     "shell.execute_reply.started": "2024-09-09T20:37:47.026686Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_full_in_order_2_shot,test_ids_no_short,'full','ideas',2)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23292a83-3218-4da0-9902-b126ffcda9a4",
   "metadata": {},
   "source": [
    "#### 1-shot (arbitrary examples) temp 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1dfe04-abf8-4f56-8fe3-b1cb909c215c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T21:08:10.000478Z",
     "iopub.status.busy": "2024-09-09T21:08:09.999987Z",
     "iopub.status.idle": "2024-09-09T21:38:37.131021Z",
     "shell.execute_reply": "2024-09-09T21:38:37.129530Z",
     "shell.execute_reply.started": "2024-09-09T21:08:10.000434Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_full_in_order,test_ids_no_short,'full','ideas',1)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450fc4c-da70-4399-85ba-1c05fcbc0a39",
   "metadata": {},
   "source": [
    "#### 2-shot (arbitrary examples) temp 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9540c1e-54e7-4554-86ad-a755699224f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T21:38:37.138391Z",
     "iopub.status.busy": "2024-09-09T21:38:37.136948Z",
     "iopub.status.idle": "2024-09-09T22:08:31.041562Z",
     "shell.execute_reply": "2024-09-09T22:08:31.040474Z",
     "shell.execute_reply.started": "2024-09-09T21:38:37.138338Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_full_in_order_2_shot,test_ids_no_short,'full','ideas',2)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f1ec3-6ebe-42a7-a0f1-6b0ded742755",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 2 - One prompt for all Categories (Information Retrieval for best shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3828fb2-ed20-425c-9a3a-2ddd7aa9ef87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T22:08:31.043752Z",
     "iopub.status.busy": "2024-09-09T22:08:31.043269Z",
     "iopub.status.idle": "2024-09-09T22:08:31.170511Z",
     "shell.execute_reply": "2024-09-09T22:08:31.169698Z",
     "shell.execute_reply.started": "2024-09-09T22:08:31.043708Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "df_data_info = pd.read_csv('test_data_info.csv')\n",
    "df_no_short_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "def minimize_labels(label):\n",
    "    n_label = []\n",
    "    if type(label) == str:\n",
    "        label = eval(label)\n",
    "    for l in label:\n",
    "        if l[4] != None:\n",
    "            n_label.append([l[0],list(l[4].keys())])\n",
    "        else:\n",
    "            n_label.append([l[0],l[4]])\n",
    "    return n_label\n",
    "def get_only_text(label):\n",
    "    n_label = []\n",
    "    if type(label) == str:\n",
    "        label = eval(label)\n",
    "    for l in label:\n",
    "        n_label.append([l[0]])\n",
    "    return n_label\n",
    "\n",
    "def bio_to_cluster_annotation (bio_annotation):\n",
    "    main_annotation = []\n",
    "    for i in range(len(bio_annotation)):\n",
    "        list_ann = eval(bio_annotation[i])\n",
    "        \n",
    "        sub_annotation = []\n",
    "        phrase = []\n",
    "        for z in range(len(list_ann)):\n",
    "            if list_ann[z][3] == 'B':\n",
    "                if len(phrase) > 0:\n",
    "                    sub_annotation.append([' '.join(phrase),list(list_ann[z][4].keys())])\n",
    "                    phrase = []\n",
    "                phrase.append(list_ann[z][0])\n",
    "            elif list_ann[z][3] == 'I':\n",
    "                phrase.append(list_ann[z][0])\n",
    "        main_annotation.append(sub_annotation)\n",
    "    return main_annotation\n",
    "\n",
    "def extract_example_shot_from_row(_row, output='string'):\n",
    "    if output == 'list':\n",
    "        shot_text = {'user_input':_row['text'],'assistant_output':_row['cluster_labels']}\n",
    "    elif output == 'string':\n",
    "        shot_text = f\"\"\"'user_input':{_row['text']}\\n 'assistant_output':\"annotations\":{_row['cluster_labels']}\"\"\"\n",
    "    return shot_text\n",
    "\n",
    "def find_top_matches(query, df, top_n):\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the text data in the dataframe\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Transform the query string\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Calculate cosine similarity between the query vector and all documents\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "    # Get the indices of the top 3 matches\n",
    "    top_indices = similarity_scores.argsort()[0][-1*top_n:][::-1]\n",
    "    # Get the corresponding documents from the dataframe\n",
    "    top_matches = df.iloc[top_indices]\n",
    "    # print(type(top_matches))\n",
    "    \n",
    "    return top_matches\n",
    "\n",
    "## Transform list of annotated tokens into continuous string for TF-IDF\n",
    "def annotation_token_to_text(df_text):\n",
    "    annotation_token_to_text = ''\n",
    "    # for id,row in df_text.iterrows():\n",
    "        # print(row['cluster_labels'])\n",
    "        # print(eval(row['cluster_labels'])[0])\n",
    "        # print('============list_row==')\n",
    "    list_row = eval(df_text['cluster_labels'])\n",
    "    for i in range(len(list_row)):\n",
    "        annotation_token_to_text += list_row[i][0] + ' '\n",
    "    return annotation_token_to_text\n",
    "\n",
    "## Retrieving similar examples from complete text (TF-IDF)\n",
    "def find_top_matches_from_annotation(query, df, top_n):\n",
    "    # for i in range(len(df['cluster_labels'])):\n",
    "    #     df['cluster_labels'][i] = minimize_labels(df['cluster_labels'][i])\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the text data in the dataframe\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['cluster_labels'])\n",
    "\n",
    "    # Transform the query string\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Calculate cosine similarity between the query vector and all documents\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "    # Get the indices of the top 3 matches\n",
    "    top_indices = similarity_scores.argsort()[0][-1*top_n:][::-1]\n",
    "    # Get the corresponding documents from the dataframe\n",
    "    top_matches = df.iloc[top_indices]\n",
    "    # print(type(top_matches))\n",
    "    \n",
    "    return top_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe7c9d-647c-48fa-8c86-d5e084cd959c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T22:08:31.175419Z",
     "iopub.status.busy": "2024-09-09T22:08:31.175033Z",
     "iopub.status.idle": "2024-09-09T22:08:31.192448Z",
     "shell.execute_reply": "2024-09-09T22:08:31.191164Z",
     "shell.execute_reply.started": "2024-09-09T22:08:31.175389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_annotations_tf_idf_shots (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-tf-idf-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            df_top_shots = find_top_matches(requested_doc[\"text\"],df_data_info[df_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "            shots_text = ''\n",
    "            for j in range(shot_n):\n",
    "                top_shot = df_top_shots.iloc[j]\n",
    "                # print('top_shot',top_shot)\n",
    "                txt = extract_example_shot_from_row(top_shot)\n",
    "                shots_text += '\\n'+txt\n",
    "            # print('bundle of shots', shots_text)\n",
    "            replace_sys_instruction = system_ideas_full_in_order_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "            # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "            input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "            report_path = path\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def predict_annotations_tf_idf_shots_from_preprocessed (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-tf-idf-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            df_top_shots = find_top_matches(requested_doc[\"text\"],df_no_short_data_info[df_no_short_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "            shots_text = ''\n",
    "            for j in range(shot_n):\n",
    "                top_shot = df_top_shots.iloc[j]\n",
    "                # print('top_shot',top_shot)\n",
    "                txt = extract_example_shot_from_row(top_shot)\n",
    "                shots_text += '\\n'+txt\n",
    "            # print('bundle of shots', shots_text)\n",
    "            replace_sys_instruction = system_ideas_full_in_order_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "            # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "            input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "            report_path = path\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da9a03-32dc-41b4-a3d5-c3ec3349a2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffa1bb-a481-435f-a040-8b7e1779a2ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T22:08:31.196157Z",
     "iopub.status.busy": "2024-09-09T22:08:31.194314Z",
     "iopub.status.idle": "2024-09-09T22:36:44.128895Z",
     "shell.execute_reply": "2024-09-09T22:36:44.127639Z",
     "shell.execute_reply.started": "2024-09-09T22:08:31.196121Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0f76f-e009-4dd7-a594-0989c10864bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T22:36:44.133052Z",
     "iopub.status.busy": "2024-09-09T22:36:44.132552Z",
     "iopub.status.idle": "2024-09-09T23:05:23.700592Z",
     "shell.execute_reply": "2024-09-09T23:05:23.698503Z",
     "shell.execute_reply.started": "2024-09-09T22:36:44.133018Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3d2173-8eef-48db-9853-697b9989209c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff351f9-c8db-44b5-852a-bce9c75071b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T23:05:23.704743Z",
     "iopub.status.busy": "2024-09-09T23:05:23.704236Z",
     "iopub.status.idle": "2024-09-09T23:34:04.336519Z",
     "shell.execute_reply": "2024-09-09T23:34:04.335297Z",
     "shell.execute_reply.started": "2024-09-09T23:05:23.704697Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e6ecf-2cfb-43de-9beb-b832d408231c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T23:34:04.338462Z",
     "iopub.status.busy": "2024-09-09T23:34:04.337903Z",
     "iopub.status.idle": "2024-09-10T00:03:12.158922Z",
     "shell.execute_reply": "2024-09-10T00:03:12.157648Z",
     "shell.execute_reply.started": "2024-09-09T23:34:04.338418Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af528022-b5be-4657-9543-c82b09e645a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639e664-41ce-41ff-b592-9db8e795d626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T00:03:12.164447Z",
     "iopub.status.busy": "2024-09-10T00:03:12.163175Z",
     "iopub.status.idle": "2024-09-10T00:32:46.628093Z",
     "shell.execute_reply": "2024-09-10T00:32:46.626930Z",
     "shell.execute_reply.started": "2024-09-10T00:03:12.164411Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10662be8-cc9a-4ac1-8c8a-ad244953174a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T00:32:46.630959Z",
     "iopub.status.busy": "2024-09-10T00:32:46.630432Z",
     "iopub.status.idle": "2024-09-10T01:02:43.499995Z",
     "shell.execute_reply": "2024-09-10T01:02:43.498874Z",
     "shell.execute_reply.started": "2024-09-10T00:32:46.630913Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513852be-d7b8-446b-a618-778ffab9e78e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9065a-b6f1-43ea-8bee-5f99f9100e78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T01:02:43.502515Z",
     "iopub.status.busy": "2024-09-10T01:02:43.502100Z",
     "iopub.status.idle": "2024-09-10T01:32:49.707723Z",
     "shell.execute_reply": "2024-09-10T01:32:49.706725Z",
     "shell.execute_reply.started": "2024-09-10T01:02:43.502472Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e30f16-4abb-4760-960c-ef6a90268f69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T01:32:49.710212Z",
     "iopub.status.busy": "2024-09-10T01:32:49.709336Z",
     "iopub.status.idle": "2024-09-10T02:03:19.834171Z",
     "shell.execute_reply": "2024-09-10T02:03:19.831770Z",
     "shell.execute_reply.started": "2024-09-10T01:32:49.710167Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c98480a-1695-4bb8-837a-c991c9aab698",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 10-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607664e7-c38e-41ba-ad95-1b7ffb723584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:03:22.277015Z",
     "iopub.status.busy": "2024-09-13T15:03:22.276164Z",
     "iopub.status.idle": "2024-09-13T15:36:40.852369Z",
     "shell.execute_reply": "2024-09-13T15:36:40.851161Z",
     "shell.execute_reply.started": "2024-09-13T15:03:22.276965Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8025c7-d11a-4b86-9471-94cb83d62f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:36:40.855908Z",
     "iopub.status.busy": "2024-09-13T15:36:40.855261Z",
     "iopub.status.idle": "2024-09-13T16:10:28.068840Z",
     "shell.execute_reply": "2024-09-13T16:10:28.067627Z",
     "shell.execute_reply.started": "2024-09-13T15:36:40.855870Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6ae66-8245-4bc5-907f-166eeeaed990",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Experiment 3 - Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9683345e-6950-4584-a415-59d8f58510e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test_ids = list(pd.read_csv('test_data_info.csv')['doc_id'])\n",
    "test_ids_no_short = list(pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')['doc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c71261-de0a-4463-a82d-8afca82d7557",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_annotation_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "        \n",
    "def run_annotation_processed_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs-augmented/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f407b76-fc56-4ff8-a22b-b619f1a54bc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### zero shot temp 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751a72f-7089-412a-ae84-536c4767f290",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_augmented_athiwaratkun_zero_shot,test_ids_no_short,'full','ideas',0)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6233a-cd60-4094-abfd-0b8c080865cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:04:45.727520Z",
     "iopub.status.busy": "2024-08-26T15:04:45.725971Z",
     "iopub.status.idle": "2024-08-26T15:04:45.732644Z",
     "shell.execute_reply": "2024-08-26T15:04:45.731899Z",
     "shell.execute_reply.started": "2024-08-26T15:04:45.727477Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment 4 - Similar annotation instead of similar text (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f6be2-459c-414e-9c4d-0aa1e5b12d7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T02:32:53.696861Z",
     "iopub.status.busy": "2024-09-10T02:32:53.696362Z",
     "iopub.status.idle": "2024-09-10T02:32:53.899329Z",
     "shell.execute_reply": "2024-09-10T02:32:53.898319Z",
     "shell.execute_reply.started": "2024-09-10T02:32:53.696815Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "df_data_info = pd.read_csv('test_data_info.csv')\n",
    "df_no_short_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "df_medical_specialist_pre_processed = pd.read_csv('annotations_medical_specialist_pre_processed.csv')\n",
    "def minimize_labels(label):\n",
    "    n_label = []\n",
    "    if type(label) == str:\n",
    "        label = eval(label)\n",
    "    for l in label:\n",
    "        if l[4] != None:\n",
    "            n_label.append([l[0],list(l[4].keys())])\n",
    "        else:\n",
    "            n_label.append([l[0],l[4]])\n",
    "    return n_label\n",
    "def get_only_text(label):\n",
    "    n_label = []\n",
    "    if type(label) == str:\n",
    "        label = eval(label)\n",
    "    for l in label:\n",
    "        n_label.append([l[0]])\n",
    "    return n_label\n",
    "\n",
    "def bio_to_cluster_annotation (bio_annotation):\n",
    "    main_annotation = []\n",
    "    for i in range(len(bio_annotation)):\n",
    "        list_ann = eval(bio_annotation[i])\n",
    "        \n",
    "        sub_annotation = []\n",
    "        phrase = []\n",
    "        for z in range(len(list_ann)):\n",
    "            if list_ann[z][3] == 'B':\n",
    "                if len(phrase) > 0:\n",
    "                    sub_annotation.append([' '.join(phrase),list(list_ann[z][4].keys())])\n",
    "                    phrase = []\n",
    "                phrase.append(list_ann[z][0])\n",
    "            elif list_ann[z][3] == 'I':\n",
    "                phrase.append(list_ann[z][0])\n",
    "        main_annotation.append(sub_annotation)\n",
    "    return main_annotation\n",
    "\n",
    "def extract_example_shot_from_row(_row, output='string'):\n",
    "    if output == 'list':\n",
    "        shot_text = {'user_input':_row['text'],'assistant_output':_row['cluster_labels']}\n",
    "    elif output == 'string':\n",
    "        shot_text = f\"\"\"'user_input':{_row['text']}\\n 'assistant_output':\"annotations\":{_row['cluster_labels']}\"\"\"\n",
    "    return shot_text\n",
    "\n",
    "## Transform list of annotated tokens into continuous string for TF-IDF \n",
    "def annotation_token_to_text(df_text):\n",
    "    annotation_token_to_text = ''\n",
    "    # for id,row in df_text.iterrows():\n",
    "        # print(row['cluster_labels'])\n",
    "        # print(eval(row['cluster_labels'])[0])\n",
    "        # print('============list_row==')\n",
    "    list_row = eval(df_text['cluster_labels'])\n",
    "    for i in range(len(list_row)):\n",
    "        annotation_token_to_text += list_row[i][0] + ' '\n",
    "    return annotation_token_to_text\n",
    "\n",
    "## Retrieving similar examples from complete text (TF-IDF)  ##\n",
    "def find_top_matches_from_annotation(query, df, top_n):\n",
    "    # for i in range(len(df['cluster_labels'])):\n",
    "    #     df['cluster_labels'][i] = minimize_labels(df['cluster_labels'][i])\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the text data in the dataframe\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['cluster_labels'])\n",
    "\n",
    "    # Transform the query string\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Calculate cosine similarity between the query vector and all documents\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "    # Get the indices of the top 3 matches\n",
    "    top_indices = similarity_scores.argsort()[0][-1*top_n:][::-1]\n",
    "    # Get the corresponding documents from the dataframe\n",
    "    top_matches = df.iloc[top_indices]\n",
    "    # print(type(top_matches))\n",
    "    \n",
    "    return top_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79f6b9-f873-4746-a440-4c7a2570e19f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T02:32:53.901224Z",
     "iopub.status.busy": "2024-09-10T02:32:53.900871Z",
     "iopub.status.idle": "2024-09-10T02:32:53.923929Z",
     "shell.execute_reply": "2024-09-10T02:32:53.922789Z",
     "shell.execute_reply.started": "2024-09-10T02:32:53.901185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_annotations_tf_idf_custom_shots (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs-augmented/full-dataset/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-tf-idf-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            df_top_shots = find_top_matches_from_annotation(requested_doc[\"text\"],df_data_info[df_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "            shots_text = ''\n",
    "            for j in range(shot_n):\n",
    "                top_shot = df_top_shots.iloc[j]\n",
    "                # print('top_shot',top_shot)\n",
    "                txt = extract_example_shot_from_row(top_shot)\n",
    "                shots_text += '\\n'+txt\n",
    "            # print('bundle of shots', shots_text)\n",
    "            replace_sys_instruction = system_ideas_augmented_athiwaratkun_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "            # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "            input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "            report_path = path\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}', end=\"\\r\")\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'], end=\"\\r\")\n",
    "        i += 1\n",
    "\n",
    "def predict_annotations_tf_idf_custom_shots_from_preprocessed (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/tf-idf-custom/ideas-tf-idf-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            df_top_shots = find_top_matches_from_annotation(requested_doc[\"text\"],df_no_short_data_info[df_no_short_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "            shots_text = ''\n",
    "            for j in range(shot_n):\n",
    "                top_shot = df_top_shots.iloc[j]\n",
    "                # print('top_shot',top_shot)\n",
    "                txt = extract_example_shot_from_row(top_shot)\n",
    "                shots_text += '\\n'+txt\n",
    "            # print('bundle of shots', shots_text)\n",
    "            replace_sys_instruction = system_ideas_augmented_athiwaratkun_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "            # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "            input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "            report_path = path\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}', end=\"\\r\")\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'], end=\"\\r\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9931590-50f0-4f15-981f-481cde1bde61",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af09e4e3-3ffa-41c8-bf87-e0ceb566259f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:09:03.992476Z",
     "iopub.status.busy": "2024-09-10T14:09:03.990383Z",
     "iopub.status.idle": "2024-09-10T14:09:55.853459Z",
     "shell.execute_reply": "2024-09-10T14:09:55.851808Z",
     "shell.execute_reply.started": "2024-09-10T14:09:03.992433Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdfdbc2-7232-413b-96e7-75c281e42e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:09:55.856922Z",
     "iopub.status.busy": "2024-09-10T14:09:55.856340Z",
     "iopub.status.idle": "2024-09-10T14:38:21.218789Z",
     "shell.execute_reply": "2024-09-10T14:38:21.211858Z",
     "shell.execute_reply.started": "2024-09-10T14:09:55.856873Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c052d75-7231-4fc1-9d6f-af2eadb0a4ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919518e-9640-4ef9-b2d4-4241f58be8ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:38:21.221014Z",
     "iopub.status.busy": "2024-09-10T14:38:21.220274Z",
     "iopub.status.idle": "2024-09-10T15:06:49.863030Z",
     "shell.execute_reply": "2024-09-10T15:06:49.861722Z",
     "shell.execute_reply.started": "2024-09-10T14:38:21.220965Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c9569-5612-469a-819d-130090cfd01a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T15:06:49.866425Z",
     "iopub.status.busy": "2024-09-10T15:06:49.866122Z",
     "iopub.status.idle": "2024-09-10T15:35:45.172045Z",
     "shell.execute_reply": "2024-09-10T15:35:45.170855Z",
     "shell.execute_reply.started": "2024-09-10T15:06:49.866397Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2626c4-2079-4580-93cc-0ef99f43d3db",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4607c58-772c-44d8-927c-766174e70c8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T15:35:45.174559Z",
     "iopub.status.busy": "2024-09-10T15:35:45.173828Z",
     "iopub.status.idle": "2024-09-10T16:05:04.075986Z",
     "shell.execute_reply": "2024-09-10T16:05:04.071333Z",
     "shell.execute_reply.started": "2024-09-10T15:35:45.174351Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add6f01-0b1d-4ba6-8e76-81e395b5eaec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:05:04.082967Z",
     "iopub.status.busy": "2024-09-10T16:05:04.079959Z",
     "iopub.status.idle": "2024-09-10T16:34:33.539905Z",
     "shell.execute_reply": "2024-09-10T16:34:33.536629Z",
     "shell.execute_reply.started": "2024-09-10T16:05:04.082911Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221dbaa-0478-4e71-9208-48caf5d5df0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 4-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ccadb2-34f4-454c-95ca-53e053a18927",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 10-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086d742-af3f-45b1-a25b-68ac54f8dde2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:17:50.052136Z",
     "iopub.status.busy": "2024-09-12T19:17:50.051664Z",
     "iopub.status.idle": "2024-09-12T19:50:57.672632Z",
     "shell.execute_reply": "2024-09-12T19:50:57.664100Z",
     "shell.execute_reply.started": "2024-09-12T19:17:50.052099Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a86e4b-1ca8-4d68-ae99-b36ce138ba2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:50:57.680665Z",
     "iopub.status.busy": "2024-09-12T19:50:57.679556Z",
     "iopub.status.idle": "2024-09-12T20:24:35.808825Z",
     "shell.execute_reply": "2024-09-12T20:24:35.807581Z",
     "shell.execute_reply.started": "2024-09-12T19:50:57.680616Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6baeb2",
   "metadata": {},
   "source": [
    "### Experiment 5 - Random Example Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0aa394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_n_shots(query, df, top_n):\n",
    "    return df.sample(n=top_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec7fdd-6c97-4925-a6ae-f8c83cc2016e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Score Assessment by annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4eab3d-ffe7-49ef-8ef8-e35e701e82fb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-09T20:04:43.221035Z",
     "iopub.status.idle": "2024-09-09T20:04:43.221989Z",
     "shell.execute_reply": "2024-09-09T20:04:43.221773Z",
     "shell.execute_reply.started": "2024-09-09T20:04:43.221741Z"
    }
   },
   "outputs": [],
   "source": [
    "# def predict_score_by_annotation_custom_shot (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "#     path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/tf-idf-custom/scoring-tf-idf-{shot_n}-shot'\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)\n",
    "#     file_names = os.listdir(path)\n",
    "#     i = 1\n",
    "#     for _id in list_id:\n",
    "#         # print(_id)\n",
    "#         requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "#         if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "#             df_top_shots = find_top_matches_from_annotation(requested_doc[\"text\"],df_no_short_data_info[df_no_short_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "#             shots_text = ''\n",
    "#             for j in range(shot_n):\n",
    "#                 top_shot = df_top_shots.iloc[j]\n",
    "#                 # print('top_shot',top_shot)\n",
    "#                 txt = extract_example_shot_from_row(top_shot)\n",
    "#                 shots_text += '\\n'+txt\n",
    "#             # print('bundle of shots', shots_text)\n",
    "#             replace_sys_instruction = system_ideas_full_in_order_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "#             # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "#             input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "#             # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "#             output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "#             dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "#             report_path = path\n",
    "#             if not os.path.exists(report_path):\n",
    "#                 os.makedirs(report_path)\n",
    "#             with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "#                 json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "#             print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "#         else:\n",
    "#             print('Already exists',requested_doc['doc_id'])\n",
    "#         i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3842d-8632-4c10-b02c-8a46bc856069",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-09T20:04:43.223244Z",
     "iopub.status.idle": "2024-09-09T20:04:43.224268Z",
     "shell.execute_reply": "2024-09-09T20:04:43.223804Z",
     "shell.execute_reply.started": "2024-09-09T20:04:43.223778Z"
    }
   },
   "outputs": [],
   "source": [
    "# system_ideas_full_in_order_2_shot = \"\"\"You are a medical assistant with expertise for education evaluation of students.\\n Your task is to evaluate the organization medical student texts. After the text, an annotation of . All the texts will be in portuguese. ONLY use JSON as the output format, starting with 'grade'. DO NOT write, only respond in JSON format. \n",
    "# Examples of user input and assistant output:\\n \"user input\":\\n \"A Doença Pulmonar Obstrutiva Crônica é uma condição de insuficiência respiratória de padrão obstrutivo, que ocorre por lesão crônica do parêqnuima pulmonar, a qual culmina em diminuição da complacência pulmonar. A fisiopatogenia envolve um processo inflamatório crônico das vias aéreas e do parênquima que não permite a saída de ar dos pulmões e leva ao acúmulo de volume morto nos alvéolos, mantendo o tórax hiperinsuflado, com acúmulo de CO2 e dificultando as trocas respiratórias.\\nA causa mais comum para DPOC é o tabagismo, mas outras causas incluem a convivência com forno a lenha por longo tempo ou a deficiência genética de alfa-1-antitripsina.\\nO principal sintoma desses pacientes é a dispneia, que se incia em grandes esforços e pode chegar até ao repouso. O diagnóstico é feito pela clínica + espirometria. A principal complicação são as exacerbações de doença que pode ser associada a quadro infeccioso sobreposto.\\nEsses pacientes podem ser divididos segundo os critérios do GOLD entre pacientes muito sintomátisoc&nbsp; e com muitas exacerbações, e o tratamento utiliza LABA, SABA LAMA e CI a depender desses\\n', 'assistant output':\\n \"annotations\": [['tabagismo', ['epidemiology', 'etiology'], ['forno a lenha', ['epidemiology', 'etiology'], ['forno a lenha + por longo tempo', ['epidemiology'], ['deficiência genética de alfa-1-antitripsina', ['etiology'], ['diagnóstico é feito pela clínica + espirometria', ['exams', 'history'], ['dispneia', ['history'], ['dispneia + incia em grandes esforços', 693, 734, ['history'], ['dispneia + incia em grandes esforços + pode chegar até ao repouso', 693, 763, ['history'], ['exacerbações de doença', ['history', 'pathophysiology'], ['exacerbações de doença + quadro infeccioso', ['history', 'pathophysiology'], ['podem ser divididos segundo os critérios do GOLD',['history'], ['critérios do GOLD + muito sintomátiso + muitas exacerbações', ['history'], ['lesão crônica do parêqnuima pulmonar', ['pathophysiology'], ['diminuição da complacência pulmonar', ['pathophysiology'], ['processo inflamatório crônico', ['pathophysiology'], ['não permite a saída de ar dos pulmões', ['pathophysiology'], ['acúmulo de volume morto nos alvéolos', ['pathophysiology'], ['tórax hiperinsuflado', ['pathophysiology'], ['acúmulo de CO2', ['pathophysiology'], ['dificultando as trocas respiratórias', ['pathophysiology'], ['insuficiência respiratória', ['pathophysiology'], ['LABA', ['therapeutic']], ['SABA', ['therapeutic']], ['LAMA', ['therapeutic']], ['CI', ['therapeutic']]]\\n\"user input\":\\n \"DPOC é uma doença que costuma ocorrer em idosos e muito associada ao tabagismo e inalação de demais partículas tóxicas, com alta prevalência.\\nÉ caracterizada por enfisema e bronquite, havendo tanto o padrão clássico do paciente soprador rosado (magro, avermelhado, predomina enfisema) quanto do tossidor azul (cianótico, sobrepeso, predomina bronquite).\\nComo sintomas clássicos, a DPOC tem como sintomas tosse expectorante crônica, dispneia, infecções de repetição, edema. No exame físico, nota-se timpanismo, tórax aumentado em volume, respiração não enche plenamente a caixa torácica, por vezes uso de musculatura acessória, ruídos adventícios.\\n\",\"assistant output\":\\n\"annotations\":[['idosos', ['epidemiology']], ['muito associada ao tabagismo', ['epidemiology']], ['inalação de demais partículas tóxicas', ['epidemiology', 'etiology']], ['alta prevalência', ['epidemiology']], ['enfisema', ['physical', 'pathophysiology']], ['bronquite', ['physical', 'pathophysiology']], ['soprador rosado', ['pathophysiology', 'physical']], ['magro', ['physical']], ['avermelhado', ['physical']], ['predomina enfisema', ['pathophysiology']], ['tossidor azul', ['pathophysiology', 'physical']], ['cianótico', ['physical']], ['sobrepeso', ['physical']], ['predomina bronquite', ['pathophysiology']], ['tosse expectorante crônica', ['history']], ['dispneia', ['history']], ['infecções de repetição', ['history']], ['edema', ['physical']], ['timpanismo', ['physical']], ['tórax aumentado em volume', ['physical']], ['respiração não enche plenamente a caixa torácica', ['uso de musculatura acessória', ['physical']],['ruídos adventícios', ['physical']]]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59a845-3029-45a8-b271-6a55dae88613",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Delete endpoint (stop billing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2f16d-7638-4807-9b60-68f511efd03f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify your AWS Region\n",
    "aws_region='us-east-1'\n",
    "\n",
    "# Create a low-level SageMaker service client.\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
    "\n",
    "# Delete endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18769f0-048c-4402-8e48-7c56d17314c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
