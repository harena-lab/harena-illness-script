{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old (Self-Consistency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "import json\n",
    "import jsonlines\n",
    "import re\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "import string\n",
    "from difflib import SequenceMatcher\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def ideas_less_to_json(_data):\n",
    "    pattern = r\"{\\n(.*?\\n})\\n}\"\n",
    "    substring = re.search(pattern, _data, re.DOTALL).group(1)\n",
    "    # if ': {' in substring:\n",
    "    #     substring = substring.replace(': {', ':[')\n",
    "    #     substring = substring.replace('}', ']')\n",
    "    \n",
    "    substring_json = json.loads('{'+substring+'}')\n",
    "    # print(type(substring_json))\n",
    "    return substring_json\n",
    "\n",
    "with jsonlines.open('../teste-progresso/04-bilou/annotations-medical_specialist-dpoc-json.jsonl') as reader:\n",
    "        annotated_dataset = [line for line in reader]\n",
    "for doc in annotated_dataset:\n",
    "    if doc['doc_id'] == '6c118cb7-47ad-41fa-b429-51c7232e7a66':\n",
    "        specialist_annotations = doc['annotations']\n",
    "n_specialist_annotations = len(specialist_annotations)\n",
    "print('Number of ideas from specialist:',len(specialist_annotations))\n",
    "ideas = []\n",
    "for idea in specialist_annotations:\n",
    "    ideas.append(idea[0])\n",
    "print(ideas)  \n",
    "print('========================')  \n",
    "\n",
    "with open('ideas-one-few-6c118cb7-47ad-41fa-b429-51c7232e7a66.json', \"r\", encoding='utf8') as file:\n",
    "    llama_annotated = json.load(file)\n",
    "\n",
    "\n",
    "for i in range(len(llama_annotated['response'])):\n",
    "    run = llama_annotated['response'][i] \n",
    "    llama_annotated['response'][i] = ideas_shot_to_json(run)\n",
    "    print(llama_annotated['response'][i])\n",
    "    idea_list = []\n",
    "    for idea in llama_annotated['response'][i]['annotations'].values():\n",
    "        if len(idea) > 1:\n",
    "            idea_list = idea\n",
    "        else:\n",
    "            idea_list.append(idea[0])\n",
    "    llama_annotated['response'][i]['grouped_ideas'] = idea_list\n",
    "def get_top_ideas(response):\n",
    "    dict_ideas = {}\n",
    "    printed_lengths = set()\n",
    "    length_count = {}\n",
    "    max_length = 0\n",
    "    for data in response['response']:\n",
    "        for idea in data['grouped_ideas']:\n",
    "            idea_no_accents = idea.replace('á', 'a').replace('ã', 'a').replace('é', 'e').replace('ê', 'e').replace('í', 'i').replace('ó', 'o').replace('ô', 'o').replace('õ', 'o').replace('ú', 'u').replace('ç', 'c')\n",
    "            dict_ideas[idea_no_accents] = dict_ideas.get(idea_no_accents,0)+1\n",
    "        length = len(data['grouped_ideas'])\n",
    "        if length not in printed_lengths:\n",
    "            printed_lengths.add(length)\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "        length_count[length] = length_count.get(length, 0) + 1\n",
    "    print(\"Amount of objects with each length:\")\n",
    "\n",
    "    for length, count in length_count.items():\n",
    "        print(f\"Length {length}: {count} objects\")\n",
    "\n",
    "    # Print the most used values throughout all objects in descending order\n",
    "    # value_count = {}\n",
    "    # for key in dict_ideas.items():\n",
    "    #     value_count[key] = value_count.get(value, 0) + 1\n",
    "    sorted_values = sorted(dict_ideas.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Most used values:\")\n",
    "    value_list = []\n",
    "    for value, count in sorted_values:\n",
    "        value_list.append(value)\n",
    "        print(f\"{value}: {count} times\")\n",
    "    # print(sorted_values[:max_length])\n",
    "    return value_list[:max_length],max_length\n",
    "\n",
    "\n",
    "llama_annotations, n_llama_annotations = get_top_ideas(llama_annotated)\n",
    "\n",
    "# for llama_idea in llama_annotations:\n",
    "#     for specialist_idea in specialist_annotations:\n",
    "#         if llama_idea in specialist_idea:\n",
    "#             print(f\"{llama_idea} is contained in {specialist_idea}\")\n",
    "#         if specialist_idea in llama_idea:\n",
    "#             print(f\"{specialist_idea} is contained in {llama_idea}\")\n",
    "print('relatedness section','==============================================')\n",
    "llama_dict_precision = {}\n",
    "spec_i_blacklist = []\n",
    "for llama_idea in llama_annotations:\n",
    "    print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "    llama_dict_precision[llama_idea] = {'precision':0,'specialist_idea_contained':[],'is_fully_contained':False}\n",
    "    spec_i = 0\n",
    "\n",
    "    for specialist_idea_info in specialist_annotations:\n",
    "        if spec_i not in spec_i_blacklist:\n",
    "            specialist_idea = specialist_idea_info[0]\n",
    "            specialist_idea_no_accents = specialist_idea.replace('á', 'a').replace('ã', 'a').replace('é', 'e').replace('ê', 'e').replace('í', 'i').replace('ó', 'o').replace('ô', 'o').replace('õ', 'o').replace('ú', 'u').replace('ç', 'c')\n",
    "            # Define the stopwords and punctuation\n",
    "            stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "            punctuation = string.punctuation\n",
    "\n",
    "            # Remove stopwords and punctuation from the strings\n",
    "            llama_idea_clean = ' '.join([word for word in llama_idea.split() if word.lower() not in stopwords and word not in punctuation])\n",
    "            specialist_idea_clean = ' '.join([word for word in specialist_idea_no_accents.split() if word.lower() not in stopwords and word not in punctuation])\n",
    "            llama_idea_clean_tokens = (llama_idea_clean).lower().split()\n",
    "            specialist_idea_clean_tokens = specialist_idea_clean.lower().split()\n",
    "            # print(llama_idea_clean_tokens)\n",
    "            # print(specialist_idea_clean_tokens)\n",
    "\n",
    "            l_sublist_of_s = set(llama_idea_clean_tokens).issubset(set(specialist_idea_clean_tokens))\n",
    "            s_sub_of_l = set(specialist_idea_clean_tokens).issubset(set(llama_idea_clean_tokens))\n",
    "            # Calculate the relatedness ratios\n",
    "            matcher = SequenceMatcher(lambda x: x in [' ', '\\n'] or x.lower() in stopwords or x in punctuation, llama_idea, specialist_idea)\n",
    "            a_to_b_ratio = matcher.ratio()\n",
    "\n",
    "            # Print the relatedness ratios\n",
    "            # print('relatedness','========================')\n",
    "            if s_sub_of_l and l_sublist_of_s:\n",
    "                llama_dict_precision[llama_idea]['precision'] = 1\n",
    "                llama_dict_precision[llama_idea]['specialist_idea_contained'].append(specialist_idea)\n",
    "                llama_dict_precision[llama_idea]['is_fully_contained'] = True\n",
    "                spec_i_blacklist.append(spec_i)   \n",
    "\n",
    "                print(llama_idea_clean,'||||', specialist_idea_clean)\n",
    "                print('100% match')\n",
    "            elif s_sub_of_l:\n",
    "                llama_dict_precision[llama_idea]['precision'] += a_to_b_ratio\n",
    "                llama_dict_precision[llama_idea]['specialist_idea_contained'].append(specialist_idea)\n",
    "                llama_dict_precision[llama_idea]['is_fully_contained'] = False\n",
    "                spec_i_blacklist.append(spec_i)   \n",
    "\n",
    "                print(llama_idea_clean,'||||', specialist_idea_clean)\n",
    "                print(f'Complete match from specialist_idea to llama_idea: {s_sub_of_l}')\n",
    "                print(f'Relatedness of llama_idea to specialist_idea: {a_to_b_ratio}')\n",
    "            elif l_sublist_of_s:\n",
    "                llama_dict_precision[llama_idea]['precision'] += a_to_b_ratio\n",
    "                llama_dict_precision[llama_idea]['specialist_idea_contained'].append(specialist_idea)\n",
    "                llama_dict_precision[llama_idea]['is_fully_contained'] = False\n",
    "                spec_i_blacklist.append(spec_i)   \n",
    "\n",
    "                print(llama_idea_clean,'||||', specialist_idea_clean)\n",
    "                print(f'Complete match from llama_idea to specialist_idea: {l_sublist_of_s}')\n",
    "                print(f'Relatedness of llama_idea to specialist_idea: {a_to_b_ratio}')\n",
    "        # elif a_to_b_ratio > 0.17:\n",
    "        spec_i += 1\n",
    "        \n",
    "    print(specialist_annotations)\n",
    "    print(llama_dict_precision[llama_idea])\n",
    "    print('################################################')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ideas_shot_to_json(_data):\n",
    "    pattern = r\"{\\n(.*?)\\n}\"\n",
    "    substring = re.search(pattern, _data, re.DOTALL).group(1)\n",
    "    # if ': {' in substring:\n",
    "    #     substring = substring.replace(': {', ':[')\n",
    "    #     substring = substring.replace('}', ']')\n",
    "    try:\n",
    "        substring_json = eval('{'+substring+'}')\n",
    "    except:\n",
    "        substring = substring.replace(\"[['\", \"['\")\n",
    "        substring_json = eval('{'+substring+'}')\n",
    "    # print(type(substring_json))\n",
    "    return substring_json\n",
    "\n",
    "with open('ideas-one-few-6c118cb7-47ad-41fa-b429-51c7232e7a66.json', \"r\", encoding='utf8') as file:\n",
    "    llama_annotated = json.load(file)\n",
    "\n",
    "\n",
    "for i in range(len(llama_annotated['response'])):\n",
    "    run = llama_annotated['response'][i] \n",
    "    llama_annotated['response'][i] = ideas_shot_to_json(run)\n",
    "\n",
    "    idea_list = []\n",
    "    for idea in llama_annotated['response'][i]['annotations']:\n",
    "        if len(idea) > 1:\n",
    "            idea_list = idea\n",
    "        else:\n",
    "            idea_list.append(idea[0])\n",
    "    llama_annotated['response'][i]['grouped_ideas'] = idea_list\n",
    "\n",
    "def get_top_ideas_w_cat(response):\n",
    "    print('====!!!======!!!')\n",
    "    print(response['response'][1]['annotations'])\n",
    "    dict_ideas = {}\n",
    "    dict_ideas_w_category = {}\n",
    "    printed_lengths = set()\n",
    "    length_count = {}\n",
    "    max_length = 0\n",
    "    for data in response['response']:\n",
    "        for idea_block in data['annotations']:\n",
    "            idea = idea_block[0]\n",
    "            category = []\n",
    "            for cat in idea_block[1]:\n",
    "                blacklist = ['certo','errado','simples','encapsulado','jargão']\n",
    "                if cat not in blacklist:\n",
    "                    category.append(cat)\n",
    "            \n",
    "            # print('category',category)\n",
    "            # print('for annotations,',idea_block)\n",
    "            # print(idea)\n",
    "            idea_no_accents = idea.replace('á', 'a').replace('ã', 'a').replace('é', 'e').replace('ê', 'e').replace('í', 'i').replace('ó', 'o').replace('ô', 'o').replace('õ', 'o').replace('ú', 'u').replace('ç', 'c')\n",
    "            dict_ideas[idea_no_accents] = dict_ideas.get(idea_no_accents,0)+1\n",
    "            # if dict_ideas_w_category.get(idea,0) == 0:\n",
    "            #     dict_ideas_w_category[idea] = {str(category): dict_ideas_w_category[idea].get(str(category),0)+1}\n",
    "            # elif dict_ideas_w_category[idea] != category:\n",
    "            #     dict_ideas_w_category[idea] = {str(category): dict_ideas_w_category[idea].get(str(category),0)+1}\n",
    "            #     print('======= CATEGORIA DIVERGENTE  ========================')\n",
    "            #     print('old',dict_ideas_w_category[idea],'new',category)\n",
    "            if dict_ideas_w_category.get(idea_no_accents,0) == 0:\n",
    "                dict_ideas_w_category[idea_no_accents] = {}\n",
    "            dict_ideas_w_category[idea_no_accents][str(category)] = dict_ideas_w_category[idea_no_accents].get(str(category),0)+1\n",
    "            print('dict_ideas_w_category',dict_ideas_w_category[idea_no_accents])\n",
    "\n",
    "        length = len(data['annotations'])\n",
    "        if length not in printed_lengths:\n",
    "            printed_lengths.add(length)\n",
    "        if length > max_length:\n",
    "            max_length = length\n",
    "        length_count[length] = length_count.get(length, 0) + 1\n",
    "    # for idea in dict_ideas_w_category:\n",
    "    #     max_category = max(dict_ideas_w_category[idea], key=dict_ideas_w_category[idea].get)\n",
    "    #     dict_ideas_w_category[idea] = max_category\n",
    "    print(\"Amount of objects with each length:\")\n",
    "\n",
    "    for length, count in length_count.items():\n",
    "        print(f\"Length {length}: {count} objects\")\n",
    "\n",
    "    # Print the most used values throughout all objects in descending order\n",
    "    # value_count = {}\n",
    "    # for key in dict_ideas.items():\n",
    "    #     value_count[key] = value_count.get(value, 0) + 1\n",
    "    sorted_values = sorted(dict_ideas.items(), key=lambda x: x[1], reverse=True)\n",
    "    print(\"Most used values:\")\n",
    "    value_list = []\n",
    "    for value, count in sorted_values:\n",
    "        value_list.append(value)\n",
    "        print(f\"{value} {dict_ideas_w_category[value]}: {count} times\")\n",
    "    # print(sorted_values[:max_length])\n",
    "    return value_list[:max_length],max_length,dict_ideas_w_category\n",
    "\n",
    "\n",
    "llama_annotations, n_llama_annotations,annotations_w_category = get_top_ideas_w_cat(llama_annotated)\n",
    "\n",
    "# for llama_idea in llama_annotations:\n",
    "#     for specialist_idea in specialist_annotations:\n",
    "#         if llama_idea in specialist_idea:\n",
    "#             print(f\"{llama_idea} is contained in {specialist_idea}\")\n",
    "#         if specialist_idea in llama_idea:\n",
    "#             print(f\"{specialist_idea} is contained in {llama_idea}\")\n",
    "print('relatedness section','==============================================')\n",
    "llama_dict_precision = {}\n",
    "spec_i_blacklist = []\n",
    "for llama_idea in llama_annotations:\n",
    "    print('@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@')\n",
    "    llama_dict_precision[llama_idea] = {'precision':0,'specialist_idea_contained':[],'is_fully_contained':False}\n",
    "    spec_i = 0\n",
    "\n",
    "    for specialist_idea_info in specialist_annotations:\n",
    "        if spec_i not in spec_i_blacklist:\n",
    "            specialist_idea = specialist_idea_info[0]\n",
    "            specialist_idea_no_accents = specialist_idea.replace('á', 'a').replace('ã', 'a').replace('é', 'e').replace('ê', 'e').replace('í', 'i').replace('ó', 'o').replace('ô', 'o').replace('õ', 'o').replace('ú', 'u').replace('ç', 'c')\n",
    "            # Define the stopwords and punctuation\n",
    "            stopwords = nltk.corpus.stopwords.words('portuguese')\n",
    "            punctuation = string.punctuation\n",
    "\n",
    "            # Remove stopwords and punctuation from the strings\n",
    "            llama_idea_clean = ' '.join([word for word in llama_idea.split() if word.lower() not in stopwords and word not in punctuation])\n",
    "            specialist_idea_clean = ' '.join([word for word in specialist_idea_no_accents.split() if word.lower() not in stopwords and word not in punctuation])\n",
    "            llama_idea_clean_tokens = (llama_idea_clean).lower().split()\n",
    "            specialist_idea_clean_tokens = specialist_idea_clean.lower().split()\n",
    "            # print(llama_idea_clean_tokens)\n",
    "            # print(specialist_idea_clean_tokens)\n",
    "\n",
    "            l_sublist_of_s = set(llama_idea_clean_tokens).issubset(set(specialist_idea_clean_tokens))\n",
    "            s_sub_of_l = set(specialist_idea_clean_tokens).issubset(set(llama_idea_clean_tokens))\n",
    "            # Calculate the relatedness ratios\n",
    "            matcher = SequenceMatcher(lambda x: x in [' ', '\\n'] or x.lower() in stopwords or x in punctuation, llama_idea, specialist_idea)\n",
    "            a_to_b_ratio = matcher.ratio()\n",
    "\n",
    "            # Print the relatedness ratios\n",
    "            # print('relatedness','========================')\n",
    "            if s_sub_of_l and l_sublist_of_s:\n",
    "                llama_dict_precision[llama_idea]['precision'] = 1\n",
    "                llama_dict_precision[llama_idea]['specialist_idea_contained'].append(specialist_idea)\n",
    "                llama_dict_precision[llama_idea]['is_fully_contained'] = True\n",
    "                spec_i_blacklist.append(spec_i)   \n",
    "\n",
    "                print(llama_idea_clean,'||||', specialist_idea_clean)\n",
    "                print(annotations_w_category[llama_idea])\n",
    "                print('100% match')\n",
    "            elif s_sub_of_l:\n",
    "                llama_dict_precision[llama_idea]['precision'] += a_to_b_ratio\n",
    "                llama_dict_precision[llama_idea]['specialist_idea_contained'].append(specialist_idea)\n",
    "                llama_dict_precision[llama_idea]['is_fully_contained'] = False\n",
    "                spec_i_blacklist.append(spec_i)   \n",
    "\n",
    "                print(llama_idea_clean,'||||', specialist_idea_clean)\n",
    "                print(annotations_w_category[llama_idea])\n",
    "                print(f'Complete match from specialist_idea to llama_idea: {s_sub_of_l}')\n",
    "                print(f'Relatedness of llama_idea to specialist_idea: {a_to_b_ratio}')\n",
    "            elif l_sublist_of_s:\n",
    "                llama_dict_precision[llama_idea]['precision'] += a_to_b_ratio\n",
    "                llama_dict_precision[llama_idea]['specialist_idea_contained'].append(specialist_idea)\n",
    "                llama_dict_precision[llama_idea]['is_fully_contained'] = False\n",
    "                spec_i_blacklist.append(spec_i)   \n",
    "\n",
    "                print(llama_idea_clean,'||||', specialist_idea_clean)\n",
    "                print(annotations_w_category[llama_idea])\n",
    "                print(f'Complete match from llama_idea to specialist_idea: {l_sublist_of_s}')\n",
    "                print(f'Relatedness of llama_idea to specialist_idea: {a_to_b_ratio}')\n",
    "        # elif a_to_b_ratio > 0.17:\n",
    "        spec_i += 1\n",
    "        \n",
    "    print(specialist_annotations)\n",
    "    print(llama_dict_precision[llama_idea])\n",
    "    print('################################################')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "df = pd.read_csv('../teste-progresso/02-anotacoes/annotations-medical_specialist-dpoc.csv')\n",
    "df.head()\n",
    "annotation_id = '05a4a8fe-6c7b-400e-98b8-e46521e6ddd8'\n",
    "filtered_df = df[df['annotation id'] == annotation_id]\n",
    "filtered_df\n",
    "organization_level = filtered_df['organization level'].values[0]\n",
    "organization_level\n",
    "with open('organization-less-05a4a8fe-6c7b-400e-98b8-e46521e6ddd8.json', \"r\", encoding='utf8') as file:\n",
    "    llama_annotated = json.load(file)\n",
    "\n",
    "votes = {}\n",
    "for org in llama_annotated['response']:\n",
    "    org_string = org\n",
    "    number = re.findall(r'\\d+', org_string)\n",
    "    if number:\n",
    "        number = int(number[0])\n",
    "        votes[number] = votes.get(number, 0) + 1\n",
    "if len(list(votes.keys())) == 1:\n",
    "    print('different choice of votes')\n",
    "else:\n",
    "    # return list(votes.keys())[0]\n",
    "    max_key = max(votes, key=lambda k: votes[k])\n",
    "    print(max_key)\n",
    "    print(list(votes.keys())[0])\n",
    "\n",
    "\n",
    "file_names = os.listdir('/')\n",
    "file_names = [file for file in os.listdir('/') if file.endswith('.json')]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of the annotations (TF-IDF and arbitrary examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
