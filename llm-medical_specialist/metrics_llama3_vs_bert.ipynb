{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276c8d1-85d7-4046-875b-69029f4fbafa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:56:43.713266Z",
     "iopub.status.busy": "2025-01-26T20:56:43.712361Z",
     "iopub.status.idle": "2025-01-26T20:56:50.794006Z",
     "shell.execute_reply": "2025-01-26T20:56:50.793042Z",
     "shell.execute_reply.started": "2025-01-26T20:56:43.713211Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install jsonlines\n",
    "import jsonlines\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import re\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6675f250",
   "metadata": {},
   "outputs": [],
   "source": [
    "biobert_data_path = '../NER/Full_Model_Balanced/Datasets/biobert/annotations-dpoc-biobert_metrics_biobert_full_model.csv'\n",
    "biobert_data = pd.read_csv(biobert_data_path)\n",
    "biobert_ids = biobert_data['annotation id'].values\n",
    "print(biobert_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e328da-ff80-4652-9611-498c4c834e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_examples(file_path):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        data = [line for line in reader]\n",
    "    return data\n",
    "def get_from_annotated_dataset(annotated_dataset,_id):\n",
    "    for doc in annotated_dataset:\n",
    "        if doc['doc_id'] == _id:\n",
    "            return doc\n",
    "annotated_dataset = get_examples('teste-progresso/annotations-medical_specialist-dpoc-bio-composed-multiple.jsonl')\n",
    "df_data_info = pd.read_csv('test_data_info_no_short.csv')\n",
    "\n",
    "#REMOVE ADDITIONAL BAD DATA\n",
    "# df_data_info.drop(df_data_info[df_data_info['doc_id'] == '8396380d-e0b6-4b81-8fe9-0b99c611f9f3'].index, inplace=True)\n",
    "# df_data_info.reset_index(drop=True,inplace=True)\n",
    "def replace_substring(string, start, end, replacement):\n",
    "    # Check if start and end are valid indices for the string\n",
    "    if start < 0 or end > len(string) or start > end:\n",
    "        return \"Invalid start or end index\"\n",
    "\n",
    "    # Replace the substring from start to end with the replacement string\n",
    "    new_string = string[:start] + replacement + string[end:]\n",
    "\n",
    "    return new_string\n",
    "def add_quotes(item):\n",
    "    if (not (item.startswith('\"') and item.endswith('\"')) and not (item.startswith(\"'\") and item.endswith(\"'\"))):\n",
    "        if (item.startswith('\"') and not item.endswith('\"')) or (item.startswith(\"'\") and not item.endswith(\"'\")):\n",
    "            item = item.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n",
    "    return item\n",
    "def fix_quotes(text):\n",
    "    # Regular expression to find lists and their items   \n",
    "    pattern = r\"\"\"\\[\\s*(([\"'][^\"']+[\"'])(?:\\s+[^,]+)?)\\s*,\\s*(\\[\\s*[\"'][^\"']+?[\"'](?:\\s*,\\s*[\"'][^\"']+?[\"'])*\\s*\\])\\s*\\]\"\"\"\n",
    "    annotation_list = {\"annotations\":[]}\n",
    "    for match in re.finditer(pattern, text):        \n",
    "        main_text = add_quotes(match.group(1))\n",
    "        if main_text.startswith('\"') and main_text.endswith('\"') or (main_text.startswith(\"'\") and main_text.endswith(\"'\")):\n",
    "            main_text = main_text[1:-1]\n",
    "        current_list = [str(match.group(1)), ast.literal_eval(match.group(3))]\n",
    "        current_list[0] = current_list[0][1:-1]\n",
    "        annotation_list[\"annotations\"].append(current_list)\n",
    "    annotation_list = str(annotation_list)\n",
    "    return annotation_list\n",
    "\n",
    "def select_after_first_brace(string):\n",
    "    pattern = r\"(|[\\'\\\"])(annotations)([\\'\\\"]|)\"\n",
    "    \n",
    "    matches = re.search(pattern, string)\n",
    "    if matches != None:\n",
    "        string = replace_substring(string, matches.span()[0], matches.span()[1], '\"annotations\"')\n",
    "    string.replace(\"]\\']\",']]')\n",
    "    string.replace(\"]\\\"]\",']]')\n",
    "    # print('STRING AFTER FIRST REGEX:', string)\n",
    "    pattern = r'\"annotations\":\\[.*\\]\\]'\n",
    "    match = re.search(pattern, string)\n",
    "    if match != None:\n",
    "        string = replace_substring(string, match.span()[0], match.span()[1], '\"annotations\":[')\n",
    "        string = match.group(0)\n",
    "        # print('STRING AFTER SECOND REGEX:', string)\n",
    "\n",
    "    brace_index = string.find('\"annotations\"')\n",
    "    # print('trying to select correct part of models output')\n",
    "    # print(brace_index)\n",
    "    # print(string)\n",
    "    # if brace_index == -1:\n",
    "    string = fix_quotes(string)\n",
    "    # print('strrrriiing\\n\\n')\n",
    "    # print(string)\n",
    "    brace_index = string.find('\"annotations\"')\n",
    "    \n",
    "    if string[-3:] == \"']]\":\n",
    "        string = string[:-3]+\"']]]\"\n",
    "    if string.find(\"}\") == -1:\n",
    "        end_annotation_index = string.find(']]]')\n",
    "        if string[-1:] != ']' and end_annotation_index != -1:                \n",
    "            string = string[:end_annotation_index+3]+'}'\n",
    "        elif string[-1:] != ']' and end_annotation_index == -1:\n",
    "            end_annotation_index = string.find(']] ]')\n",
    "            if end_annotation_index != -1:\n",
    "                string = string[:end_annotation_index+4]+'}'\n",
    "            else:\n",
    "                string = string+']}'\n",
    "        elif string[-7:].count(']') < 3:\n",
    "            string = string+']}'\n",
    "        else:\n",
    "            string = string+'}'\n",
    "    else:\n",
    "        string = string[:string.find(\"}\")+1]\n",
    "    # print('results...')\n",
    "    # print(string)\n",
    "    string = fix_quotes(string)\n",
    "    return string\n",
    "# prediction_annotation = eval(model_response[0])\n",
    "def prediction_to_labels(prediction_labels, data_info):\n",
    "    if type(prediction_labels) == list:\n",
    "        ze = prediction_labels[0]\n",
    "        prediction_labels = ze\n",
    "    prediction_labels = select_after_first_brace(prediction_labels)\n",
    "    prediction_annotation = ast.literal_eval(prediction_labels)\n",
    "    full_text = data_info['text']\n",
    "    text_tokenized = data_info['labels']\n",
    "    categorized_prediction = annotation_to_tokens(full_text, text_tokenized, prediction_annotation)\n",
    "    labels = extract_labels_from_prediction(categorized_prediction)\n",
    "    return labels\n",
    "def truth_to_labels(data_info):\n",
    "    labels = extract_labels_from_truth(data_info['labels'])\n",
    "    return labels\n",
    "def extract_labels_from_truth (data_info):\n",
    "    text_tokenized = data_info\n",
    "    categories = []\n",
    "    for token in text_tokenized:\n",
    "        if token[4] != None:\n",
    "            categories.append(list(token[4].keys()))\n",
    "        else:\n",
    "            categories.append('0')\n",
    "    return categories\n",
    "def annotation_to_tokens (full_text, text_tokenized, prediction_annotation):\n",
    "    clean_text_tokenized = [[token[0],token[1],token[2]] for token in text_tokenized]\n",
    "    annotations = prediction_annotation['annotations']\n",
    "    for annotation in annotations:\n",
    "        # print('============= new annotation', annotation[0])\n",
    "        start_pos = full_text.find(annotation[0])\n",
    "        end_pos = len(annotation[0])+start_pos-1\n",
    "        # print(f'end pos is {len(annotation)} + {start_pos} - 1 = {end_pos}')\n",
    "        categorizing = False\n",
    "        \n",
    "        for token in clean_text_tokenized:\n",
    "            # print(f'token pos {token[1]} annotation pos {start_pos} token {token[0]}')\n",
    "            if token[1] == start_pos:\n",
    "                # print('starting categorization...')\n",
    "                # print(f'start pos {start_pos} end pos {end_pos} token {token[0]}')\n",
    "                categorizing = True\n",
    "            if categorizing:\n",
    "                #adds category to token\n",
    "                # print(f'adding category {annotation[1]} to token {token[0]}')\n",
    "                token.append(annotation[1])\n",
    "                if token[2] == end_pos:\n",
    "                    # print(f'ending categorization at {token[0]}...')\n",
    "                    categorizing = False\n",
    "                    break\n",
    "            \n",
    "            # print(token)\n",
    "    return clean_text_tokenized\n",
    "def extract_labels_from_prediction (categorized_prediction):\n",
    "    labels = []\n",
    "    for token in categorized_prediction:\n",
    "        if len(token) > 3:\n",
    "            labels.append(token[3])\n",
    "        else:\n",
    "            labels.append('0')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2580d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## AUGMENTED ANNOTATION DATA FUNCTIONS\n",
    "\n",
    "def escape_inner_quotes(text):\n",
    "    pattern = r\"\"\"(?<=[:])[\\s\\S]*?[\"'](.*)(?<!\\\\)[\"'](?=\\s*[}])\"\"\"\n",
    "    escaped_text = re.sub(pattern, lambda m: \"'\" + m.group(1).replace('\"', '\\\\\"').replace(\"'\",\"\\\\'\") + \"'\", text)\n",
    "    return escaped_text\n",
    "####### Function that extracts the entities and respective categories and transforms into a dictionary of annotations  #############\n",
    "\n",
    "def transform_augmented_data_to_pattern(data_info,doc_id=None):\n",
    "    # print(data_info)\n",
    "    data_info = data_info.replace('```','')\n",
    "    data_info = re.sub(r'(?<![\"\\\\]):', r'\\:', data_info)\n",
    "    data_info = data_info.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    raw_data_info = data_info\n",
    "\n",
    "    data_info = re.sub(r\"\"\"[\"']*annotations[\"']*:\"\"\", r'\"annotations\":', data_info)\n",
    "    ann_pattern = r\"\"\"\"annotations\":\\s*[\"']*{*(.*)(?=\\s*[\"']*\\s*[}]*)\"\"\"\n",
    "    match = re.search(ann_pattern, data_info)\n",
    "    if match:\n",
    "        data_info = f'\"annotations\": \"{match.group(1)}\"'\n",
    "        \n",
    "        if data_info[-1:] == \"'\" or data_info[-1:] == '\"':\n",
    "            data_info = data_info[:-1]\n",
    "    else:\n",
    "        data_info = \"\"\" \"annotations\":{\"\"} \"\"\"\n",
    "    closing_braces = data_info.find(\"}\")\n",
    "    opening_braces = data_info.find(\"{\")\n",
    "    if opening_braces == -1:\n",
    "        data_info = '{'+ data_info\n",
    "    elif data_info[:1] != '{':\n",
    "        data_info = '{'+ data_info\n",
    "    if closing_braces == -1:\n",
    "        data_info = data_info[:len(data_info)] + '}'\n",
    "        closing_braces = data_info.find(\"}\")\n",
    "    if (data_info.find('\"}') == -1 and data_info.find('\" }') == -1):\n",
    "        data_info = data_info[:closing_braces] + '\"}'\n",
    "    # if doc_id == '93a9745c-5bd4-469d-a10f-f425377f2e47':\n",
    "    #     print('specific finding')\n",
    "    #     print(data_info)\n",
    "    #     print(data_info[:data_info.find(\"}\")+1])\n",
    "    data_info = escape_inner_quotes(data_info)\n",
    "    # print('after escape inner quotes', data_info)\n",
    "    if data_info[-3] == '\\\\':\n",
    "        data_info = data_info[:len(data_info)-3]+data_info[-2:len(data_info)+1]\n",
    "    data_info = ast.literal_eval(data_info)\n",
    "    data_info['annotations'] = data_info['annotations'].replace('\\\\','')\n",
    "    pattern = r'\\[([^\\[\\]]+)\\s*\\|\\s*([^\\[\\]]+)\\]'\n",
    "    matches = re.findall(pattern, data_info['annotations'])\n",
    "    if len(matches) == 0:\n",
    "        print(\"---------------------------------Error: No matches found in the data.--------------------------\")\n",
    "        print(doc_id)\n",
    "        print(raw_data_info)\n",
    "        print('@@@@@\\n')\n",
    "        print(data_info)\n",
    "        annotations = {\"annotations\": []}\n",
    "    else:\n",
    "        annotations = {\"annotations\": [[match[0].strip(), [val.strip() for val in match[1].split(',')]] for match in matches]}\n",
    "    return annotations\n",
    "def prediction_to_labels_augmented(prediction_labels, data_info,doc_id=None):\n",
    "    prediction_labels = transform_augmented_data_to_pattern(prediction_labels,doc_id)\n",
    "    if type(prediction_labels) == list:\n",
    "        ze = prediction_labels[0]\n",
    "        prediction_labels = ze\n",
    "    prediction_labels = select_after_first_brace(str(prediction_labels))\n",
    "    prediction_annotation = eval(prediction_labels)\n",
    "    full_text = data_info['text']\n",
    "    text_tokenized = data_info['labels']\n",
    "    categorized_prediction = annotation_to_tokens(full_text, text_tokenized, prediction_annotation)\n",
    "    labels = extract_labels_from_prediction(categorized_prediction)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0692ed-4f0f-48b2-b4ac-3a08cade757a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert labels of a annotation to binary vector\n",
    "cats = {'pathophysiology':0,\n",
    "        'etiology':1,\n",
    "        'epidemiology':2,\n",
    "        'history':3,\n",
    "        'physical':4,\n",
    "        'exams':5,\n",
    "        'differential':6,\n",
    "        'therapeutic':7}\n",
    "\n",
    "def label2binary(labels):\n",
    "    vet = [0] * 8\n",
    "    # print(vet)\n",
    "    for label in labels:\n",
    "        if label in list(cats.keys()):\n",
    "            vet[cats[label]] = 1\n",
    "    return vet\n",
    "\n",
    "# Convert all the text - divided in tokens - labels to binary vectors\n",
    "def preprocess_classification(classi_data):\n",
    "    # print('preprocess classification ======= ',classi_data)\n",
    "    for index in range(len(classi_data)):\n",
    "        classi_data[index] = label2binary(classi_data[index])\n",
    "    return classi_data\n",
    "# Convert all the text - divided in tokens - labels to boolean\n",
    "def preprocess_annotation(ann_data):\n",
    "    for index in range(len(ann_data)):\n",
    "        ann_data[index] = 1 if sum(label2binary(ann_data[index])) >= 1 else 0\n",
    "    return ann_data\n",
    "\n",
    "def get_substrings_from_text(text):\n",
    "    substring = text.split(\"\\n\")\n",
    "    return substring\n",
    "\n",
    "def find_complete_text_from_substring(substring, texts):\n",
    "    complete_text = None\n",
    "    count = 0\n",
    "    for text in texts:\n",
    "        if substring[0] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "        elif substring[0][:100] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "        elif substring[0].split('.')[0] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "    if count == 1:\n",
    "        return complete_text\n",
    "    elif count > 1 and len(substring) > 1:\n",
    "        substring.pop(0)\n",
    "        find_complete_text_from_substring(substring, texts)\n",
    "    else:\n",
    "        print('not FOUND', substring[0][:60])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb887f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_metrics(df_data_raw,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path, augmented = False):\n",
    "    df_data_info = df_data_raw[df_data_raw['doc_id'].isin(biobert_ids)].copy().reset_index(drop=True)\n",
    "    df_data_info['truth'] = ''\n",
    "    df_data_info['truth_annotation'] = ''\n",
    "    df_data_info['prediction'] = ''\n",
    "    df_data_info['prediction_annotation'] = ''\n",
    "    for name in file_names:\n",
    "        # print(name)\n",
    "        llama_annotated = ''\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            if augmented:\n",
    "                pred_labels = prediction_to_labels_augmented(llama_annotated['response'],truth_data,doc_id)\n",
    "            else:\n",
    "                pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "            truth_labels = truth_to_labels(truth_data)\n",
    "            # print(doc_id)\n",
    "            pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "            pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "            # print(pred_classification_vector)\n",
    "            # print(\"==========\")\n",
    "            truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "            # print(truth_classification_vector)\n",
    "            # print('######3')\n",
    "            truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "            # print(truth_general_annotation_vector)\n",
    "            \n",
    "\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "    # df_data_info['truth'] = list(df_data_info['truth'])\n",
    "    # df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "    # df_data_info['prediction'] = df_data_info['prediction']\n",
    "    # df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "    # df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "\n",
    "    aux = []\n",
    "    aux2 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    for index in tqdm(range(len(df_data_info))):\n",
    "        truth = eval(df_data_info[\"truth\"][index])\n",
    "        prediction = eval(df_data_info[\"prediction\"][index])\n",
    "        truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "        prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "        value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "        aux.append([value,len(truth)])\n",
    "        value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "        aux2.append([value2,len(truth)])\n",
    "        \n",
    "        # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "        # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "        precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        precision.append([precision_now,len(truth)])\n",
    "\n",
    "        recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        recall.append([recall_now,len(truth)])\n",
    "\n",
    "        f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        f1.append([f1_now,len(truth)])\n",
    "\n",
    "    df_data_info[\"label_score\"] = aux\n",
    "    df_data_info[\"annotation_score\"] = aux2\n",
    "    df_data_info[\"precision\"] = precision\n",
    "    df_data_info[\"recall\"] = recall\n",
    "    df_data_info[\"f1\"] = f1\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n",
    "\n",
    "    #Ponderada\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "\n",
    "    label_score_weight = score/count\n",
    "    row_metrics.append(label_score_weight)\n",
    "    # Mean\n",
    "\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "\n",
    "    label_score_mean = score/count\n",
    "    row_metrics.append(label_score_mean)\n",
    "    # Ponderada\n",
    "\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    annotation_score_weight = score/count\n",
    "    row_metrics.append(annotation_score_weight)\n",
    "    # Mean\n",
    "\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "\n",
    "    annotation_score_mean = score/count\n",
    "    row_metrics.append(annotation_score_mean)\n",
    "    # Ponderada\n",
    "\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    precision_weight = score/count\n",
    "    row_metrics.append(precision_weight)\n",
    "    # Mean\n",
    "\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "\n",
    "    precision_mean = score/count\n",
    "    row_metrics.append(precision_mean)\n",
    "    # Ponderada\n",
    "\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    recall_weight = score/count\n",
    "    row_metrics.append(recall_weight)\n",
    "    # Mean\n",
    "\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "\n",
    "    recall_mean = score/count\n",
    "    row_metrics.append(recall_mean)\n",
    "    # Ponderada\n",
    "\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    f1_weight = score/count\n",
    "    row_metrics.append(f1_weight)\n",
    "    # Mean\n",
    "\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "\n",
    "    f1_mean = score/count\n",
    "    row_metrics.append(f1_mean)\n",
    "\n",
    "    weight_score = annotation_score_weight * label_score_weight\n",
    "    mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "    row_metrics.append(weight_score)\n",
    "\n",
    "    row_metrics.append(mean_score)\n",
    "    return row_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f40c0-bb96-458a-ae3c-f6c374a6d1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24160765-7e51-499b-ba87-118f45cbdb15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "\n",
    "print('starting ',0,'shot ================================')\n",
    "df_data = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "row_metrics = [f'0_shot']\n",
    "path = f'llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "metrics_path = f'metrics-result/biobert-trainig-set/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "metrics_result = return_metrics(df_data,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path)\n",
    "rows_metrics_report.append(metrics_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78304027",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 1 - zero shot annotation augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69cf597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "\n",
    "print('starting ',0,'shot ================================')\n",
    "df_data = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "row_metrics = [f'0_shot_aug']\n",
    "path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "metrics_path = f'metrics-result-augmented/biobert-trainig-set/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "\n",
    "metrics_result = return_metrics(df_data,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path, augmented = True)\n",
    "rows_metrics_report.append(metrics_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bc9cd-868e-448a-8fc2-d40913f88fba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - Static Examples (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce973b-0ecd-4045-839c-e91ef508203b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_static']\n",
    "    path = f'llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result/biobert-trainig-set/temp-0.0/top-p-0.6/ideas-static-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "    metrics_result = return_metrics(df_data_info,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path)\n",
    "    rows_metrics_report.append(metrics_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b864e0f4",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 1 - Static Examples Annotation Augmented (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865ddcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_static_aug']\n",
    "    path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result-augmented/biobert-trainig-set/temp-0.0/top-p-0.6/ideas-static-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "    metrics_result = return_metrics(df_data_info,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path, augmented = True)\n",
    "    rows_metrics_report.append(metrics_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7a8d7b",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 2 - TF-IDF (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e18b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_tf_id']\n",
    "    path = f'llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result/biobert-trainig-set/temp-0.0/top-p-0.6/ideas-tf-idf-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    print(len(os.listdir(path)))\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "    metrics_result = return_metrics(df_data,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path)\n",
    "    rows_metrics_report.append(metrics_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75daab",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 2 - TF-IDF Annotation Augmented (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0486fb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_tf_id_aug']\n",
    "    path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result-augmented/biobert-trainig-set/temp-0.0/top-p-0.6/ideas-tf-idf-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    print(len(os.listdir(path)))\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "    metrics_result = return_metrics(df_data,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path, augmented = True)\n",
    "    rows_metrics_report.append(metrics_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782f9af2",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 3 - TF-IDF Custom (similarity by text VS annotation) (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e41eb99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_tf_id_custom']\n",
    "\n",
    "    path = f'llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result/biobert-trainig-set/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "    metrics_result = return_metrics(df_data_info,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path)\n",
    "    rows_metrics_report.append(metrics_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea75ff6",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 3 - TF-IDF Custom Annotation Augmented (similarity by text VS annotation) (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9547e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_tf_id_custom_aug']\n",
    "\n",
    "    path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result-augmented/biobert-trainig-set/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "    metrics_result = return_metrics(df_data_info,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path, augmented = True)\n",
    "    rows_metrics_report.append(metrics_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690687cd",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 5 - Random Examples (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd832e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_random']\n",
    "    path = f'llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result/biobert-trainig-set/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path) \n",
    "    metrics_result = return_metrics(df_data_info,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path)\n",
    "    rows_metrics_report.append(metrics_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1399177f",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 5 - Random Examples Annotations Augmented (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed11518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_random_aug']\n",
    "    path = f'llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result-augmented/biobert-trainig-set/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path) \n",
    "    metrics_result = return_metrics(df_data_info,row_metrics,file_names,biobert_ids,annotated_dataset,path,metrics_path, augmented = True)\n",
    "    rows_metrics_report.append(metrics_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57357656-8f35-477d-abd8-a0fc9c9f1826",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save and compress metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394fc04-78be-40e8-b8c0-c9cb6cf58cbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448eb28-fea4-443b-a980-2bf8b01c842a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "#Create a DataFrame object\n",
    "df_metrics = pd.DataFrame(rows_metrics_report,\n",
    "                  columns = ['approach','class_weight' , 'class_mean', 'ann_weight' , 'ann_mean','precision_weight','precision_mean','recall_weight','recall_mean','f1_weight','f1_mean','global_weight','global_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fc2dd-ad58-4dc4-b46f-25c273e2e7d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_metrics.to_csv('df_metrics_llama_vs_bert.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the data\n",
    "data = {\n",
    "    'Approach': ['Static examples', 'Random examples', 'Text similarity', 'Labeled entity similarity'],\n",
    "    '0-shot': [None, None, None, None],\n",
    "    '1-shot': [None, None, None, None],\n",
    "    '2-shot': [None, None, None, None],\n",
    "    '3-shot': [None, None, None, None],\n",
    "    '4-shot': [None, None, None, None],\n",
    "    '10-shot': [None, None, None, None]\n",
    "}\n",
    "# Read the metrics data\n",
    "df_metrics = pd.read_csv('df_metrics_llama_vs_bert.csv')\n",
    "\n",
    "# Initialize a dictionary to store the f1_mean values\n",
    "f1_mean_values = {\n",
    "    'Static examples': {},\n",
    "    'Random examples': {},\n",
    "    'Text similarity': {},\n",
    "    'Labeled entity similarity': {}\n",
    "}\n",
    "\n",
    "# Populate the dictionary with f1_mean values\n",
    "for index, row in df_metrics.iterrows():\n",
    "    approach = row['approach']\n",
    "    f1_mean = row['f1_mean']\n",
    "    shots = approach.split('_')[0]\n",
    "    if approach.endswith('shot_static') :\n",
    "        f1_mean_values['Static examples'][shots] = f1_mean\n",
    "    elif approach.endswith('shot_random') :\n",
    "        f1_mean_values['Random examples'][shots] = f1_mean\n",
    "    elif approach.endswith('shot_tf_id'):\n",
    "        f1_mean_values['Text similarity'][shots] = f1_mean\n",
    "    elif approach.endswith('tf_id_custom') :\n",
    "        f1_mean_values['Labeled entity similarity'][shots] = f1_mean\n",
    "\n",
    "# Update the data dictionary with the f1_mean values\n",
    "for approach in f1_mean_values:\n",
    "    for shots, f1_mean in f1_mean_values[approach].items():\n",
    "        data[f'{shots}-shot'][data['Approach'].index(approach)] = f1_mean\n",
    "\n",
    "# Create a DataFrame for augmented approaches\n",
    "augmented_data = {\n",
    "    'Approach': ['Static examples', 'Random examples', 'Text similarity', 'Labeled entity similarity'],\n",
    "    '0-shot': [None, None, None, None],\n",
    "    '1-shot': [None, None, None, None],\n",
    "    '2-shot': [None, None, None, None],\n",
    "    '3-shot': [None, None, None, None],\n",
    "    '4-shot': [None, None, None, None],\n",
    "    '10-shot': [None, None, None, None]\n",
    "}\n",
    "f1_mean_values_aug = {\n",
    "    'Static examples': {},\n",
    "    'Random examples': {},\n",
    "    'Text similarity': {},\n",
    "    'Labeled entity similarity': {}\n",
    "}\n",
    "\n",
    "\n",
    "# Populate the augmented data dictionary with f1_mean values\n",
    "for index, row in df_metrics.iterrows():\n",
    "    approach = row['approach']\n",
    "    f1_mean = row['f1_mean']\n",
    "    shots = approach.split('_')[0]\n",
    "    \n",
    "    if approach.endswith('shot_static_aug'):\n",
    "        f1_mean_values_aug['Static examples'][shots] = f1_mean\n",
    "    elif approach.endswith('shot_random_aug'):\n",
    "        f1_mean_values_aug['Random examples'][shots] = f1_mean\n",
    "    elif approach.endswith('shot_tf_id_aug') and 'custom' not in approach:\n",
    "        f1_mean_values_aug['Text similarity'][shots] = f1_mean\n",
    "    elif approach.endswith('tf_id_custom_aug'):\n",
    "        f1_mean_values_aug['Labeled entity similarity'][shots] = f1_mean\n",
    "for approach in f1_mean_values_aug:\n",
    "    for shots, f1_mean in f1_mean_values_aug[approach].items():\n",
    "        augmented_data[f'{shots}-shot'][augmented_data['Approach'].index(approach)] = f1_mean\n",
    "# Create a DataFrame for augmented data\n",
    "df_augmented = pd.DataFrame(augmented_data)\n",
    "\n",
    "# Save augmented data to CSV\n",
    "df_augmented.to_csv('augmented_skeleton.csv', index=False)\n",
    "# Create a DataFrame\n",
    "df_skeleton = pd.DataFrame(data)\n",
    "\n",
    "# Save to CSV\n",
    "df_skeleton.to_csv('skeleton.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa227796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
