{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab249cb-cc87-4426-89e9-ae42f3ef7b5e",
   "metadata": {},
   "source": [
    "# Self Order Score\n",
    "\n",
    "Algoritmo em quatro passos: (1) ideias vizinhas da mesma categoria são agrupadas -- a posição do grupo é associada à posição do primeiro elemento do mesmo; (2) grupos são ordenados conforme à posição do grupo no texto; (3) agrega grupos por categoria (cada grupo é agregado na primeira posição que aparece) e conta quantas trocas foram necessárias para cada agregação; (4) o total somado de todas as trocas é o índice `self order score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9123e-ac65-402f-836e-8dc71d96632f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_keys(element):\n",
    "  return element[1]\n",
    "\n",
    "def self_order_groups(categories_order):\n",
    "  # sort by text position (second element)\n",
    "  categories_order.sort(key=order_keys) \n",
    "  # print(categories_order)\n",
    "\n",
    "  # group by category (first element)\n",
    "  # group = [category, position of the first group element, score]\n",
    "  grouped = []\n",
    "  for cat in range(1, 9):\n",
    "    prev = -1\n",
    "    prev_g = -1\n",
    "    cat_g = None\n",
    "    for i in range(0, len(categories_order)):\n",
    "      if categories_order[i][0] == cat:\n",
    "        # if any element in the previous position is not in the same category\n",
    "        if prev == -1 or (categories_order[prev][0] != cat and \\\n",
    "           (prev_g == -1 or categories_order[prev][1] > categories_order[prev_g][1])):\n",
    "          cat_g = [cat, categories_order[i][1], 1]  # new category grouping\n",
    "          grouped.append(cat_g)\n",
    "        else:\n",
    "          cat_g[2] += 1\n",
    "        prev_g = i\n",
    "      # last distinct position in the sequence\n",
    "      if i+1 == len(categories_order) or categories_order[i+1][1] != categories_order[i][1]:\n",
    "        prev = i\n",
    "\n",
    "  # sort groups by position (second element)\n",
    "  grouped.sort(key=order_keys)\n",
    "\n",
    "  return grouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5bf77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_order_score(categories_order):\n",
    "\n",
    "  grouped = self_order_groups(categories_order)\n",
    "\n",
    "  # score order change to group together categories\n",
    "  subs = 0\n",
    "  for cat in range(1, 9):\n",
    "    prev = -1\n",
    "    i = 0\n",
    "    while i < len(grouped):\n",
    "      if grouped[i][0] == cat:\n",
    "        if prev == -1:\n",
    "          prev = i\n",
    "        else:\n",
    "          subs += 1\n",
    "          grouped[prev][2] += grouped[i][2]\n",
    "          grouped = grouped[slice(0, i)] + grouped[slice(i+1, len(grouped))]\n",
    "      i += 1\n",
    "\n",
    "  return subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63172325",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(self_order_groups([[1, 10], [2, 20], [1, 20], [2, 30]]))\n",
    "print(self_order_groups([[1, 10], [2, 20], [1, 25], [2, 30]]))\n",
    "print(self_order_groups([[1, 10], [2, 20], [1, 20], [3, 20], [1, 30], [2, 30], [3, 30]]))\n",
    "print(self_order_groups([[2, 71], [2, 96], [3, 98], [2, 100], [5, 120], [5, 130], [5, 135], [3, 140], [5, 180]]))\n",
    "print(self_order_groups([[5, 135], [2, 100], [2, 71], [2, 96], [5, 130], [3, 98], [5, 180], [5, 120], [3, 140]]))\n",
    "print(self_order_groups([[2, 71], [2, 96], [3, 98], [2, 98], [5, 98], [5, 130], [5, 135], [3, 140], [5, 180]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5b06a9-d72a-4edb-ba39-e5c30403fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(self_order_score([[1, 10], [2, 20], [1, 20], [2, 30]]))\n",
    "print(self_order_score([[1, 10], [2, 20], [1, 25], [2, 30]]))\n",
    "print(self_order_score([[1, 10], [2, 20], [1, 20], [3, 20], [1, 30], [2, 30], [3, 30]]))\n",
    "print(self_order_score([[2, 71], [2, 96], [3, 98], [2, 100], [5, 120], [5, 130], [5, 135], [3, 140], [5, 180]]))\n",
    "print(self_order_score([[5, 135], [2, 100], [2, 71], [2, 96], [5, 130], [3, 98], [5, 180], [5, 120], [3, 140]]))\n",
    "print(self_order_score([[2, 71], [2, 96], [3, 98], [2, 98], [5, 98], [5, 130], [5, 135], [3, 140], [5, 180]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435fb83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'pressao',['patho',5],['physio',5],[''], 'pressao',['patho','history'], 'pressao',['patho','history']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2a6d30-ca70-48ae-b0cf-fa17c23c0084",
   "metadata": {},
   "source": [
    "# Clustering in Free Recall\n",
    "\n",
    "Algoritmo de category clustering em free recall conforme descrito em https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3665324/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0884a48e-bb18-4248-963b-d9844796e35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clustering_free_recall(categories_order):\n",
    "  n = len(categories_order)  # number of recalled items\n",
    "\n",
    "  # sort by text position (second element)\n",
    "  categories_order.sort(key=order_keys)\n",
    "\n",
    "  nc = {}  # number of recalled items in each recalled category\n",
    "  r = 0  # number of category repetition\n",
    "  for i in range(0, len(categories_order)): \n",
    "    cat = categories_order[i][0]\n",
    "    if not cat in nc:\n",
    "      nc[cat] = 1\n",
    "    else:\n",
    "      nc[cat] += 1\n",
    "    next_pos = i + 1\n",
    "    # find next position of the same category or neighbor start\n",
    "    while next_pos < len(categories_order) and \\\n",
    "          categories_order[next_pos][0] != cat and categories_order[next_pos][1] == categories_order[i][1]:\n",
    "      next_pos += 1\n",
    "    if next_pos < len(categories_order):\n",
    "      sp = next_pos\n",
    "      while sp < len(categories_order) and categories_order[sp][1] == categories_order[next_pos][1]:\n",
    "        if cat == categories_order[sp][0]:\n",
    "          r += 1\n",
    "          break\n",
    "        sp += 1\n",
    "\n",
    "  c = len(nc)  # number of recalled categories\n",
    "  max = n - c  # maximum possible number of category repetitions\n",
    "\n",
    "  er = 0  # expected number of category repetitions\n",
    "  for cat in nc:\n",
    "    er += nc[cat] * nc[cat]\n",
    "  er = er / n - 1\n",
    "\n",
    "  # rr = r / (n - 1)  # ratio of repetition\n",
    "\n",
    "  # mrr = r / max  # modified ratio of repetition\n",
    "\n",
    "  # ds = r - er  # deviation score\n",
    "\n",
    "  # adjusted ratio of clustering\n",
    "  arc = ('' + str(r - er)) + ('/' + str(max - er)) if max - er == 0 else (r - er) / (max - er)\n",
    "\n",
    "  return arc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017556b0-96e9-4586-9f07-27c1830f9e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(clustering_free_recall([[2, 1], [4, 2], [4, 3], [3, 4], [2, 5], [3, 6], [1, 7], [4, 8], [4, 9]]))\n",
    "print(clustering_free_recall([[3, 1], [4, 2], [4, 3], [3, 4], [1, 5], [1, 6], [3, 7], [1, 8], [1, 9], [2, 10], [2, 11], [2, 12], [4, 13], [4, 14], [3, 15]]))\n",
    "print(clustering_free_recall([[2, 1], [2, 2], [3, 3], [1, 4], [1, 5], [1, 6], [1, 7], [2, 8], [3, 9], [3, 10], [2, 11], [1, 12], [4, 13], [4, 14], [4, 15], [4, 16], [2, 17], [2, 18], [3, 19], [1, 20]]))\n",
    "print(clustering_free_recall([[5, 135], [2, 100], [2, 71], [2, 96], [5, 130], [3, 98], [5, 180], [5, 120], [3, 140]]))\n",
    "print(clustering_free_recall([[2, 71], [2, 96], [3, 98], [2, 98], [5, 98], [5, 130], [5, 135], [3, 140], [5, 180]]))\n",
    "print(clustering_free_recall([[2, 71], [2, 96], [3, 98], [2, 98], [5, 98], [7,98], [5, 130], [5, 135], [3, 140], [5, 180]]))\n",
    "print(clustering_free_recall([[2, 1], [5, 2], [2, 3], [3, 4], [1, 5], [5, 6]]))\n",
    "print(clustering_free_recall([[1, 1], [1, 2], [1, 3], [2, 4]]))\n",
    "print(clustering_free_recall([[7,1],[3,2],[7,3],[3,4],[2,5],[2,6]]))\n",
    "print(clustering_free_recall([[1,1],[1,2],[5,3],[2,4],[8,5],[1,6],[1,7]]))\n",
    "print(clustering_free_recall([[5,1], [5,2], [5,3]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6a3c8a",
   "metadata": {},
   "source": [
    "# Transforming Llama answer for clustering and self order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653b2c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install fuzzywuzzy\n",
    "%pip install levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e3bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "from difflib import SequenceMatcher\n",
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "import unicodedata\n",
    "import ast\n",
    "\n",
    "def get_examples(file_path):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        data = [line for line in reader]\n",
    "    return data\n",
    "def get_from_annotated_dataset(annotated_dataset,_id):\n",
    "    for doc in annotated_dataset:\n",
    "        if doc['doc_id'] == _id:\n",
    "            return doc\n",
    "annotated_dataset = get_examples('../annotations-medical_specialist-dpoc-bio-composed-multiple.jsonl')\n",
    "df_data_info = pd.read_csv('../annotations_medical_specialist_pre_processed.csv')\n",
    "\n",
    "#REMOVE ADDITIONAL BAD DATA\n",
    "# df_data_info.drop(df_data_info[df_data_info['doc_id'] == '8396380d-e0b6-4b81-8fe9-0b99c611f9f3'].index, inplace=True)\n",
    "# df_data_info.reset_index(drop=True,inplace=True)\n",
    "def replace_substring(string, start, end, replacement):\n",
    "    # Check if start and end are valid indices for the string\n",
    "    if start < 0 or end > len(string) or start > end:\n",
    "        return \"Invalid start or end index\"\n",
    "\n",
    "    # Replace the substring from start to end with the replacement string\n",
    "    new_string = string[:start] + replacement + string[end:]\n",
    "\n",
    "    return new_string\n",
    "\n",
    "def add_quotes(item):\n",
    "    if (not (item.startswith('\"') and item.endswith('\"')) and not (item.startswith(\"'\") and item.endswith(\"'\"))):\n",
    "        if (item.startswith('\"') and not item.endswith('\"')) or (item.startswith(\"'\") and not item.endswith(\"'\")):\n",
    "            item = item.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n",
    "    return item\n",
    "def fix_quotes(text):\n",
    "    # Regular expression to find lists and their items   \n",
    "    pattern = r\"\"\"\\[\\s*(([\"'][^\"']+[\"'])(?:\\s+[^,]+)?)\\s*,\\s*(\\[\\s*[\"'][^\"']+?[\"'](?:\\s*,\\s*[\"'][^\"']+?[\"'])*\\s*\\])\\s*\\]\"\"\"\n",
    "    annotation_list = {\"annotations\":[]}\n",
    "    for match in re.finditer(pattern, text):\n",
    "        main_text = add_quotes(match.group(1))\n",
    "        if main_text.startswith('\"') and main_text.endswith('\"') or (main_text.startswith(\"'\") and main_text.endswith(\"'\")):\n",
    "            main_text = main_text[1:-1]\n",
    "        current_list = [str(match.group(1)), ast.literal_eval(match.group(3))]\n",
    "        current_list[0] = current_list[0][1:-1]\n",
    "        annotation_list[\"annotations\"].append(current_list)\n",
    "    annotation_list = str(annotation_list)\n",
    "    return annotation_list\n",
    "\n",
    "def select_after_first_brace(string):\n",
    "    pattern = r\"(|[\\'\\\"])(annotations)([\\'\\\"]|)\"\n",
    "    \n",
    "    matches = re.search(pattern, string)\n",
    "    if matches != None:\n",
    "        string = replace_substring(string, matches.span()[0], matches.span()[1], '\"annotations\"')\n",
    "    string.replace(\"]\\']\",']]')\n",
    "    string.replace(\"]\\\"]\",']]')\n",
    "    # print('STRING AFTER FIRST REGEX:', string)\n",
    "    pattern = r'\"annotations\":\\[.*\\]\\]'\n",
    "    match = re.search(pattern, string)\n",
    "    if match != None:\n",
    "        string = replace_substring(string, match.span()[0], match.span()[1], '\"annotations\":[')\n",
    "        string = match.group(0)\n",
    "        # print('STRING AFTER SECOND REGEX:', string)\n",
    "\n",
    "    brace_index = string.find('\"annotations\"')\n",
    "    # print('trying to select correct part of models output')\n",
    "    # print(brace_index)\n",
    "    # print(string)\n",
    "    # if brace_index == -1:\n",
    "    string = fix_quotes(string)\n",
    "    # print('strrrriiing\\n\\n')\n",
    "    # print(string)\n",
    "    brace_index = string.find('\"annotations\"')\n",
    "    \n",
    "    if string[-3:] == \"']]\":\n",
    "        string = string[:-3]+\"']]]\"\n",
    "    if string.find(\"}\") == -1:\n",
    "        end_annotation_index = string.find(']]]')\n",
    "        if string[-1:] != ']' and end_annotation_index != -1:                \n",
    "            string = string[:end_annotation_index+3]+'}'\n",
    "        elif string[-1:] != ']' and end_annotation_index == -1:\n",
    "            end_annotation_index = string.find(']] ]')\n",
    "            if end_annotation_index != -1:\n",
    "                string = string[:end_annotation_index+4]+'}'\n",
    "            else:\n",
    "                string = string+']}'\n",
    "        elif string[-7:].count(']') < 3:\n",
    "            string = string+']}'\n",
    "        else:\n",
    "            string = string+'}'\n",
    "    else:\n",
    "        string = string[:string.find(\"}\")+1]\n",
    "    # print('results...')\n",
    "    # print(string)\n",
    "    string = fix_quotes(string)\n",
    "    return string\n",
    "\n",
    "def escape_inner_quotes(text):\n",
    "    # pattern = r\"\"\"(?<=:).*[\"'](.*)(?<!\\\\)[\"'](?=\\s*[,}])\"\"\"\n",
    "    pattern = r\"\"\"(?<=[:])[\\s\\S]*?[\"'](.*)(?<!\\\\)[\"'](?=\\s*[}])\"\"\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    # escaped_text = \"'\"+matches[0].replace('\"', '\\\\\"')+\"'\"\n",
    "    escaped_text = re.sub(pattern, lambda m: \"'\" + m.group(1).replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\") + \"'\", text)\n",
    "    return escaped_text\n",
    "####### Function that extracts the entities and respective categories and transforms into a dictionary of annotations  #############\n",
    "def transform_augmented_data_to_pattern(data_info):\n",
    "\n",
    "    data_info = data_info.replace('```','')\n",
    "    data_info = re.sub(r'(?<![\"\\\\]):', r'\\\\:', data_info)\n",
    "    data_info = data_info.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    \n",
    "    data_info = re.sub(r\"\"\"[\"']*annotations[\"']*:\"\"\", r'\"annotations\":', data_info)\n",
    "    ann_pattern = r\"\"\"\"annotations\":\\s*[\"']*{*(.*)(?=\\s*[\"']*\\s*[}]*)\"\"\"\n",
    "    match = re.search(ann_pattern, data_info)\n",
    "    if match:\n",
    "        raw_data = data_info\n",
    "        data_info = f'\"annotations\": \"{match.group(1)}\"'\n",
    "        if data_info[-1:] == \"'\" or data_info[-1:] == '\"':\n",
    "            data_info = data_info[:-1]\n",
    "    else:\n",
    "        data_info = \"\"\" \"annotations\":{\"\"} \"\"\"\n",
    "    closing_braces = data_info.find(\"}\")\n",
    "    opening_braces = data_info.find(\"{\")\n",
    "    if opening_braces == -1:\n",
    "        data_info = '{'+ data_info\n",
    "    elif data_info[:1] != '{':\n",
    "        data_info = '{'+ data_info\n",
    "    if closing_braces == -1:\n",
    "        data_info = data_info[:len(data_info)] + '}'\n",
    "        closing_braces = data_info.find(\"}\")\n",
    "    if (data_info.find('\"}') == -1 and data_info.find('\" }') == -1):\n",
    "        data_info = data_info[:closing_braces] + '\"}'\n",
    "    data_info = escape_inner_quotes(data_info)\n",
    "    # print('after escape inner quotes', data_info)\n",
    "    if data_info[-3] == '\\\\':\n",
    "        data_info = data_info[:len(data_info)-3]+data_info[-2:len(data_info)+1]\n",
    "    data_info = ast.literal_eval(data_info)\n",
    "    data_info['annotations'] = data_info['annotations'].replace('\\\\','')\n",
    "\n",
    "    pattern = r'\\[([^\\[\\]]+)\\s*\\|\\s*([^\\[\\]]+)\\]'\n",
    "    matches = re.findall(pattern, data_info['annotations'])\n",
    "    if len(matches) == 0:\n",
    "        print(\"---------------------------------Error: No matches found in the data.--------------------------\")\n",
    "        print(data_info)\n",
    "        annotations = {\"annotations\": []}\n",
    "    else:\n",
    "        annotations = {\"annotations\": [[match[0].strip(), [val.strip() for val in match[1].split(',')]] for match in matches]}\n",
    "    return annotations\n",
    "\n",
    "# prediction_annotation = eval(model_response[0])\n",
    "def prediction_to_labels(prediction_labels, data_info):\n",
    "    prediction_labels = transform_augmented_data_to_pattern(prediction_labels)\n",
    "    if type(prediction_labels) == list:\n",
    "        ze = prediction_labels[0]\n",
    "        prediction_labels = ze\n",
    "    prediction_labels = select_after_first_brace(prediction_labels)\n",
    "    prediction_annotation = eval(prediction_labels)\n",
    "    full_text = data_info['text']\n",
    "    text_tokenized = data_info['labels']\n",
    "    categorized_prediction = annotation_to_tokens(full_text, text_tokenized, prediction_annotation)\n",
    "    labels = extract_labels_from_prediction(categorized_prediction)\n",
    "    return labels\n",
    "def truth_to_labels(data_info):\n",
    "    labels = extract_labels_from_truth(data_info['labels'])\n",
    "    return labels\n",
    "def extract_labels_from_truth (data_info):\n",
    "    text_tokenized = data_info\n",
    "    categories = []\n",
    "    for token in text_tokenized:\n",
    "        if token[4] != None:\n",
    "            categories.append(list(token[4].keys()))\n",
    "        else:\n",
    "            categories.append('0')\n",
    "    return categories\n",
    "def annotation_to_tokens (full_text, text_tokenized, prediction_annotation):\n",
    "    clean_text_tokenized = [[token[0],token[1],token[2]] for token in text_tokenized]\n",
    "    annotations = prediction_annotation['annotations']\n",
    "    for annotation in annotations:\n",
    "        # print('============= new annotation', annotation[0])\n",
    "        start_pos = full_text.find(annotation[0])\n",
    "        end_pos = len(annotation[0])+start_pos-1\n",
    "        # print(f'end pos is {len(annotation)} + {start_pos} - 1 = {end_pos}')\n",
    "        categorizing = False\n",
    "        \n",
    "        for token in clean_text_tokenized:\n",
    "            # print(f'token pos {token[1]} annotation pos {start_pos} token {token[0]}')\n",
    "            if token[1] == start_pos:\n",
    "                # print('starting categorization...')\n",
    "                # print(f'start pos {start_pos} end pos {end_pos} token {token[0]}')\n",
    "                categorizing = True\n",
    "            if categorizing:\n",
    "                # adds category to token\n",
    "                # print('====',annotation)\n",
    "                # print(f'adding category {annotation[1]} to token {token[0]}')\n",
    "                token.append(annotation[1])\n",
    "                if token[2] == end_pos:\n",
    "                    # print(f'ending categorization at {token[0]}...')\n",
    "                    categorizing = False\n",
    "                    break\n",
    "            \n",
    "            # print(token)\n",
    "    return clean_text_tokenized\n",
    "def extract_labels_from_prediction (categorized_prediction):\n",
    "    labels = []\n",
    "    for token in categorized_prediction:\n",
    "        if len(token) > 3:\n",
    "            labels.append(token[3])\n",
    "        else:\n",
    "            labels.append('0')\n",
    "    return labels\n",
    "\n",
    "def remove_upper_and_accents(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[áàâãä]', 'a', text)\n",
    "    text = re.sub(r'[éèêë]', 'e', text)\n",
    "    text = re.sub(r'[íìîï]', 'i', text)\n",
    "    text = re.sub(r'[óòôõö]', 'o', text)\n",
    "    text = re.sub(r'[úùûü]', 'u', text)\n",
    "    text = re.sub(r'[ç]', 'c', text)\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub('  ', ' ', text)\n",
    "\n",
    "    return text\n",
    "def normalize_text(text):\n",
    "    # Normalize text to remove accents and special characters\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def group_category_for_score_calc(annotation_list, llama_txt):\n",
    "    llama_txt = remove_upper_and_accents(llama_txt)\n",
    "    llama_immutable = llama_txt\n",
    "    cat_to_int = {\n",
    "        'pathophysiology':1, \n",
    "        'etiology':2,\n",
    "        'epidemiology':3,\n",
    "        'history':4,\n",
    "        'physical':5,\n",
    "        'exams':6,\n",
    "        'differential':7,\n",
    "        'therapeutic':8\n",
    "    }\n",
    "    categories_order = []\n",
    "    previous_len = 0\n",
    "    findings = []\n",
    "    not_found = 0\n",
    "    stop = False\n",
    "    for annotation in annotation_list:\n",
    "        processed_annotation = remove_upper_and_accents(annotation[0])\n",
    "        best_match = find_best_match(processed_annotation, llama_txt)\n",
    "        start_pos = llama_txt.find(best_match)\n",
    "        for cat in annotation[1]:\n",
    "            if cat != '0':\n",
    "                # print('adding', cat, processed_annotation)\n",
    "                if cat in cat_to_int:\n",
    "                    # print(annotation)\n",
    "                    # findings.append(('finding idea...', best_match,'pos', start_pos,(start_pos + len(best_match))))\n",
    "                    # findings.append(('================================= dynamic text',llama_txt))\n",
    "                    # print('finding idea...', best_match,'pos', start_pos,start_pos + len(annotation))\n",
    "                    # print('================================= \\n',llama_txt)\n",
    "\n",
    "                    if start_pos == -1:\n",
    "                        findings.append(llama_immutable)\n",
    "                        findings.append(('start pos not found', best_match))\n",
    "                        not_found += 1\n",
    "                        print('================================= \\n',llama_txt,'=================================')\n",
    "                        print('========== start pos not found', processed_annotation)\n",
    "                        print('========= match',  best_match)\n",
    "                    # print('start', start_pos, 'preivous', previous_len)\n",
    "                    # print([cat_to_int[cat], start_pos+previous_len])\n",
    "                    categories_order.append([cat_to_int[cat], start_pos+previous_len])\n",
    "        if not stop:\n",
    "            with open(f'print_finding_position.txt', 'a', encoding='utf8') as f:\n",
    "                for finding in findings:\n",
    "                    f.write(f\"{finding}\\n\")\n",
    "            stop = True\n",
    "        end_pos = start_pos + len(processed_annotation)\n",
    "        llama_txt = llama_txt[end_pos:]\n",
    "        previous_len += end_pos   \n",
    "    if not_found > 0:\n",
    "        print('not found:', not_found)  \n",
    "    return categories_order\n",
    "\n",
    "def find_num_category(annotation_list):\n",
    "    category_list = []\n",
    "    for cat in annotation_list:\n",
    "        for c in cat[1]:\n",
    "            category_list.append(c)\n",
    "    # print(category_list)\n",
    "    distinct_values = list(set(category_list))\n",
    "    return len(distinct_values)\n",
    "\n",
    "def format_annotation_to_self_order(llama_annotated):\n",
    "    # pred_annotations = eval(select_after_first_brace(llama_annotated['response']))\n",
    "    formatted__annotations = []\n",
    "    for annotation in pred_annotations:\n",
    "        start_pos = llama_annotated['text'].find(annotation)\n",
    "        end_pos = start_pos + len(annotation)\n",
    "        current_txt = current_txt[end_pos:]\n",
    "        formatted__annotations.append([annotation, start_pos, end_pos])\n",
    "def gen_comparation_clustering_self_order_llm_vs_medical_specialist(path, approach):\n",
    "\n",
    "    teste_progresso = pd.read_csv('../teste-progresso/resultados_anotacoes_teste_progresso_dpoc.csv')\n",
    "\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "    medical_specialist_metrics = teste_progresso[['annotation id', 'self order groups', 'self order score', 'clustering in free recall']]\n",
    "    medical_specialist_metrics.columns = ['doc_id', 'self_order_groups', 'self_order_medical_specialist', 'clustering_medical_specialist']\n",
    "\n",
    "    for name in file_names:\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            # print(doc_id)\n",
    "            formatted_prediction = eval(select_after_first_brace(llama_annotated['response']))\n",
    "            pred_labels = formatted_prediction['annotations']\n",
    "            current_txt = llama_annotated['text']\n",
    "            ideas_number = len(formatted_prediction['annotations'])\n",
    "            cat_number = find_num_category(pred_labels)\n",
    "            # print(f\"Position of 'BLOQUEIO DO AR NOS PULMÕES': {start_pos}\")\n",
    "            # print(llama_annotated['text'])\n",
    "            # print(len(formatted_prediction['annotations']))\n",
    "            # print(len(annotation_to_int), type(len(annotation_to_int)), len(annotation_to_int)>2)\n",
    "            # print(pred_labels)\n",
    "            # print(group_category_for_score_calc(pred_labels, llama_annotated['text']))\n",
    "            \n",
    "            if len(formatted_prediction['annotations']) > 0:\n",
    "                annotation_to_int = group_category_for_score_calc(pred_labels, llama_annotated['text'])\n",
    "                self_order = int(self_order_score(annotation_to_int))\n",
    "                self_order_group_list = int(self_order_groups(annotation_to_int))\n",
    "                clustering = clustering_free_recall(annotation_to_int)\n",
    "                medical_specialist_metrics.loc[medical_specialist_metrics['doc_id'] == doc_id, 'self_order_llm'] = self_order\n",
    "                medical_specialist_metrics.loc[medical_specialist_metrics['doc_id'] == doc_id, 'clustering_llm'] = clustering\n",
    "\n",
    "                medical_specialist_metrics.to_csv(f'detailed/detailed_self_order_clustering_metrics_medical_specialist_vs_{approach}.csv', index=False)\n",
    "\n",
    "def transform_int_to_named_cat(int_cat):\n",
    "    int_to_cat = {\n",
    "        1: 'pathophysiology', \n",
    "        2: 'etiology',\n",
    "        3: 'epidemiology',\n",
    "        4: 'history',\n",
    "        5: 'physical',\n",
    "        6: 'exams',\n",
    "        7: 'differential',\n",
    "        8: 'therapeutic'\n",
    "    }\n",
    "    self_order_translated = []\n",
    "\n",
    "    for i in int_cat:\n",
    "        cat = int_to_cat[i[0]]\n",
    "        rep_cat = i[2]\n",
    "        self_order_translated.append(f'{cat}:{i[1]}/{rep_cat};')\n",
    "    self_order_translated_str = ' '.join(self_order_translated)\n",
    "    return self_order_translated_str\n",
    "def gen_augmented_annotation_self_order_group_named(path, approach):\n",
    "\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    self_order_group_results = pd.DataFrame(columns=['annotation id', 'categories ordered'])\n",
    "    for name in file_names:\n",
    "        doc_id = name[-41:-5]\n",
    "        # print(doc_id)\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            formatted_prediction = transform_augmented_data_to_pattern(llama_annotated['response'])\n",
    "            if formatted_prediction['annotations'] == []:\n",
    "                print(doc_id)\n",
    "                print(formatted_prediction)\n",
    "                \n",
    "            if len(formatted_prediction) == 0:\n",
    "                print(doc_id,'====================================================')\n",
    "            # print('======================')\n",
    "            # if formatted_prediction[-1] =! '{':\n",
    "            #     formatted_prediction = '{'+formatted_prediction\n",
    "            # formatted_prediction = eval(select_after_first_brace(llama_annotated))\n",
    "            pred_labels = formatted_prediction['annotations']\n",
    "            current_txt = llama_annotated['text']\n",
    "            ideas_number = len(formatted_prediction['annotations'])\n",
    "            cat_number = find_num_category(pred_labels)\n",
    "            \n",
    "            if len(formatted_prediction['annotations']) > 0:\n",
    "                annotation_to_int = group_category_for_score_calc(pred_labels, llama_annotated['text'])\n",
    "                self_order_group_list = transform_int_to_named_cat(self_order_groups(annotation_to_int))\n",
    "                # print(self_order_group_list)\n",
    "                \n",
    "                new_row = pd.DataFrame({'annotation id': [doc_id], 'categories ordered': [self_order_group_list]})\n",
    "                self_order_group_results = pd.concat([self_order_group_results, new_row], ignore_index=True)\n",
    "                self_order_group_results.to_csv(f'teste_progresso_self_order_groups_{approach}.csv', index=False)\n",
    "\n",
    "def gen_self_order_group_named(path, approach):\n",
    "\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    self_order_group_results = pd.DataFrame(columns=['annotation id', 'categories ordered'])\n",
    "    for name in file_names:\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            formatted_prediction = select_after_first_brace(llama_annotated['response'])\n",
    "            # print('##############################\\n')\n",
    "            # print(formatted_prediction)\n",
    "            formatted_prediction = eval(formatted_prediction)\n",
    "            \n",
    "            pred_labels = formatted_prediction['annotations']\n",
    "            current_txt = llama_annotated['text']\n",
    "            ideas_number = len(formatted_prediction['annotations'])\n",
    "            cat_number = find_num_category(pred_labels)\n",
    "        \n",
    "        \n",
    "            if len(formatted_prediction['annotations']) > 0:\n",
    "                annotation_to_int = group_category_for_score_calc(pred_labels, llama_annotated['text'])\n",
    "                self_order_group_list = transform_int_to_named_cat(self_order_groups(annotation_to_int))\n",
    "                # print(self_order_group_list)\n",
    "                \n",
    "                new_row = pd.DataFrame({'annotation id': [doc_id], 'categories ordered': [self_order_group_list]})\n",
    "            else:\n",
    "                print('No annotations found for', doc_id)\n",
    "                print(llama_annotated['response'],'\\n')\n",
    "                print(formatted_prediction,'\\n=================')\n",
    "                print('==============================================\\n')\n",
    "                new_row = pd.DataFrame({'annotation id': [doc_id], 'categories ordered': ['']})\n",
    "            self_order_group_results = pd.concat([self_order_group_results, new_row], ignore_index=True)\n",
    "            self_order_group_results.to_csv(f'teste_progresso_self_order_groups_{approach}.csv', index=False)\n",
    "\n",
    "#     return best_match\n",
    "def normalize_text(text):\n",
    "    # Normalize text to remove accents and special characters\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', text)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "def find_best_match(phrase, text):\n",
    "    normalized_phrase = remove_upper_and_accents(phrase.lower())\n",
    "    normalized_text = remove_upper_and_accents(text.lower())\n",
    "    \n",
    "    # Tokenize the normalized phrase and text\n",
    "    # phrase_tokens = re.findall(r'\\w+|[^\\w\\s]', normalized_phrase, re.UNICODE)\n",
    "    # text_tokens = re.findall(r'\\w+|[^\\w\\s]', normalized_text, re.UNICODE)\n",
    "    phrase_tokens = normalized_phrase.split()\n",
    "    text_tokens = normalized_text.split()\n",
    "    \n",
    "    best_match_tokens = []\n",
    "    best_match = \"\"\n",
    "    best_score = 0\n",
    "    \n",
    "    # Define the range of lengths to consider for text slices\n",
    "    min_length = max(1, len(phrase_tokens) - 1)\n",
    "    max_length = len(phrase_tokens) + 1\n",
    "    \n",
    "    for length in range(min_length, max_length + 1):\n",
    "        for i in range(len(text_tokens) - length + 1):\n",
    "            # Extract a slice of the text tokens\n",
    "            text_slice = text_tokens[i:i + length]\n",
    "            text_slice_str = ''\n",
    "            text_slice_str = \" \".join(text_slice)\n",
    "            # print(text_slice)\n",
    "            # print('VS ==== ',\" \".join(phrase_tokens))\n",
    "            # Calculate the fuzzy matching score\n",
    "            score = fuzz.ratio(\" \".join(phrase_tokens), text_slice_str)\n",
    "            # Update the best match if the current score is higher\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = text_slice_str\n",
    "                best_match_tokens = text_slice\n",
    "    \n",
    "\n",
    "    return best_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"nos seus pulmoes\"\n",
    "text = \"nos pulmoes é comum um caso de filler pulmoes nos é diars\"\n",
    "best_match = find_best_match(phrase, text)\n",
    "print(f\"Best match: {best_match}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt = 'A doença pulmonar obstrutiva cronica é uma obstrução pulmonar irreversivel, caracterizada pela clinica de tosse cronica, expectoração cornica'\n",
    "test_match = 'Doença pulmonar obstrutiva crônica (DPOC)'\n",
    "print(find_best_match(test_match, test_txt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc5af71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "path_0_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "path_0_shot_raw_typos = f'../llama-outputs/raw-typos/ideas-0-shot'\n",
    "\n",
    "path_1_static_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-1-shot'\n",
    "path_2_static_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-2-shot'\n",
    "path_3_static_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-3-shot'\n",
    "path_4_static_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-4-shot'\n",
    "path_10_static_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-10-shot'\n",
    "\n",
    "path_1_tf_idf_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-1-shot'\n",
    "path_2_tf_idf_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-2-shot'\n",
    "path_3_tf_idf_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "path_4_tf_idf_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-4-shot'\n",
    "path_4_tf_idf_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-10-shot'\n",
    "\n",
    "path_1_tf_idf_custom_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-1-shot'\n",
    "path_2_tf_idf_custom_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-2-shot'\n",
    "path_3_tf_idf_custom_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-3-shot'\n",
    "path_4_tf_idf_custom_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-4-shot'\n",
    "path_10_tf_idf_custom_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-10-shot'\n",
    "\n",
    "path_1_random_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-1-shot'\n",
    "path_2_random_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-2-shot'\n",
    "path_3_random_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-3-shot'\n",
    "path_4_random_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-4-shot'\n",
    "path_10_random_shot = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-10-shot'\n",
    "\n",
    "print('generating ZERO-SHOT')\n",
    "gen_self_order_group_named(path_0_shot, '0_shot')\n",
    "\n",
    "print('generating ONE-SHOT STATIC')\n",
    "gen_self_order_group_named(path_1_static_shot, '1_static_shot')\n",
    "print('generating TWO-SHOT STATIC')\n",
    "gen_self_order_group_named(path_2_static_shot, '2_static_shot')\n",
    "print('generating THREE-SHOT STATIC')\n",
    "gen_self_order_group_named(path_3_static_shot, '3_static_shot')\n",
    "print('generating FOUR-SHOT STATIC')\n",
    "gen_self_order_group_named(path_4_static_shot, '4_static_shot')\n",
    "print('generating TEN-SHOT STATIC')\n",
    "gen_self_order_group_named(path_10_static_shot, '10_static_shot')\n",
    "\n",
    "\n",
    "print('generating ONE-SHOT TF-IDF')\n",
    "gen_self_order_group_named(path_1_tf_idf_shot, '1_tf_idf_shot')\n",
    "print('generating TWO-SHOT TF-IDF')\n",
    "gen_self_order_group_named(path_2_tf_idf_shot, '2_tf_idf_shot')\n",
    "print('generating THREE-SHOT TF-IDF')\n",
    "gen_self_order_group_named(path_3_tf_idf_shot, '3_tf_idf_shot')\n",
    "print('generating FOUR-SHOT TF-IDF')\n",
    "gen_self_order_group_named(path_4_tf_idf_shot, '4_tf_idf_shot')\n",
    "print('generating TEN-SHOT TF-IDF')\n",
    "gen_self_order_group_named(path_4_tf_idf_shot, '10_tf_idf_shot')\n",
    "\n",
    "print('generating ONE-SHOT TF-IDF CUSTOM')\n",
    "gen_self_order_group_named(path_1_tf_idf_custom_shot, '1_tf_idf_custom_shot')\n",
    "print('generating TWO-SHOT TF-IDF CUSTOM')\n",
    "gen_self_order_group_named(path_2_tf_idf_custom_shot, '2_tf_idf_custom_shot')\n",
    "print('generating THREE-SHOT TF-IDF CUSTOM')\n",
    "gen_self_order_group_named(path_3_tf_idf_custom_shot, '3_tf_idf_custom_shot')\n",
    "print('generating FOUR-SHOT TF-IDF CUSTOM')\n",
    "gen_self_order_group_named(path_4_tf_idf_custom_shot, '4_tf_idf_custom_shot')\n",
    "print('generating TEN-SHOT TF-IDF CUSTOM')\n",
    "gen_self_order_group_named(path_10_tf_idf_custom_shot, '10_tf_idf_custom_shot')\n",
    "\n",
    "print('generating ONE-SHOT STATIC')\n",
    "gen_self_order_group_named(path_1_random_shot, '1_random_shot')\n",
    "print('generating TWO-SHOT random')\n",
    "gen_self_order_group_named(path_2_random_shot, '2_random_shot')\n",
    "print('generating THREE-SHOT random')\n",
    "gen_self_order_group_named(path_3_random_shot, '3_random_shot')\n",
    "print('generating FOUR-SHOT random')\n",
    "gen_self_order_group_named(path_4_random_shot, '4_random_shot')\n",
    "print('generating TEN-SHOT random')\n",
    "gen_self_order_group_named(path_10_random_shot, '10_random_shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550e3ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_0_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "\n",
    "path_1_static_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-1-shot'\n",
    "path_2_static_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-2-shot'\n",
    "path_3_static_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-3-shot'\n",
    "path_4_static_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-4-shot'\n",
    "path_10_static_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-10-shot'\n",
    "\n",
    "path_1_tf_idf_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-1-shot'\n",
    "path_2_tf_idf_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-2-shot'\n",
    "path_3_tf_idf_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "path_4_tf_idf_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-4-shot'\n",
    "path_4_tf_idf_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-10-shot'\n",
    "\n",
    "path_1_tf_idf_custom_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-1-shot'\n",
    "path_2_tf_idf_custom_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-2-shot'\n",
    "path_3_tf_idf_custom_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-3-shot'\n",
    "path_4_tf_idf_custom_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-4-shot'\n",
    "path_10_tf_idf_custom_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-10-shot'\n",
    "\n",
    "path_1_random_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-1-shot'\n",
    "path_2_random_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-2-shot'\n",
    "path_3_random_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-3-shot'\n",
    "path_4_random_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-4-shot'\n",
    "path_10_random_shot_augmented = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-10-shot'\n",
    "\n",
    "print('generating ZERO-SHOT STATIC augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_0_shot_augmented, '0_shot_aug')\n",
    "\n",
    "print('generating ONE-SHOT STATIC augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_1_static_shot_augmented, '1_static_shot_aug')\n",
    "print('generating TWO-SHOT STATIC augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_2_static_shot_augmented, '2_static_shot_aug')\n",
    "print('generating THREE-SHOT STATIC augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_3_static_shot_augmented, '3_static_shot_aug')\n",
    "print('generating FOUR-SHOT STATIC augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_4_static_shot_augmented, '4_static_shot_aug')\n",
    "print('generating TEN-SHOT STATIC augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_10_static_shot_augmented, '10_static_shot_aug')\n",
    "\n",
    "\n",
    "print('generating ONE-SHOT TF-IDF augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_1_tf_idf_shot_augmented, '1_tf_idf_shot_aug')\n",
    "print('generating TWO-SHOT TF-IDF augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_2_tf_idf_shot_augmented, '2_tf_idf_shot_aug')\n",
    "print('generating THREE-SHOT TF-IDF augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_3_tf_idf_shot_augmented, '3_tf_idf_shot_aug')\n",
    "print('generating FOUR-SHOT TF-IDF augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_4_tf_idf_shot_augmented, '4_tf_idf_shot_aug')\n",
    "print('generating TEN-SHOT TF-IDF augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_4_tf_idf_shot_augmented, '10_tf_idf_shot_aug')\n",
    "\n",
    "print('generating ONE-SHOT TF-IDF CUSTOM augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_1_tf_idf_custom_shot_augmented, '1_tf_idf_custom_shot_aug')\n",
    "print('generating TWO-SHOT TF-IDF CUSTOM augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_2_tf_idf_custom_shot_augmented, '2_tf_idf_custom_shot_aug')\n",
    "print('generating THREE-SHOT TF-IDF CUSTOM augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_3_tf_idf_custom_shot_augmented, '3_tf_idf_custom_shot_aug')\n",
    "print('generating FOUR-SHOT TF-IDF CUSTOM augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_4_tf_idf_custom_shot_augmented, '4_tf_idf_custom_shot_aug')\n",
    "print('generating TEN-SHOT TF-IDF CUSTOM augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_10_tf_idf_custom_shot_augmented, '10_tf_idf_custom_shot_aug')\n",
    "\n",
    "print('generating ONE-SHOT random augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_1_random_shot_augmented, '1_random_shot_aug')\n",
    "print('generating TWO-SHOT random augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_2_random_shot_augmented, '2_random_shot_aug')\n",
    "print('generating THREE-SHOT random augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_3_random_shot_augmented, '3_random_shot_aug')\n",
    "print('generating FOUR-SHOT random augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_4_random_shot_augmented, '4_random_shot_aug')\n",
    "print('generating TEN-SHOT random augmented annotation')\n",
    "gen_augmented_annotation_self_order_group_named(path_10_random_shot_augmented, '10_random_shot_aug')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc586f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"\"\"\"annotations\": \"DPOC trata-se de uma condição na qual há [diminuição do fluxo de ar nas vias aéreas | pathophysiology], ocorrendo em quadros como [enfisema | physical] e [bronquite crônica | exams]. Trata-se de uma [doença pulmonar crônica | history], e está muito [associada a exposição a fumaça de cigarro | epidemiology]. Um [sinal clínico de sua presença é o \"tórax em barril\" | physical].\"}\"\"\"\n",
    "matches = re.findall(r\"\"\"(?<=:)[\\s\\S]*[\"'](.*)(?<!\\\\)[\"'](?=\\s*[}])\"\"\", test)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b4c828",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_0_shot_raw_typos = f'../llama-outputs/raw-typos/ideas-0-shot'\n",
    "gen_self_order_group_named(path_0_shot_raw_typos, '0_shot_raw_typos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887cc8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_0_shot, '0_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_1_static_shot, '1_static_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_2_static_shot, '2_static_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_3_static_shot, '3_static_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_4_static_shot, '4_static_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_10_static_shot, '10_static_shot')\n",
    "\n",
    "\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_1_tf_idf_shot, '1_tf_idf_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_2_tf_idf_shot, '2_tf_idf_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_3_tf_idf_shot, '3_tf_idf_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_4_tf_idf_shot, '4_tf_idf_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_4_tf_idf_shot, '10_tf_idf_shot')\n",
    "\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_1_tf_idf_custom_shot, '1_tf_idf_custom_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_2_tf_idf_custom_shot, '2_tf_idf_custom_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_3_tf_idf_custom_shot, '3_tf_idf_custom_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_4_tf_idf_custom_shot, '4_tf_idf_custom_shot')\n",
    "gen_comparation_clustering_self_order_llm_vs_medical_specialist(path_10_tf_idf_custom_shot, '10_tf_idf_custom_shot')\n",
    "\n",
    "            # with open(f'self_order_clustering_metrics.csv', mode='a', newline='') as file:\n",
    "            #     writer = csv.writer(file)\n",
    "            #     writer.writerow([doc_id, self_order, clustering, cat_number, ideas_number])\n",
    "            # print(self_order_score(group_category_for_score_calc(pred_labels)))\n",
    "            # print(clustering_free_recall(group_category_for_score_calc(pred_labels)))\n",
    "        # print('\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54772106",
   "metadata": {},
   "source": [
    "# Joining data from Medical Specialist and LLM - Self Order Groups, Score and Clustering in Free Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c14752",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "llm_metrics_files = glob.glob(\"self_order_clustering_metrics_medical_specialist_vs_*.csv\")\n",
    "medical_specialist_metrics_and_groups = pd.read_csv('../teste-progresso/resultados_anotacoes_teste_progresso_dpoc.csv')\n",
    "llm_groups_files = glob.glob(\"teste_progresso_self_order_groups_*.csv\")\n",
    "medical_specialist_metrics_and_groups.rename(columns={'annotation id': 'doc_id'}, inplace=True)\n",
    "medical_specialist_metrics_and_groups = medical_specialist_metrics_and_groups[['doc_id', 'self order groups']]\n",
    "medical_specialist_metrics_and_groups.columns = ['doc_id', 'self_order_groups_medical_specialist']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0c9cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_metrics_files\n",
    "# pd.read_csv(file_comp)\n",
    "for file in llm_metrics_files:\n",
    "    llm_metrics = pd.read_csv(file)\n",
    "    approach = file[len('self_order_clustering_metrics_medical_specialist_vs_'):-4]\n",
    "    llm_groups = pd.read_csv(f'teste_progresso_self_order_groups_{approach}.csv')\n",
    "    llm_groups.rename(columns = {'self_order_group': 'self_order_groups_llm'}, inplace=True)\n",
    "    llm_metrics = llm_metrics.merge(medical_specialist_metrics_and_groups, on='doc_id', how='inner')\n",
    "    llm_metrics = llm_metrics.merge(llm_groups, on='doc_id', how='inner')\n",
    "    llm_metrics.to_csv(f'detailed/detailed_self_order_clustering_metrics_medical_specialist_vs_{approach}.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b71dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
