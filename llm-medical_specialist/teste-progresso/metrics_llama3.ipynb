{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276c8d1-85d7-4046-875b-69029f4fbafa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:49:41.564588Z",
     "iopub.status.busy": "2024-07-01T13:49:41.563876Z",
     "iopub.status.idle": "2024-07-01T13:49:47.797702Z",
     "shell.execute_reply": "2024-07-01T13:49:47.796721Z",
     "shell.execute_reply.started": "2024-07-01T13:49:41.564554Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install jsonlines\n",
    "!pip install tqdm\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e328da-ff80-4652-9611-498c4c834e27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:49:47.801713Z",
     "iopub.status.busy": "2024-07-01T13:49:47.801000Z",
     "iopub.status.idle": "2024-07-01T13:49:51.182904Z",
     "shell.execute_reply": "2024-07-01T13:49:51.181965Z",
     "shell.execute_reply.started": "2024-07-01T13:49:47.801665Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def get_examples(file_path):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        data = [line for line in reader]\n",
    "    return data\n",
    "def get_from_annotated_dataset(annotated_dataset,_id):\n",
    "    for doc in annotated_dataset:\n",
    "        if doc['doc_id'] == _id:\n",
    "            return doc\n",
    "annotated_dataset = get_examples('../annotations-medical_specialist-dpoc-bio-composed-multiple.jsonl')\n",
    "df_data_info = pd.read_csv('../annotations_medical_specialist_pre_processed.csv')\n",
    "\n",
    "#REMOVE ADDITIONAL BAD DATA\n",
    "# df_data_info.drop(df_data_info[df_data_info['doc_id'] == '8396380d-e0b6-4b81-8fe9-0b99c611f9f3'].index, inplace=True)\n",
    "# df_data_info.reset_index(drop=True,inplace=True)\n",
    "def replace_substring(string, start, end, replacement):\n",
    "    # Check if start and end are valid indices for the string\n",
    "    if start < 0 or end > len(string) or start > end:\n",
    "        return \"Invalid start or end index\"\n",
    "\n",
    "    # Replace the substring from start to end with the replacement string\n",
    "    new_string = string[:start] + replacement + string[end:]\n",
    "\n",
    "    return new_string\n",
    "def select_after_first_brace(string):\n",
    "    if \"```\" in string:\n",
    "        if string[:3] == \"```\":\n",
    "            string = string[3:]\n",
    "        if string[-3:] == \"```\":\n",
    "            string = string[:-3]\n",
    "        string = string.replace('\\n', '').replace('\\r', '')\n",
    "        # print('==================================================')\n",
    "        # print(string)\n",
    "    pattern = r\"(|[\\'\\\"])(annotations)([\\'\\\"]|)\"\n",
    "    \n",
    "    matches = re.search(pattern, string)\n",
    "    if matches != None:\n",
    "        string = replace_substring(string, matches.span()[0], matches.span()[1], '\"annotations\"')\n",
    "    else:\n",
    "        if '[' in string or ']' in string:\n",
    "            string = '\"annotations\":'+string\n",
    "    # print('STRING AFTER FIRST REGEX:', string)\n",
    "        \n",
    "\n",
    "    string.replace(\"]\\']\",']]')\n",
    "    string.replace(\"]\\\"]\",']]')\n",
    "    # print('STRING AFTER FIRST REGEX:', string)\n",
    "    pattern = r'\"annotations\"\\s*=\\s*\\['\n",
    "    match = re.search(pattern, string)\n",
    "    if match != None:\n",
    "        string = replace_substring(string, match.span()[0], match.span()[1], '\"annotations\":[')\n",
    "        # print('STRING AFTER SECOND REGEX:', string)\n",
    "\n",
    "    brace_index = string.find('\"annotations\"')\n",
    "    string = '{'+string[brace_index:]\n",
    "    # print('trying to select correct part of models output')\n",
    "    # print(brace_index)\n",
    "    # print(string)\n",
    "    missing_brace_pattern = r\"\"\"('],|\"],)\"\"\"\n",
    "    missing_brace_match = re.search(missing_brace_pattern, string)\n",
    "    if missing_brace_match != None:\n",
    "        # print(type(missing_brace_match))\n",
    "        # print(missing_brace_match)\n",
    "        string = string[:missing_brace_match.end()-1] + '],' + string[missing_brace_match.end():]\n",
    "    if string[-3:] == \"']]\" or string[-3:] == '\"]]':\n",
    "        string += \"]\"\n",
    "    if string.find(\"}\") == -1:\n",
    "        end_annotation_index = string.find(']]]')\n",
    "        # print('no closing brackets found...')\n",
    "        # print(string)\n",
    "        if string[-1:] != ']' and end_annotation_index != -1:\n",
    "            string = string[:end_annotation_index+3]+'}'\n",
    "        elif string[-1:] != ']' and end_annotation_index == -1:\n",
    "            end_annotation_index = string.find(']] ]')\n",
    "            # print('double braces plus third brace not found...')\n",
    "            # print(string[-1:],'\\n')\n",
    "            # print(end_annotation_index,'\\n')\n",
    "            # print(string[:end_annotation_index+4],'\\n')\n",
    "\n",
    "            string = string[:end_annotation_index+4]+'}'\n",
    "        else:\n",
    "            if string[-1:] == \"']]\" or string[-1:] == '\"]]':\n",
    "                string = string+']'\n",
    "            string = string+'}'\n",
    "    else:\n",
    "        string = string[:string.find(\"}\")+1]\n",
    "    # print('results...')\n",
    "    # print(string)\n",
    "    return string\n",
    "# prediction_annotation = eval(model_response[0])\n",
    "def prediction_to_labels(prediction_labels, data_info):\n",
    "    if type(prediction_labels) == list:\n",
    "        ze = prediction_labels[0]\n",
    "        prediction_labels = ze\n",
    "    \n",
    "    print(prediction_labels)\n",
    "    print('\\n\\n')\n",
    "    prediction_labels = select_after_first_brace(prediction_labels)\n",
    "    print(prediction_labels)\n",
    "    prediction_annotation = eval(prediction_labels)\n",
    "    full_text = data_info['text']\n",
    "    text_tokenized = data_info['labels']\n",
    "    categorized_prediction = annotation_to_tokens(full_text, text_tokenized, prediction_annotation)\n",
    "    labels = extract_labels_from_prediction(categorized_prediction)\n",
    "    return labels\n",
    "def truth_to_labels(data_info):\n",
    "    labels = extract_labels_from_truth(data_info['labels'])\n",
    "    return labels\n",
    "def extract_labels_from_truth (data_info):\n",
    "    text_tokenized = data_info\n",
    "    categories = []\n",
    "    for token in text_tokenized:\n",
    "        if token[4] != None:\n",
    "            categories.append(list(token[4].keys()))\n",
    "        else:\n",
    "            categories.append('0')\n",
    "    return categories\n",
    "def annotation_to_tokens (full_text, text_tokenized, prediction_annotation):\n",
    "    clean_text_tokenized = [[token[0],token[1],token[2]] for token in text_tokenized]\n",
    "    annotations = prediction_annotation['annotations']\n",
    "    for annotation in annotations:\n",
    "        # print('============= new annotation', annotation[0])\n",
    "        start_pos = full_text.find(annotation[0])\n",
    "        end_pos = len(annotation[0])+start_pos-1\n",
    "        # print(f'end pos is {len(annotation)} + {start_pos} - 1 = {end_pos}')\n",
    "        categorizing = False\n",
    "        \n",
    "        for token in clean_text_tokenized:\n",
    "            # print(f'token pos {token[1]} annotation pos {start_pos} token {token[0]}')\n",
    "            if token[1] == start_pos:\n",
    "                # print('starting categorization...')\n",
    "                # print(f'start pos {start_pos} end pos {end_pos} token {token[0]}')\n",
    "                categorizing = True\n",
    "            if categorizing:\n",
    "                #adds category to token\n",
    "                # print(f'adding category {annotation[1]} to token {token[0]}')\n",
    "                token.append(annotation[1])\n",
    "                if token[2] == end_pos:\n",
    "                    # print(f'ending categorization at {token[0]}...')\n",
    "                    categorizing = False\n",
    "                    break\n",
    "            \n",
    "            # print(token)\n",
    "    return clean_text_tokenized\n",
    "def extract_labels_from_prediction (categorized_prediction):\n",
    "    labels = []\n",
    "    for token in categorized_prediction:\n",
    "        if len(token) > 3:\n",
    "            labels.append(token[3])\n",
    "        else:\n",
    "            labels.append('0')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0692ed-4f0f-48b2-b4ac-3a08cade757a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:49:51.184553Z",
     "iopub.status.busy": "2024-07-01T13:49:51.184218Z",
     "iopub.status.idle": "2024-07-01T13:49:51.197742Z",
     "shell.execute_reply": "2024-07-01T13:49:51.195951Z",
     "shell.execute_reply.started": "2024-07-01T13:49:51.184513Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert labels of a annotation to binary vector\n",
    "cats = {'pathophysiology':0,\n",
    "        'etiology':1,\n",
    "        'epidemiology':2,\n",
    "        'history':3,\n",
    "        'physical':4,\n",
    "        'exams':5,\n",
    "        'differential':6,\n",
    "        'therapeutic':7}\n",
    "\n",
    "def label2binary(labels):\n",
    "    vet = [0] * 8\n",
    "    # print(vet)\n",
    "    for label in labels:\n",
    "        if label in list(cats.keys()):\n",
    "            vet[cats[label]] = 1\n",
    "    return vet\n",
    "\n",
    "# Convert all the text - divided in tokens - labels to binary vectors\n",
    "def preprocess_classification(classi_data):\n",
    "    # print('preprocess classification ======= ',classi_data)\n",
    "    for index in range(len(classi_data)):\n",
    "        classi_data[index] = label2binary(classi_data[index])\n",
    "    return classi_data\n",
    "# Convert all the text - divided in tokens - labels to boolean\n",
    "def preprocess_annotation(ann_data):\n",
    "    for index in range(len(ann_data)):\n",
    "        ann_data[index] = 1 if sum(label2binary(ann_data[index])) >= 1 else 0\n",
    "    return ann_data\n",
    "\n",
    "def get_substrings_from_text(text):\n",
    "    substring = text.split(\"\\n\")\n",
    "    return substring\n",
    "\n",
    "def find_complete_text_from_substring(substring, texts):\n",
    "    complete_text = None\n",
    "    count = 0\n",
    "    for text in texts:\n",
    "        if substring[0] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "        elif substring[0][:100] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "        elif substring[0].split('.')[0] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "    if count == 1:\n",
    "        return complete_text\n",
    "    elif count > 1 and len(substring) > 1:\n",
    "        substring.pop(0)\n",
    "        find_complete_text_from_substring(substring, texts)\n",
    "    else:\n",
    "        print('not FOUND', substring[0][:60])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f40c0-bb96-458a-ae3c-f6c374a6d1a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:41.910174Z",
     "iopub.status.busy": "2024-07-01T15:55:41.909762Z",
     "iopub.status.idle": "2024-07-01T15:55:41.915869Z",
     "shell.execute_reply": "2024-07-01T15:55:41.914951Z",
     "shell.execute_reply.started": "2024-07-01T15:55:41.910141Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24160765-7e51-499b-ba87-118f45cbdb15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ba2b8-aab5-43f4-880e-1ad06cc289a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:53.874643Z",
     "iopub.status.busy": "2024-07-01T15:55:53.874258Z",
     "iopub.status.idle": "2024-07-01T15:55:54.551357Z",
     "shell.execute_reply": "2024-07-01T15:55:54.550391Z",
     "shell.execute_reply.started": "2024-07-01T15:55:53.874610Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['zero_shot_temp_0']\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas_zero_shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        # print(doc_id)\n",
    "        # if doc_id==\"8396380d-e0b6-4b81-8fe9-0b99c611f9f3\":\n",
    "        #     print(str(truth_classification_vector))\n",
    "        #     print(str(truth_general_annotation_vector))\n",
    "        #     print(str(pred_classification_vector))\n",
    "        #     print(str(pred_general_annotation_vector))\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9df299-1c5c-479f-8f0e-3c860b9814c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:54.553787Z",
     "iopub.status.busy": "2024-07-01T15:55:54.553439Z",
     "iopub.status.idle": "2024-07-01T15:55:55.632530Z",
     "shell.execute_reply": "2024-07-01T15:55:55.631194Z",
     "shell.execute_reply.started": "2024-07-01T15:55:54.553747Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    # print(df_data_info.iloc[index])\n",
    "    # print(df_data_info['doc_id'][index])\n",
    "    # print(df_data_info[\"text\"][index])\n",
    "    # print(df_data_info[index:index+1])\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e2af0-9602-4db8-840c-3cc4069f7b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:55.635100Z",
     "iopub.status.busy": "2024-07-01T15:55:55.634194Z",
     "iopub.status.idle": "2024-07-01T15:55:55.744054Z",
     "shell.execute_reply": "2024-07-01T15:55:55.742992Z",
     "shell.execute_reply.started": "2024-07-01T15:55:55.634963Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e8ad9-b729-471d-8b2f-319e182fc4b6",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045f38d-97cf-4506-8bf0-f748a4401fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:55.747750Z",
     "iopub.status.busy": "2024-07-01T15:55:55.747268Z",
     "iopub.status.idle": "2024-07-01T15:55:55.754192Z",
     "shell.execute_reply": "2024-07-01T15:55:55.753291Z",
     "shell.execute_reply.started": "2024-07-01T15:55:55.747704Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb53bf-40aa-49b6-92e4-653f9895257d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:55.756677Z",
     "iopub.status.busy": "2024-07-01T15:55:55.755955Z",
     "iopub.status.idle": "2024-07-01T15:55:55.854301Z",
     "shell.execute_reply": "2024-07-01T15:55:55.852001Z",
     "shell.execute_reply.started": "2024-07-01T15:55:55.756635Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8575d-2149-417d-8de4-4d82a9da5f8b",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4fc07-1472-4618-b166-5ed2051df860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:55.858297Z",
     "iopub.status.busy": "2024-07-01T15:55:55.856040Z",
     "iopub.status.idle": "2024-07-01T15:55:55.969812Z",
     "shell.execute_reply": "2024-07-01T15:55:55.968346Z",
     "shell.execute_reply.started": "2024-07-01T15:55:55.858247Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6831438-2ec3-4c53-b56f-0fce1716ee1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:55.983272Z",
     "iopub.status.busy": "2024-07-01T15:55:55.981194Z",
     "iopub.status.idle": "2024-07-01T15:55:56.089340Z",
     "shell.execute_reply": "2024-07-01T15:55:56.088445Z",
     "shell.execute_reply.started": "2024-07-01T15:55:55.983217Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da868557-e0a1-4018-a78f-37ff77ffa2a3",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4349b8d-db60-4915-8aeb-7848e651e09f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:56.092101Z",
     "iopub.status.busy": "2024-07-01T15:55:56.091206Z",
     "iopub.status.idle": "2024-07-01T15:55:56.195664Z",
     "shell.execute_reply": "2024-07-01T15:55:56.194636Z",
     "shell.execute_reply.started": "2024-07-01T15:55:56.092056Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d52cf5-b119-441f-b5e0-7edefb398ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:56.203421Z",
     "iopub.status.busy": "2024-07-01T15:55:56.203076Z",
     "iopub.status.idle": "2024-07-01T15:55:56.287825Z",
     "shell.execute_reply": "2024-07-01T15:55:56.286095Z",
     "shell.execute_reply.started": "2024-07-01T15:55:56.203389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e98536-f610-4493-b37e-9751e2f477ba",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9141e8-5b1d-4917-80a6-058326bdfc0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:56.293108Z",
     "iopub.status.busy": "2024-07-01T15:55:56.292436Z",
     "iopub.status.idle": "2024-07-01T15:55:56.388873Z",
     "shell.execute_reply": "2024-07-01T15:55:56.386107Z",
     "shell.execute_reply.started": "2024-07-01T15:55:56.293072Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_weight)\n",
    "    \n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe18813-346f-417d-8363-0d262d093620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:56.398794Z",
     "iopub.status.busy": "2024-07-01T15:55:56.397846Z",
     "iopub.status.idle": "2024-07-01T15:55:56.508999Z",
     "shell.execute_reply": "2024-07-01T15:55:56.506256Z",
     "shell.execute_reply.started": "2024-07-01T15:55:56.398743Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23061d-a826-4775-bab7-e7bc4fa68438",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cc487-cd4b-4b1c-8cc1-809bfbd1e29a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:56.519523Z",
     "iopub.status.busy": "2024-07-01T15:55:56.517575Z",
     "iopub.status.idle": "2024-07-01T15:55:56.619881Z",
     "shell.execute_reply": "2024-07-01T15:55:56.617296Z",
     "shell.execute_reply.started": "2024-07-01T15:55:56.519471Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f7a24-7f00-43fc-a5e9-0aebada803f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:56.627725Z",
     "iopub.status.busy": "2024-07-01T15:55:56.625561Z",
     "iopub.status.idle": "2024-07-01T15:55:56.735471Z",
     "shell.execute_reply": "2024-07-01T15:55:56.734185Z",
     "shell.execute_reply.started": "2024-07-01T15:55:56.627668Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e9096-0795-45d7-a83a-f9e740598000",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047a6a4-4a10-4716-8527-9f4a1804fbbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:56.738144Z",
     "iopub.status.busy": "2024-07-01T15:55:56.736951Z",
     "iopub.status.idle": "2024-07-01T15:55:56.845500Z",
     "shell.execute_reply": "2024-07-01T15:55:56.844275Z",
     "shell.execute_reply.started": "2024-07-01T15:55:56.738098Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd835c5-b725-4dbc-8a57-fab53a045dd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:56.848910Z",
     "iopub.status.busy": "2024-07-01T15:55:56.847864Z",
     "iopub.status.idle": "2024-07-01T15:55:56.950564Z",
     "shell.execute_reply": "2024-07-01T15:55:56.949765Z",
     "shell.execute_reply.started": "2024-07-01T15:55:56.848810Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c04bca-8afa-4a0e-ab85-7143844988d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42bb76-3ca3-4f01-9676-fa1bf9cd0e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:56.953686Z",
     "iopub.status.busy": "2024-07-01T15:55:56.952542Z",
     "iopub.status.idle": "2024-07-01T15:55:57.784507Z",
     "shell.execute_reply": "2024-07-01T15:55:57.783116Z",
     "shell.execute_reply.started": "2024-07-01T15:55:56.953641Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['one_shot_temp_0.9']\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.9/top-p-0.6/ideas-1-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.9/top-p-0.6/ideas'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        # print(doc_id)\n",
    "        # if doc_id==\"8396380d-e0b6-4b81-8fe9-0b99c611f9f3\":\n",
    "        #     print(str(truth_classification_vector))\n",
    "        #     print(str(truth_general_annotation_vector))\n",
    "        #     print(str(pred_classification_vector))\n",
    "        #     print(str(pred_general_annotation_vector))\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1af5e-bb40-468c-8ad3-e4dc60130abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:57.799326Z",
     "iopub.status.busy": "2024-07-01T15:55:57.798797Z",
     "iopub.status.idle": "2024-07-01T15:55:58.911316Z",
     "shell.execute_reply": "2024-07-01T15:55:58.910288Z",
     "shell.execute_reply.started": "2024-07-01T15:55:57.799289Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    # print(df_data_info.iloc[index])\n",
    "    # print(df_data_info['doc_id'][index])\n",
    "    # print(df_data_info[\"text\"][index])\n",
    "    # print(df_data_info[index:index+1])\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc523bad-793a-4525-a0c9-09eda598832c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:58.913365Z",
     "iopub.status.busy": "2024-07-01T15:55:58.912997Z",
     "iopub.status.idle": "2024-07-01T15:55:59.021243Z",
     "shell.execute_reply": "2024-07-01T15:55:59.020399Z",
     "shell.execute_reply.started": "2024-07-01T15:55:58.913326Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd6fbf-3f39-47eb-b142-d11b982d56ab",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62be74-7cdd-44c0-bf71-4af361c7f2cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.023400Z",
     "iopub.status.busy": "2024-07-01T15:55:59.022704Z",
     "iopub.status.idle": "2024-07-01T15:55:59.032514Z",
     "shell.execute_reply": "2024-07-01T15:55:59.031661Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.023360Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ab13e-e39a-4a78-8c2c-4587881fdda6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.034394Z",
     "iopub.status.busy": "2024-07-01T15:55:59.033930Z",
     "iopub.status.idle": "2024-07-01T15:55:59.122462Z",
     "shell.execute_reply": "2024-07-01T15:55:59.121609Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.034351Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc51dd-c5ad-4712-94fb-9ac5aa8aaabe",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e16de-68a9-4b06-846f-e7bb2fc0bb28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.127187Z",
     "iopub.status.busy": "2024-07-01T15:55:59.125097Z",
     "iopub.status.idle": "2024-07-01T15:55:59.227946Z",
     "shell.execute_reply": "2024-07-01T15:55:59.226969Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.127137Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6fd24-dd1c-4174-836e-0f278f6c5ade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.230104Z",
     "iopub.status.busy": "2024-07-01T15:55:59.229529Z",
     "iopub.status.idle": "2024-07-01T15:55:59.331373Z",
     "shell.execute_reply": "2024-07-01T15:55:59.330244Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.230064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae6b9a-b189-41a9-841f-d6f200b0fe05",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5f829-c257-4ed5-a4f1-2dc57988c73c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.333930Z",
     "iopub.status.busy": "2024-07-01T15:55:59.333225Z",
     "iopub.status.idle": "2024-07-01T15:55:59.424557Z",
     "shell.execute_reply": "2024-07-01T15:55:59.422657Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.333886Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ee524-f11b-448b-864f-6dc68b570de4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.427065Z",
     "iopub.status.busy": "2024-07-01T15:55:59.425869Z",
     "iopub.status.idle": "2024-07-01T15:55:59.519004Z",
     "shell.execute_reply": "2024-07-01T15:55:59.517562Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.427022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36332c1-5f29-4899-9f31-86cb97500204",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c7917-4e6c-4886-ab86-e44a7f70e901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.521576Z",
     "iopub.status.busy": "2024-07-01T15:55:59.520618Z",
     "iopub.status.idle": "2024-07-01T15:55:59.626879Z",
     "shell.execute_reply": "2024-07-01T15:55:59.625961Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.521532Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "\n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3f3a9-9788-4cc3-ab10-a64e7bfa45a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.629426Z",
     "iopub.status.busy": "2024-07-01T15:55:59.628510Z",
     "iopub.status.idle": "2024-07-01T15:55:59.716807Z",
     "shell.execute_reply": "2024-07-01T15:55:59.715632Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.629382Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eedabe0-d13a-4725-8468-fc80f59d00df",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ddd60-8844-4e82-a32b-989f49566097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.719096Z",
     "iopub.status.busy": "2024-07-01T15:55:59.718153Z",
     "iopub.status.idle": "2024-07-01T15:55:59.813954Z",
     "shell.execute_reply": "2024-07-01T15:55:59.813047Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.719055Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e922f6-8eab-4bae-8319-20de9705ba96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.847206Z",
     "iopub.status.busy": "2024-07-01T15:55:59.846189Z",
     "iopub.status.idle": "2024-07-01T15:55:59.948350Z",
     "shell.execute_reply": "2024-07-01T15:55:59.947457Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.847161Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11e381-b90d-42c4-a6eb-da2380bda56f",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd86d0-07dc-4f1f-8f03-0f967af34b09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:55:59.953024Z",
     "iopub.status.busy": "2024-07-01T15:55:59.950653Z",
     "iopub.status.idle": "2024-07-01T15:56:00.075520Z",
     "shell.execute_reply": "2024-07-01T15:56:00.072422Z",
     "shell.execute_reply.started": "2024-07-01T15:55:59.952977Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222787c-e65c-4262-a37c-cf883b12400b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:00.093261Z",
     "iopub.status.busy": "2024-07-01T15:56:00.092277Z",
     "iopub.status.idle": "2024-07-01T15:56:00.201299Z",
     "shell.execute_reply": "2024-07-01T15:56:00.200341Z",
     "shell.execute_reply.started": "2024-07-01T15:56:00.093209Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bc9cd-868e-448a-8fc2-d40913f88fba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - one shot temp 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce973b-0ecd-4045-839c-e91ef508203b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:00.203371Z",
     "iopub.status.busy": "2024-07-01T15:56:00.202768Z",
     "iopub.status.idle": "2024-07-01T15:56:00.966360Z",
     "shell.execute_reply": "2024-07-01T15:56:00.964543Z",
     "shell.execute_reply.started": "2024-07-01T15:56:00.203327Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "df_data_info = pd.read_csv('../annotations_medical_specialist_pre_processed.csv')\n",
    "row_metrics = ['one_shot_temp_0']\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-1-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-1-shot'\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "    \n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        \n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c7aa5-c957-46d0-a925-eaf1e8e7b148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:00.975119Z",
     "iopub.status.busy": "2024-07-01T15:56:00.974272Z",
     "iopub.status.idle": "2024-07-01T15:56:03.018346Z",
     "shell.execute_reply": "2024-07-01T15:56:03.017123Z",
     "shell.execute_reply.started": "2024-07-01T15:56:00.975069Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbec646-472a-4910-a0f5-915a4e82bfce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.020818Z",
     "iopub.status.busy": "2024-07-01T15:56:03.020284Z",
     "iopub.status.idle": "2024-07-01T15:56:03.136295Z",
     "shell.execute_reply": "2024-07-01T15:56:03.134747Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.020775Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb7747-afa4-4eef-8ed2-11b9ea515da4",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a64e4-fe8f-40e0-bb74-0bceae2f1eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.142056Z",
     "iopub.status.busy": "2024-07-01T15:56:03.139898Z",
     "iopub.status.idle": "2024-07-01T15:56:03.156669Z",
     "shell.execute_reply": "2024-07-01T15:56:03.155151Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.142014Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4180a7-80f3-4343-9730-f0d87b64985c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.160574Z",
     "iopub.status.busy": "2024-07-01T15:56:03.159553Z",
     "iopub.status.idle": "2024-07-01T15:56:03.251468Z",
     "shell.execute_reply": "2024-07-01T15:56:03.249873Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.160527Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb76688-5b9f-48d8-8a32-0136771e77f5",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7d324-39de-4232-af76-e4aa1f090f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.253752Z",
     "iopub.status.busy": "2024-07-01T15:56:03.253125Z",
     "iopub.status.idle": "2024-07-01T15:56:03.350686Z",
     "shell.execute_reply": "2024-07-01T15:56:03.349731Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.253706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505722b-2abe-4251-b515-b1069df7ec78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.353322Z",
     "iopub.status.busy": "2024-07-01T15:56:03.352101Z",
     "iopub.status.idle": "2024-07-01T15:56:03.450130Z",
     "shell.execute_reply": "2024-07-01T15:56:03.449004Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.353276Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b891322-dd13-452d-bf6a-d14b2d76f7e0",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb82108-33bf-4554-9c0d-2b3661754559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.451940Z",
     "iopub.status.busy": "2024-07-01T15:56:03.451578Z",
     "iopub.status.idle": "2024-07-01T15:56:03.560084Z",
     "shell.execute_reply": "2024-07-01T15:56:03.559074Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.451898Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291273ee-85e6-42d6-9fac-335709fbae2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.563012Z",
     "iopub.status.busy": "2024-07-01T15:56:03.562157Z",
     "iopub.status.idle": "2024-07-01T15:56:03.666506Z",
     "shell.execute_reply": "2024-07-01T15:56:03.665407Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.562969Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67e453-eb7a-4057-8e13-4bf22f5618ba",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3395c4-9654-41bb-b355-a62e0d8e91f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.669415Z",
     "iopub.status.busy": "2024-07-01T15:56:03.668310Z",
     "iopub.status.idle": "2024-07-01T15:56:03.767864Z",
     "shell.execute_reply": "2024-07-01T15:56:03.766730Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.669372Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3f6d9-da33-43d6-a69b-f762b7aa32ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.770610Z",
     "iopub.status.busy": "2024-07-01T15:56:03.769474Z",
     "iopub.status.idle": "2024-07-01T15:56:03.861213Z",
     "shell.execute_reply": "2024-07-01T15:56:03.860311Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.770534Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38489b92-ad4b-4ff0-80f4-3720f3a62bb1",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863d14c-2589-42d3-a90b-13f3053a0603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.863853Z",
     "iopub.status.busy": "2024-07-01T15:56:03.863077Z",
     "iopub.status.idle": "2024-07-01T15:56:03.960282Z",
     "shell.execute_reply": "2024-07-01T15:56:03.959073Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.863809Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8699a2f-ed33-4c8b-8e2f-7be4dbacc9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:03.963025Z",
     "iopub.status.busy": "2024-07-01T15:56:03.962020Z",
     "iopub.status.idle": "2024-07-01T15:56:04.060212Z",
     "shell.execute_reply": "2024-07-01T15:56:04.058945Z",
     "shell.execute_reply.started": "2024-07-01T15:56:03.962983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057bc0e-7349-43e7-b1d2-2cdae9e2166d",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b16ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40fc257",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a5148-fc07-4379-b52f-178ec0357fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - 2 shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fa3d7-ecdb-4656-8eca-17fcda0939fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:04.064057Z",
     "iopub.status.busy": "2024-07-01T15:56:04.062724Z",
     "iopub.status.idle": "2024-07-01T15:56:04.752646Z",
     "shell.execute_reply": "2024-07-01T15:56:04.751668Z",
     "shell.execute_reply.started": "2024-07-01T15:56:04.063996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['two_shot_temp_0']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.9/top-p-0.6/ideas-2-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.9/top-p-0.6/ideas-2-shot'\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "    \n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        \n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc974af-a82f-4e27-ae6b-58085551a261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:04.754635Z",
     "iopub.status.busy": "2024-07-01T15:56:04.754190Z",
     "iopub.status.idle": "2024-07-01T15:56:05.845346Z",
     "shell.execute_reply": "2024-07-01T15:56:05.844422Z",
     "shell.execute_reply.started": "2024-07-01T15:56:04.754592Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decde5d7-9fa9-4123-bca1-62d83ac5c074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:05.847327Z",
     "iopub.status.busy": "2024-07-01T15:56:05.846919Z",
     "iopub.status.idle": "2024-07-01T15:56:05.950481Z",
     "shell.execute_reply": "2024-07-01T15:56:05.949577Z",
     "shell.execute_reply.started": "2024-07-01T15:56:05.847284Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22daa84-bcfb-484e-abf5-a4900b8a7798",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad1642-5ab3-4440-89a5-b83ab2f6d66e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:05.954735Z",
     "iopub.status.busy": "2024-07-01T15:56:05.954307Z",
     "iopub.status.idle": "2024-07-01T15:56:05.963378Z",
     "shell.execute_reply": "2024-07-01T15:56:05.962276Z",
     "shell.execute_reply.started": "2024-07-01T15:56:05.954701Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4398f-e2f0-4bba-8b2d-cd68cb5724f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:05.965799Z",
     "iopub.status.busy": "2024-07-01T15:56:05.964892Z",
     "iopub.status.idle": "2024-07-01T15:56:06.077693Z",
     "shell.execute_reply": "2024-07-01T15:56:06.072270Z",
     "shell.execute_reply.started": "2024-07-01T15:56:05.965532Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17cef1-a1bf-4ebf-ab17-5c0f50144d2f",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d642739-9f78-460f-9f19-a91dc99ee681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:06.083759Z",
     "iopub.status.busy": "2024-07-01T15:56:06.079293Z",
     "iopub.status.idle": "2024-07-01T15:56:06.180118Z",
     "shell.execute_reply": "2024-07-01T15:56:06.178750Z",
     "shell.execute_reply.started": "2024-07-01T15:56:06.083704Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef5ad9-7084-40b5-bfb5-269dff823dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:06.182994Z",
     "iopub.status.busy": "2024-07-01T15:56:06.181854Z",
     "iopub.status.idle": "2024-07-01T15:56:06.275543Z",
     "shell.execute_reply": "2024-07-01T15:56:06.274619Z",
     "shell.execute_reply.started": "2024-07-01T15:56:06.182946Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb3c05-d60c-4832-acce-6ef0fe64e9a0",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44232568-6658-4913-9f10-36a18ce2ab74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:06.283122Z",
     "iopub.status.busy": "2024-07-01T15:56:06.281399Z",
     "iopub.status.idle": "2024-07-01T15:56:06.381069Z",
     "shell.execute_reply": "2024-07-01T15:56:06.380204Z",
     "shell.execute_reply.started": "2024-07-01T15:56:06.283071Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f821f-0182-4ec6-ad5c-dbb83bf3811c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:06.384016Z",
     "iopub.status.busy": "2024-07-01T15:56:06.383306Z",
     "iopub.status.idle": "2024-07-01T15:56:06.499159Z",
     "shell.execute_reply": "2024-07-01T15:56:06.498130Z",
     "shell.execute_reply.started": "2024-07-01T15:56:06.383970Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5fccb-634b-4aaa-910c-cccc345bd4a9",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ed5c2-38f7-4b03-9190-fe49659113d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:06.508549Z",
     "iopub.status.busy": "2024-07-01T15:56:06.505917Z",
     "iopub.status.idle": "2024-07-01T15:56:06.612614Z",
     "shell.execute_reply": "2024-07-01T15:56:06.609471Z",
     "shell.execute_reply.started": "2024-07-01T15:56:06.508410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6567ec2b-4dfa-4dcc-acdc-d8ce73f0aac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:06.618778Z",
     "iopub.status.busy": "2024-07-01T15:56:06.618250Z",
     "iopub.status.idle": "2024-07-01T15:56:06.723183Z",
     "shell.execute_reply": "2024-07-01T15:56:06.721368Z",
     "shell.execute_reply.started": "2024-07-01T15:56:06.618733Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5a5df-2b65-465a-821f-849400864c53",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10b0dc-52f0-48d5-82ca-90253b85af79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:06.727992Z",
     "iopub.status.busy": "2024-07-01T15:56:06.726927Z",
     "iopub.status.idle": "2024-07-01T15:56:06.843351Z",
     "shell.execute_reply": "2024-07-01T15:56:06.841245Z",
     "shell.execute_reply.started": "2024-07-01T15:56:06.727444Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06ca75-c2b9-473b-a9dd-c71512534faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:06.849059Z",
     "iopub.status.busy": "2024-07-01T15:56:06.846704Z",
     "iopub.status.idle": "2024-07-01T15:56:06.951138Z",
     "shell.execute_reply": "2024-07-01T15:56:06.949870Z",
     "shell.execute_reply.started": "2024-07-01T15:56:06.849005Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e072a-f9bc-41a5-b025-9850020d802d",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9987a6-5644-495b-9107-679df5e749f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:06.953753Z",
     "iopub.status.busy": "2024-07-01T15:56:06.953306Z",
     "iopub.status.idle": "2024-07-01T15:56:07.050683Z",
     "shell.execute_reply": "2024-07-01T15:56:07.049205Z",
     "shell.execute_reply.started": "2024-07-01T15:56:06.953710Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e4c5c-a6f5-4feb-bba6-d9debead6684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:07.059135Z",
     "iopub.status.busy": "2024-07-01T15:56:07.056945Z",
     "iopub.status.idle": "2024-07-01T15:56:07.173947Z",
     "shell.execute_reply": "2024-07-01T15:56:07.172737Z",
     "shell.execute_reply.started": "2024-07-01T15:56:07.059081Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b63614-f79f-454a-9021-391fa639c923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 1 shot (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482a376-9e84-40db-b520-cc3ed30dec85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:07.175978Z",
     "iopub.status.busy": "2024-07-01T15:56:07.175549Z",
     "iopub.status.idle": "2024-07-01T15:56:07.874903Z",
     "shell.execute_reply": "2024-07-01T15:56:07.874042Z",
     "shell.execute_reply.started": "2024-07-01T15:56:07.175934Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['one_shot_tf_idf']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-1-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-1-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        print(doc_id)\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcb648-2c9d-4879-9f84-d2de5b1e046e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:07.876466Z",
     "iopub.status.busy": "2024-07-01T15:56:07.876105Z",
     "iopub.status.idle": "2024-07-01T15:56:09.270025Z",
     "shell.execute_reply": "2024-07-01T15:56:09.268864Z",
     "shell.execute_reply.started": "2024-07-01T15:56:07.876424Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775ba4c-6c51-4e85-bd59-936d9beb0064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:09.272101Z",
     "iopub.status.busy": "2024-07-01T15:56:09.271468Z",
     "iopub.status.idle": "2024-07-01T15:56:09.381609Z",
     "shell.execute_reply": "2024-07-01T15:56:09.380752Z",
     "shell.execute_reply.started": "2024-07-01T15:56:09.272061Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195b663-53a9-43d9-962e-a8ea2511ad19",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889026de-9008-4bf1-980a-8bba483d52c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:09.388020Z",
     "iopub.status.busy": "2024-07-01T15:56:09.387574Z",
     "iopub.status.idle": "2024-07-01T15:56:09.403002Z",
     "shell.execute_reply": "2024-07-01T15:56:09.401951Z",
     "shell.execute_reply.started": "2024-07-01T15:56:09.387987Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed862b-da0f-4ae7-8bc3-1ad9ef7b78ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:09.408596Z",
     "iopub.status.busy": "2024-07-01T15:56:09.407898Z",
     "iopub.status.idle": "2024-07-01T15:56:09.502190Z",
     "shell.execute_reply": "2024-07-01T15:56:09.501286Z",
     "shell.execute_reply.started": "2024-07-01T15:56:09.408550Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108dc1f-7273-4284-aea2-e308704405fd",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c34d4-8ce2-4dcc-96b7-8e449e3d201a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:09.512758Z",
     "iopub.status.busy": "2024-07-01T15:56:09.504430Z",
     "iopub.status.idle": "2024-07-01T15:56:09.603631Z",
     "shell.execute_reply": "2024-07-01T15:56:09.601835Z",
     "shell.execute_reply.started": "2024-07-01T15:56:09.512696Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb637b25-12fc-4c19-a5bb-4ec87a1c9043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:09.607533Z",
     "iopub.status.busy": "2024-07-01T15:56:09.606804Z",
     "iopub.status.idle": "2024-07-01T15:56:09.699949Z",
     "shell.execute_reply": "2024-07-01T15:56:09.698894Z",
     "shell.execute_reply.started": "2024-07-01T15:56:09.607359Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc951a5-9ff6-484c-88f3-5c020ecda05d",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e23d9-a897-49b9-8aae-9397b51268b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:09.703800Z",
     "iopub.status.busy": "2024-07-01T15:56:09.703365Z",
     "iopub.status.idle": "2024-07-01T15:56:09.798009Z",
     "shell.execute_reply": "2024-07-01T15:56:09.796762Z",
     "shell.execute_reply.started": "2024-07-01T15:56:09.703768Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992768d6-695d-4273-a8f8-0ff8a3d6edc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:09.799965Z",
     "iopub.status.busy": "2024-07-01T15:56:09.799641Z",
     "iopub.status.idle": "2024-07-01T15:56:09.909207Z",
     "shell.execute_reply": "2024-07-01T15:56:09.908233Z",
     "shell.execute_reply.started": "2024-07-01T15:56:09.799938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ce79d-0efb-4047-b8d1-cfbf36ebc171",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c953d-857c-4412-b6df-059992b89928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:09.911609Z",
     "iopub.status.busy": "2024-07-01T15:56:09.910932Z",
     "iopub.status.idle": "2024-07-01T15:56:10.015981Z",
     "shell.execute_reply": "2024-07-01T15:56:10.012905Z",
     "shell.execute_reply.started": "2024-07-01T15:56:09.911567Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a85e08-6647-4b4d-bf34-f475361e8b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:10.019387Z",
     "iopub.status.busy": "2024-07-01T15:56:10.018811Z",
     "iopub.status.idle": "2024-07-01T15:56:10.111715Z",
     "shell.execute_reply": "2024-07-01T15:56:10.110532Z",
     "shell.execute_reply.started": "2024-07-01T15:56:10.019340Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4854e2-7e6f-414b-b265-ef87122f819c",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43fd0c-ccd4-4f68-ba3c-0ec2346d64f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:10.115071Z",
     "iopub.status.busy": "2024-07-01T15:56:10.114219Z",
     "iopub.status.idle": "2024-07-01T15:56:10.205111Z",
     "shell.execute_reply": "2024-07-01T15:56:10.204247Z",
     "shell.execute_reply.started": "2024-07-01T15:56:10.115022Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10103682-7f3d-4225-9da9-b9271dfb0aac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:10.208384Z",
     "iopub.status.busy": "2024-07-01T15:56:10.207748Z",
     "iopub.status.idle": "2024-07-01T15:56:10.309734Z",
     "shell.execute_reply": "2024-07-01T15:56:10.308915Z",
     "shell.execute_reply.started": "2024-07-01T15:56:10.208340Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed4d42-c9dd-40d2-bbb2-2f7e93b3d583",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00ec24-bb4e-49ec-9afe-e4d3d05cf905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:10.315432Z",
     "iopub.status.busy": "2024-07-01T15:56:10.314638Z",
     "iopub.status.idle": "2024-07-01T15:56:10.420424Z",
     "shell.execute_reply": "2024-07-01T15:56:10.418141Z",
     "shell.execute_reply.started": "2024-07-01T15:56:10.315398Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36dbee6-ae0d-4285-8e85-2b3b79731b52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:10.443526Z",
     "iopub.status.busy": "2024-07-01T15:56:10.441770Z",
     "iopub.status.idle": "2024-07-01T15:56:10.544763Z",
     "shell.execute_reply": "2024-07-01T15:56:10.543814Z",
     "shell.execute_reply.started": "2024-07-01T15:56:10.443471Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297bc86-295f-4aa4-84b7-ac27923683b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 2 shot (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c0a0a-2ed4-4dc8-b0c6-fd87acac0d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:10.546945Z",
     "iopub.status.busy": "2024-07-01T15:56:10.546391Z",
     "iopub.status.idle": "2024-07-01T15:56:11.290147Z",
     "shell.execute_reply": "2024-07-01T15:56:11.288825Z",
     "shell.execute_reply.started": "2024-07-01T15:56:10.546901Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['two_shot_tf_idf']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-2-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-2-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee839b1-5d26-4caf-b465-4095df33bdfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:11.292860Z",
     "iopub.status.busy": "2024-07-01T15:56:11.291985Z",
     "iopub.status.idle": "2024-07-01T15:56:12.474559Z",
     "shell.execute_reply": "2024-07-01T15:56:12.473240Z",
     "shell.execute_reply.started": "2024-07-01T15:56:11.292815Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb65ae-ff6d-4390-836a-2a6269ba250e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:12.478111Z",
     "iopub.status.busy": "2024-07-01T15:56:12.477316Z",
     "iopub.status.idle": "2024-07-01T15:56:12.587157Z",
     "shell.execute_reply": "2024-07-01T15:56:12.585765Z",
     "shell.execute_reply.started": "2024-07-01T15:56:12.478068Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc11a3-7649-492b-a506-f5ea135bdd0e",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e635c6-e8a7-4e00-9c14-3f35cc0cffbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:12.592067Z",
     "iopub.status.busy": "2024-07-01T15:56:12.588794Z",
     "iopub.status.idle": "2024-07-01T15:56:12.628452Z",
     "shell.execute_reply": "2024-07-01T15:56:12.627226Z",
     "shell.execute_reply.started": "2024-07-01T15:56:12.592002Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443df34a-c606-4a08-9a0d-d3d72d7909ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:12.630989Z",
     "iopub.status.busy": "2024-07-01T15:56:12.630160Z",
     "iopub.status.idle": "2024-07-01T15:56:12.734306Z",
     "shell.execute_reply": "2024-07-01T15:56:12.732711Z",
     "shell.execute_reply.started": "2024-07-01T15:56:12.630943Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad92aa-9428-4971-8a11-eae8572e767d",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d3ed3-463e-4c47-a1f5-76cfea6b666e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:12.742456Z",
     "iopub.status.busy": "2024-07-01T15:56:12.737369Z",
     "iopub.status.idle": "2024-07-01T15:56:12.830267Z",
     "shell.execute_reply": "2024-07-01T15:56:12.829067Z",
     "shell.execute_reply.started": "2024-07-01T15:56:12.737893Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff9059-a71b-465a-ada3-88f32e95cf57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:12.837842Z",
     "iopub.status.busy": "2024-07-01T15:56:12.836326Z",
     "iopub.status.idle": "2024-07-01T15:56:12.949702Z",
     "shell.execute_reply": "2024-07-01T15:56:12.947881Z",
     "shell.execute_reply.started": "2024-07-01T15:56:12.837793Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee3a67-2dc4-4557-ace8-6eccf2f1c92c",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131d075-3796-4838-8c05-ab14b9017923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:12.955944Z",
     "iopub.status.busy": "2024-07-01T15:56:12.953581Z",
     "iopub.status.idle": "2024-07-01T15:56:13.061238Z",
     "shell.execute_reply": "2024-07-01T15:56:13.060017Z",
     "shell.execute_reply.started": "2024-07-01T15:56:12.955496Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1b745-7523-43fb-a477-a1895675783a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:13.074180Z",
     "iopub.status.busy": "2024-07-01T15:56:13.073245Z",
     "iopub.status.idle": "2024-07-01T15:56:13.157582Z",
     "shell.execute_reply": "2024-07-01T15:56:13.155715Z",
     "shell.execute_reply.started": "2024-07-01T15:56:13.074142Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f447261-a8ec-4d3a-bf1a-89b0575287f0",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310c8f4-4380-4076-a226-2b88ed6a1011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:13.159876Z",
     "iopub.status.busy": "2024-07-01T15:56:13.159156Z",
     "iopub.status.idle": "2024-07-01T15:56:13.273273Z",
     "shell.execute_reply": "2024-07-01T15:56:13.271876Z",
     "shell.execute_reply.started": "2024-07-01T15:56:13.159832Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac9075-8724-4fa9-9d78-3d3b5cbde81e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:13.287660Z",
     "iopub.status.busy": "2024-07-01T15:56:13.275773Z",
     "iopub.status.idle": "2024-07-01T15:56:13.379531Z",
     "shell.execute_reply": "2024-07-01T15:56:13.377901Z",
     "shell.execute_reply.started": "2024-07-01T15:56:13.287602Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9fd24-16a2-4797-a8ac-d19dfd1685be",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e78fa4-570a-4c55-af46-12b97167ac63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:13.388340Z",
     "iopub.status.busy": "2024-07-01T15:56:13.384121Z",
     "iopub.status.idle": "2024-07-01T15:56:13.488118Z",
     "shell.execute_reply": "2024-07-01T15:56:13.486886Z",
     "shell.execute_reply.started": "2024-07-01T15:56:13.388284Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a4c85-ff4c-499e-853d-112d123a5a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:13.490122Z",
     "iopub.status.busy": "2024-07-01T15:56:13.489778Z",
     "iopub.status.idle": "2024-07-01T15:56:13.594025Z",
     "shell.execute_reply": "2024-07-01T15:56:13.593000Z",
     "shell.execute_reply.started": "2024-07-01T15:56:13.490082Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ec841-af4d-4a24-a68e-c25ab24b3219",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8733d-be76-4bf6-a505-5e78448b8c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:13.604487Z",
     "iopub.status.busy": "2024-07-01T15:56:13.601640Z",
     "iopub.status.idle": "2024-07-01T15:56:13.701733Z",
     "shell.execute_reply": "2024-07-01T15:56:13.697104Z",
     "shell.execute_reply.started": "2024-07-01T15:56:13.604434Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d73f1-18b1-4fd4-999c-54350b5e88bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:13.714883Z",
     "iopub.status.busy": "2024-07-01T15:56:13.714118Z",
     "iopub.status.idle": "2024-07-01T15:56:13.815376Z",
     "shell.execute_reply": "2024-07-01T15:56:13.814241Z",
     "shell.execute_reply.started": "2024-07-01T15:56:13.714827Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc1d11-9314-483a-9b14-4012a3adeae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 3 shot (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f4613-d94e-4941-a801-da0bdfecac03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:13.817511Z",
     "iopub.status.busy": "2024-07-01T15:56:13.817152Z",
     "iopub.status.idle": "2024-07-01T15:56:14.466747Z",
     "shell.execute_reply": "2024-07-01T15:56:14.464675Z",
     "shell.execute_reply.started": "2024-07-01T15:56:13.817469Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['three_shot_tf_idf']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b1875-083f-476d-ad51-68a5db1c88dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:14.468815Z",
     "iopub.status.busy": "2024-07-01T15:56:14.468409Z",
     "iopub.status.idle": "2024-07-01T15:56:15.519158Z",
     "shell.execute_reply": "2024-07-01T15:56:15.517943Z",
     "shell.execute_reply.started": "2024-07-01T15:56:14.468771Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c062ef0-b7ce-41ec-8770-876c4c829761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:15.521153Z",
     "iopub.status.busy": "2024-07-01T15:56:15.520516Z",
     "iopub.status.idle": "2024-07-01T15:56:15.615906Z",
     "shell.execute_reply": "2024-07-01T15:56:15.614975Z",
     "shell.execute_reply.started": "2024-07-01T15:56:15.521111Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f73c3-cded-46aa-8998-1c6808b1aead",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175005f8-f215-4c08-b7de-8a3c28c4b405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:15.629240Z",
     "iopub.status.busy": "2024-07-01T15:56:15.627801Z",
     "iopub.status.idle": "2024-07-01T15:56:15.651277Z",
     "shell.execute_reply": "2024-07-01T15:56:15.649842Z",
     "shell.execute_reply.started": "2024-07-01T15:56:15.629083Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ea8c8-1ec3-4f58-aea4-6e895523d31b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:15.655460Z",
     "iopub.status.busy": "2024-07-01T15:56:15.654540Z",
     "iopub.status.idle": "2024-07-01T15:56:15.752632Z",
     "shell.execute_reply": "2024-07-01T15:56:15.751447Z",
     "shell.execute_reply.started": "2024-07-01T15:56:15.655201Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98054456-bad3-4761-919e-45e85fb1d508",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93074427-0622-49cd-a3ba-46430882ed25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:15.754872Z",
     "iopub.status.busy": "2024-07-01T15:56:15.754100Z",
     "iopub.status.idle": "2024-07-01T15:56:15.844332Z",
     "shell.execute_reply": "2024-07-01T15:56:15.843406Z",
     "shell.execute_reply.started": "2024-07-01T15:56:15.754812Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575aca04-504e-497e-855c-4632ca54fb80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:15.846319Z",
     "iopub.status.busy": "2024-07-01T15:56:15.845687Z",
     "iopub.status.idle": "2024-07-01T15:56:15.959096Z",
     "shell.execute_reply": "2024-07-01T15:56:15.957878Z",
     "shell.execute_reply.started": "2024-07-01T15:56:15.846257Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39fe3c-ef1c-400a-856a-61de88374175",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2000e-2974-4ab6-b90a-2e0c47cfa96b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:15.964382Z",
     "iopub.status.busy": "2024-07-01T15:56:15.962242Z",
     "iopub.status.idle": "2024-07-01T15:56:16.064656Z",
     "shell.execute_reply": "2024-07-01T15:56:16.063596Z",
     "shell.execute_reply.started": "2024-07-01T15:56:15.964336Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff2686-de39-445e-b746-e82af1e334bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:16.066758Z",
     "iopub.status.busy": "2024-07-01T15:56:16.066057Z",
     "iopub.status.idle": "2024-07-01T15:56:16.165838Z",
     "shell.execute_reply": "2024-07-01T15:56:16.164979Z",
     "shell.execute_reply.started": "2024-07-01T15:56:16.066715Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbb94b-1ee4-4b6f-bbd1-9784c9c0a406",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e05e19-6a6c-4c72-824a-b521da42d153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:16.172905Z",
     "iopub.status.busy": "2024-07-01T15:56:16.167308Z",
     "iopub.status.idle": "2024-07-01T15:56:16.273971Z",
     "shell.execute_reply": "2024-07-01T15:56:16.273090Z",
     "shell.execute_reply.started": "2024-07-01T15:56:16.170836Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7f3cb-ff5d-49c1-ab3d-c4c1241a216f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:16.276607Z",
     "iopub.status.busy": "2024-07-01T15:56:16.275987Z",
     "iopub.status.idle": "2024-07-01T15:56:16.381662Z",
     "shell.execute_reply": "2024-07-01T15:56:16.380602Z",
     "shell.execute_reply.started": "2024-07-01T15:56:16.276566Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7fa37-4ab6-41ef-8080-465fa5c21e6f",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f117e-8579-4849-962b-359e04f56e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:16.383791Z",
     "iopub.status.busy": "2024-07-01T15:56:16.383133Z",
     "iopub.status.idle": "2024-07-01T15:56:16.489341Z",
     "shell.execute_reply": "2024-07-01T15:56:16.488422Z",
     "shell.execute_reply.started": "2024-07-01T15:56:16.383638Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9160e5e-737a-4c9b-b00b-d8e1b6a4f3e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:16.493056Z",
     "iopub.status.busy": "2024-07-01T15:56:16.491670Z",
     "iopub.status.idle": "2024-07-01T15:56:16.591234Z",
     "shell.execute_reply": "2024-07-01T15:56:16.590073Z",
     "shell.execute_reply.started": "2024-07-01T15:56:16.493007Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bac705-a8a3-4179-8a05-c6796c027add",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93045935-e10f-40e7-aa1a-169ff6ecd375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:16.593038Z",
     "iopub.status.busy": "2024-07-01T15:56:16.592575Z",
     "iopub.status.idle": "2024-07-01T15:56:16.693415Z",
     "shell.execute_reply": "2024-07-01T15:56:16.691995Z",
     "shell.execute_reply.started": "2024-07-01T15:56:16.592996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a18a53-9d32-4ed2-8335-e263363a6062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:16.695912Z",
     "iopub.status.busy": "2024-07-01T15:56:16.695205Z",
     "iopub.status.idle": "2024-07-01T15:56:16.795510Z",
     "shell.execute_reply": "2024-07-01T15:56:16.794583Z",
     "shell.execute_reply.started": "2024-07-01T15:56:16.695868Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfe9c8-0d61-45d1-a281-08f0785237a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 4 shot (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5741c-b240-4e6d-8243-5c409c5a320e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:16.797454Z",
     "iopub.status.busy": "2024-07-01T15:56:16.797104Z",
     "iopub.status.idle": "2024-07-01T15:56:17.571417Z",
     "shell.execute_reply": "2024-07-01T15:56:17.570012Z",
     "shell.execute_reply.started": "2024-07-01T15:56:16.797415Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['four_shot_tf_id']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-4-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-4-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c8a37-1ef1-468b-aca4-cf6d65ccefb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:17.574085Z",
     "iopub.status.busy": "2024-07-01T15:56:17.573752Z",
     "iopub.status.idle": "2024-07-01T15:56:18.826862Z",
     "shell.execute_reply": "2024-07-01T15:56:18.825749Z",
     "shell.execute_reply.started": "2024-07-01T15:56:17.574055Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4188b8c-00fb-42db-812b-c01653db1b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:18.829045Z",
     "iopub.status.busy": "2024-07-01T15:56:18.828172Z",
     "iopub.status.idle": "2024-07-01T15:56:18.940534Z",
     "shell.execute_reply": "2024-07-01T15:56:18.938981Z",
     "shell.execute_reply.started": "2024-07-01T15:56:18.829004Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff80200-8621-48a3-9cb7-306f505556e8",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25677d-2634-49b9-8548-6bb4ee44e78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:18.942582Z",
     "iopub.status.busy": "2024-07-01T15:56:18.941848Z",
     "iopub.status.idle": "2024-07-01T15:56:18.961124Z",
     "shell.execute_reply": "2024-07-01T15:56:18.959941Z",
     "shell.execute_reply.started": "2024-07-01T15:56:18.942512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70cebf-be63-43dd-a398-15068842b510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:18.963278Z",
     "iopub.status.busy": "2024-07-01T15:56:18.962493Z",
     "iopub.status.idle": "2024-07-01T15:56:19.061708Z",
     "shell.execute_reply": "2024-07-01T15:56:19.060602Z",
     "shell.execute_reply.started": "2024-07-01T15:56:18.963236Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a65c9-55f9-43dc-9fb1-8a9e38806ae6",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d906a2c-9fce-4e77-8a97-c160a1c72d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:19.064189Z",
     "iopub.status.busy": "2024-07-01T15:56:19.063634Z",
     "iopub.status.idle": "2024-07-01T15:56:19.162490Z",
     "shell.execute_reply": "2024-07-01T15:56:19.161535Z",
     "shell.execute_reply.started": "2024-07-01T15:56:19.064157Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c050312-b19a-4475-b80b-df52aa30b646",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:19.166116Z",
     "iopub.status.busy": "2024-07-01T15:56:19.165769Z",
     "iopub.status.idle": "2024-07-01T15:56:19.273485Z",
     "shell.execute_reply": "2024-07-01T15:56:19.272511Z",
     "shell.execute_reply.started": "2024-07-01T15:56:19.166085Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc2757-2998-420b-8a2f-dc01d0f54f96",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4afb31-2d8e-42d6-bdcf-962bdaeb45cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:19.275548Z",
     "iopub.status.busy": "2024-07-01T15:56:19.274721Z",
     "iopub.status.idle": "2024-07-01T15:56:19.372849Z",
     "shell.execute_reply": "2024-07-01T15:56:19.371827Z",
     "shell.execute_reply.started": "2024-07-01T15:56:19.275506Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024e1f5-ec2a-4993-9d48-a6125e504d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:19.375051Z",
     "iopub.status.busy": "2024-07-01T15:56:19.374513Z",
     "iopub.status.idle": "2024-07-01T15:56:19.468794Z",
     "shell.execute_reply": "2024-07-01T15:56:19.467868Z",
     "shell.execute_reply.started": "2024-07-01T15:56:19.375008Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18806fbc-5ecc-41a1-8a35-95bc1fcb772b",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd25ef-2f0b-42b6-90b3-488bb16ba4d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:19.471135Z",
     "iopub.status.busy": "2024-07-01T15:56:19.470499Z",
     "iopub.status.idle": "2024-07-01T15:56:19.571006Z",
     "shell.execute_reply": "2024-07-01T15:56:19.570127Z",
     "shell.execute_reply.started": "2024-07-01T15:56:19.471091Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81b278-bf2f-4a68-b3b2-e943a79dbfbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:19.572875Z",
     "iopub.status.busy": "2024-07-01T15:56:19.572459Z",
     "iopub.status.idle": "2024-07-01T15:56:19.667281Z",
     "shell.execute_reply": "2024-07-01T15:56:19.665745Z",
     "shell.execute_reply.started": "2024-07-01T15:56:19.572832Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088df1bc-0a16-4b03-b41d-a4ca84125947",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d82318-086c-4bd5-94f5-57602a0546d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:19.670140Z",
     "iopub.status.busy": "2024-07-01T15:56:19.669262Z",
     "iopub.status.idle": "2024-07-01T15:56:19.771019Z",
     "shell.execute_reply": "2024-07-01T15:56:19.769962Z",
     "shell.execute_reply.started": "2024-07-01T15:56:19.670095Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170b4ae-7e3f-41f1-93bc-7eead8847058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:19.775140Z",
     "iopub.status.busy": "2024-07-01T15:56:19.774758Z",
     "iopub.status.idle": "2024-07-01T15:56:19.861398Z",
     "shell.execute_reply": "2024-07-01T15:56:19.860431Z",
     "shell.execute_reply.started": "2024-07-01T15:56:19.775108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e514b3-67e3-464a-9e2b-c0685654d0c8",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad875427-0910-4dec-96c5-79a4db35ec2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:19.863781Z",
     "iopub.status.busy": "2024-07-01T15:56:19.863164Z",
     "iopub.status.idle": "2024-07-01T15:56:19.955559Z",
     "shell.execute_reply": "2024-07-01T15:56:19.954452Z",
     "shell.execute_reply.started": "2024-07-01T15:56:19.863738Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1501a-4f13-4e9d-970b-9336335f514b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T15:56:19.961093Z",
     "iopub.status.busy": "2024-07-01T15:56:19.956965Z",
     "iopub.status.idle": "2024-07-01T15:56:20.051701Z",
     "shell.execute_reply": "2024-07-01T15:56:20.050886Z",
     "shell.execute_reply.started": "2024-07-01T15:56:19.961048Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9c68a8",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 2 - 10 shot (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da98c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['ten_shot_tf_id']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-10-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-10-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5500506",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3465478a",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c960416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ce5ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7334c4b4",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10504f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba485f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8b921",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37b639e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd9497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9566d95d",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fec690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa2445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b99f78f",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c84454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d5997f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c251581",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e0fbc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f25fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489ab1a9",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 3 - 1 shot (TF-IDF only annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50888bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['one_shot_tf_id_custom']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-1-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-1-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ae52ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf270255",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe6b015",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a052c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f1f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854b0685",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6cd1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190dc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e1809",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78994d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c98293ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2426b369",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20f81b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869e1ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85b6b1e",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7854c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdac10b",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0755e547",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9965da08",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cfcbbe",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 3 - 2 shot (TF-IDF only annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941265ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['two_shot_tf_id_custom']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-2-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-2-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c791c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b17dfed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff33c01",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69afdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918de9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a81621",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157e5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955b377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8170ac",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011750d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239f9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd22ff",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5cf6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1105b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009f3145",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71ad089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bc3f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa87de7d",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797280db",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1e176c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43953d4e",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 3 - 3 shot (TF-IDF only annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ee2d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['three_shot_tf_id_custom']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-3-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-3-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7e777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0138d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9263b7ed",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c25b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e5053",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6e56e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6debee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c766611",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcdd0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74b777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79906dfa",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "022107df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e57edf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70afc70b",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d816a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e00688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f7a227",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781bfac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bec2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b459fd8a",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 3 - 4 shot (TF-IDF only annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5184df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['four_shot_tf_id_custom']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-4-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-4-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80c6c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92486742",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c27f93",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dd5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f3f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425f9b60",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2261f4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e3421c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfaf6b3",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b61864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6733c02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97a9ce4",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df93a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27aa37e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8f2323",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62f4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c8b6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3534c589",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23b176d",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f3aebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef285f2f",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 3 - 10 shot (TF-IDF only annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4209879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['ten_shot_tf_id_custom']\n",
    "\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-10-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-10-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1abfd38",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03629ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4824e948",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88a6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adccc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53188a78",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768e049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11b5130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a9ec2a4",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cbefe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7e7dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583fe5f5",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c96442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a42896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c493ee8",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b811de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a668db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e2282d",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab71daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4393bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575e076c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "#Create a DataFrame object\n",
    "df_metrics = pd.DataFrame(rows_metrics_report,\n",
    "                  columns = ['approach','label_weight' , 'label_mean', 'ann_weight' , 'ann_mean','precision_weight','precision_mean','recall_weight','recall_mean','f1_weight','f1_mean','global_weight','global_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881a47ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metrics.to_csv('df_metrics_no_short.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcfd5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ccba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
