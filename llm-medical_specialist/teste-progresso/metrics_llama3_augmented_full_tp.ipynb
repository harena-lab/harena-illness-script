{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276c8d1-85d7-4046-875b-69029f4fbafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e328da-ff80-4652-9611-498c4c834e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def get_examples(file_path):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        data = [line for line in reader]\n",
    "    return data\n",
    "def get_from_annotated_dataset(annotated_dataset,_id):\n",
    "    for doc in annotated_dataset:\n",
    "        if doc['doc_id'] == _id:\n",
    "            return doc\n",
    "annotated_dataset = get_examples('annotations-medical_specialist-dpoc-bio-composed-multiple.jsonl')\n",
    "df_data_info = pd.read_csv('../annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "\n",
    "def escape_inner_quotes(text):\n",
    "    pattern =r\"\"\"(?<=[:])[\\s\\S]*?[\"'](.*)(?<!\\\\)[\"'](?=\\s*[}])\"\"\"\n",
    "    escaped_text = re.sub(pattern, lambda m: \"'\" + m.group(1).replace('\"', '\\\\\"') + \"'\", text)\n",
    "    return escaped_text\n",
    "####### Function that extracts the entities and respective categories and transforms into a dictionary of annotations  #############\n",
    "def transform_augmented_data_to_pattern(data_info):\n",
    "    data_info = data_info.replace('```','')\n",
    "    data_info = re.sub(r'(?<![\"\\\\]):', r'\\\\:', data_info)\n",
    "    data_info = data_info.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    closing_braces = data_info.find(\"}\")\n",
    "    opening_braces = data_info.find(\"{\")\n",
    "    annotations_pos = data_info.find('\"annotations\"')\n",
    "\n",
    "    if opening_braces == -1:\n",
    "        data_info = '{'+ data_info\n",
    "    elif opening_braces != -1 and opening_braces < annotations_pos:\n",
    "        data_info = '{' + data_info[annotations_pos:]\n",
    "        \n",
    "    if closing_braces == -1:\n",
    "        # print('======= didnt found closing braces')\n",
    "        data_info = data_info[:len(data_info)] + '}'\n",
    "        # print('======= after insertion',data_info)\n",
    "        closing_braces = data_info.find(\"}\")\n",
    "    if (data_info.find('\"}') == -1 and data_info.find('\" }') == -1):\n",
    "        data_info = data_info[:closing_braces-1] + '\"}'    \n",
    "    if data_info[-3] == '\\\\':\n",
    "        data_info = data_info[:len(data_info)-3]+data_info[-2:len(data_info)+1]\n",
    "    data_info = escape_inner_quotes(data_info)\n",
    "    opening_braces = data_info.find(\"{\")\n",
    "    if opening_braces == -1:\n",
    "        data_info = '{'+ data_info\n",
    "    data_info = ast.literal_eval(data_info)\n",
    "    \n",
    "    pattern = r'\\[([^\\[\\]]+)\\s*\\|\\s*([^\\[\\]]+)\\]'\n",
    "    matches = re.findall(pattern, data_info['annotations'])\n",
    "    annotations = {\"annotations\": [[match[0].strip(), [val.strip() for val in match[1].split(',')]] for match in matches]}\n",
    "    return annotations\n",
    "    \n",
    "#REMOVE ADDITIONAL BAD DATA\n",
    "# df_data_info.drop(df_data_info[df_data_info['doc_id'] == '8396380d-e0b6-4b81-8fe9-0b99c611f9f3'].index, inplace=True)\n",
    "# df_data_info.reset_index(drop=True,inplace=True)\n",
    "def replace_substring(string, start, end, replacement):\n",
    "    # Check if start and end are valid indices for the string\n",
    "    if start < 0 or end > len(string) or start > end:\n",
    "        return \"Invalid start or end index\"\n",
    "\n",
    "    # Replace the substring from start to end with the replacement string\n",
    "    new_string = string[:start] + replacement + string[end:]\n",
    "\n",
    "    return new_string\n",
    "def select_after_first_brace(string):\n",
    "    pattern = r\"(|[\\'\\\"])(annotations)([\\'\\\"]|)\"\n",
    "    \n",
    "    matches = re.search(pattern, string)\n",
    "    if matches != None:\n",
    "        string = replace_substring(string, matches.span()[0], matches.span()[1], '\"annotations\"')\n",
    "    string.replace(\"]\\']\",']]')\n",
    "    string.replace(\"]\\\"]\",']]')\n",
    "    # print('STRING AFTER FIRST REGEX:', string)\n",
    "    pattern = r'\"annotations\"\\s*=\\s*\\['\n",
    "    match = re.search(pattern, string)\n",
    "    if match != None:\n",
    "        string = replace_substring(string, match.span()[0], match.span()[1], '\"annotations\":[')\n",
    "        # print('STRING AFTER SECOND REGEX:', string)\n",
    "\n",
    "    brace_index = string.find('\"annotations\"')\n",
    "    string = '{'+string[brace_index:]\n",
    "    # print('trying to select correct part of models output')\n",
    "    # print(brace_index)\n",
    "    # print(string)\n",
    "    if string[-3:] == \"']]\":\n",
    "        string = string[:-3]+\"']]]\"\n",
    "    if string.find(\"}\") == -1:\n",
    "        end_annotation_index = string.find(']]]')\n",
    "\n",
    "        if string[-1:] != ']' and end_annotation_index != -1:\n",
    "            string = string[:end_annotation_index+3]+'}'\n",
    "        elif string[-1:] != ']' and end_annotation_index == -1:\n",
    "            end_annotation_index = string.find(']] ]')\n",
    "\n",
    "            string = string[:end_annotation_index+4]+'}'\n",
    "        else:\n",
    "            string = string+'}'\n",
    "    else:\n",
    "        string = string[:string.find(\"}\")+1]\n",
    "    # print('results...')\n",
    "    # print(string)\n",
    "    return string\n",
    "# prediction_annotation = eval(model_response[0])\n",
    "\n",
    "def remove_upper_and_accents(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[áàâãä]', 'a', text)\n",
    "    text = re.sub(r'[éèêë]', 'e', text)\n",
    "    text = re.sub(r'[íìîï]', 'i', text)\n",
    "    text = re.sub(r'[óòôõö]', 'o', text)\n",
    "    text = re.sub(r'[úùûü]', 'u', text)\n",
    "    text = re.sub(r'[ç]', 'c', text)\n",
    "    return text\n",
    "def prediction_to_labels(prediction_labels, data_info):\n",
    "    prediction_labels = transform_augmented_data_to_pattern(prediction_labels)\n",
    "    if type(prediction_labels) == list:\n",
    "        ze = prediction_labels[0]\n",
    "        prediction_labels = ze\n",
    "    prediction_labels = select_after_first_brace(str(prediction_labels))\n",
    "    prediction_annotation = ast.literal_eval(prediction_labels)\n",
    "    # print('prediction annotation \\n',prediction_annotation)\n",
    "    full_text = data_info['text']\n",
    "    text_tokenized = data_info['labels']\n",
    "    # print('tokenized len ==== \\n',len(text_tokenized))\n",
    "    categorized_prediction = annotation_to_tokens(full_text, text_tokenized, prediction_annotation)\n",
    "    # print('tokenized cat ==== \\n',categorized_prediction)\n",
    "    labels = extract_labels_from_prediction(categorized_prediction)\n",
    "    # print(len(labels),'tokenized labels ==== \\n',labels)\n",
    "    return labels\n",
    "def truth_to_labels(data_info):\n",
    "    labels = extract_labels_from_truth(data_info['labels'])\n",
    "    return labels\n",
    "def extract_labels_from_truth (data_info):\n",
    "    text_tokenized = data_info\n",
    "    categories = []\n",
    "    for token in text_tokenized:\n",
    "        if token[4] != None:\n",
    "            categories.append(list(token[4].keys()))\n",
    "        else:\n",
    "            categories.append('0')\n",
    "    return categories\n",
    "def annotation_to_tokens (full_text, text_tokenized, prediction_annotation):\n",
    "    full_text = remove_upper_and_accents(full_text)\n",
    "    clean_text_tokenized = [[token[0],token[1],token[2]] for token in text_tokenized]\n",
    "    \n",
    "    # print('===========================================================================================')\n",
    "    # print(clean_text_tokenized)\n",
    "    annotations = prediction_annotation['annotations']\n",
    "    for annotation in annotations:\n",
    "        annotation[0] = remove_upper_and_accents(annotation[0])\n",
    "        annotation[0] = annotation[0].strip()\n",
    "        # print('============= new annotation', annotation[0])\n",
    "        start_pos = full_text.find(annotation[0])\n",
    "        end_pos = len(annotation[0])+start_pos-1\n",
    "        # print(f'end pos is {len(annotation[0])} + {start_pos} - 1 = {end_pos}')\n",
    "        categorizing = False\n",
    "        \n",
    "        for token in clean_text_tokenized:\n",
    "            token[0] = remove_upper_and_accents(token[0])\n",
    "            token[0] = token[0].strip()\n",
    "            # print(f'token pos {token[1]} annotation pos {start_pos} token {token[0]}')\n",
    "            if token[1] == start_pos:\n",
    "                # print('starting categorization...')\n",
    "                # print(f'start pos {start_pos} end pos {end_pos} token {token[0]}')\n",
    "                categorizing = True\n",
    "            if categorizing:\n",
    "                #adds category to token\n",
    "                # print(f'adding category {annotation[1]} to token {token[0]}')\n",
    "                token.append(annotation[1])\n",
    "                if token[2] == end_pos:\n",
    "                    # print(f'ending categorization at {token[0]}...')\n",
    "                    categorizing = False\n",
    "                    break\n",
    "            \n",
    "            # print(token)\n",
    "    return clean_text_tokenized\n",
    "def extract_labels_from_prediction (categorized_prediction):\n",
    "    labels = []\n",
    "    for token in categorized_prediction:\n",
    "        if len(token) > 3:\n",
    "            labels.append(token[3])\n",
    "        else:\n",
    "            labels.append('0')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0692ed-4f0f-48b2-b4ac-3a08cade757a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert labels of a annotation to binary vector\n",
    "cats = {'pathophysiology':0,\n",
    "        'etiology':1,\n",
    "        'epidemiology':2,\n",
    "        'history':3,\n",
    "        'physical':4,\n",
    "        'exams':5,\n",
    "        'differential':6,\n",
    "        'therapeutic':7}\n",
    "\n",
    "def label2binary(labels):\n",
    "    vet = [0] * 8\n",
    "    # print(vet)\n",
    "    for label in labels:\n",
    "        if label in list(cats.keys()):\n",
    "            vet[cats[label]] = 1\n",
    "    return vet\n",
    "\n",
    "# Convert all the text - divided in tokens - labels to binary vectors\n",
    "def preprocess_classification(classi_data):\n",
    "    # print('preprocess classification ======= ',classi_data)\n",
    "    for index in range(len(classi_data)):\n",
    "        classi_data[index] = label2binary(classi_data[index])\n",
    "    return classi_data\n",
    "# Convert all the text - divided in tokens - labels to boolean\n",
    "def preprocess_annotation(ann_data):\n",
    "    for index in range(len(ann_data)):\n",
    "        ann_data[index] = 1 if sum(label2binary(ann_data[index])) >= 1 else 0\n",
    "    return ann_data\n",
    "\n",
    "def get_substrings_from_text(text):\n",
    "    substring = text.split(\"\\n\")\n",
    "    return substring\n",
    "\n",
    "def find_complete_text_from_substring(substring, texts):\n",
    "    complete_text = None\n",
    "    count = 0\n",
    "    for text in texts:\n",
    "        if substring[0] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "        elif substring[0][:100] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "        elif substring[0].split('.')[0] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "    if count == 1:\n",
    "        return complete_text\n",
    "    elif count > 1 and len(substring) > 1:\n",
    "        substring.pop(0)\n",
    "        find_complete_text_from_substring(substring, texts)\n",
    "    else:\n",
    "        print('not FOUND', substring[0][:60])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f40c0-bb96-458a-ae3c-f6c374a6d1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24160765-7e51-499b-ba87-118f45cbdb15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ba2b8-aab5-43f4-880e-1ad06cc289a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['zero_shot_temp_0']\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas_0_shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        \n",
    "        # print(llama_annotated['doc_id'])\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        # print(pred_labels)\n",
    "        # print('\\n\\n')\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print('pred ======')\n",
    "        # print(pred_general_annotation_vector)\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(len(truth_classification_vector))\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(' truth ======')\n",
    "        # print(truth_general_annotation_vector)\n",
    "        # if doc_id==\"8396380d-e0b6-4b81-8fe9-0b99c611f9f3\":\n",
    "        #     print(str(truth_classification_vector))\n",
    "        #     print(str(truth_general_annotation_vector))\n",
    "        #     print(str(pred_classification_vector))\n",
    "        #     print(str(pred_general_annotation_vector))\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9df299-1c5c-479f-8f0e-3c860b9814c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    # print(df_data_info.iloc[index])\n",
    "    # print(df_data_info['doc_id'][index])\n",
    "    # print(df_data_info[\"text\"][index])\n",
    "    # print(df_data_info[index:index+1])\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e2af0-9602-4db8-840c-3cc4069f7b6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e8ad9-b729-471d-8b2f-319e182fc4b6",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045f38d-97cf-4506-8bf0-f748a4401fa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb53bf-40aa-49b6-92e4-653f9895257d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8575d-2149-417d-8de4-4d82a9da5f8b",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4fc07-1472-4618-b166-5ed2051df860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6831438-2ec3-4c53-b56f-0fce1716ee1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da868557-e0a1-4018-a78f-37ff77ffa2a3",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4349b8d-db60-4915-8aeb-7848e651e09f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d52cf5-b119-441f-b5e0-7edefb398ff7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e98536-f610-4493-b37e-9751e2f477ba",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9141e8-5b1d-4917-80a6-058326bdfc0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_weight)\n",
    "    \n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe18813-346f-417d-8363-0d262d093620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23061d-a826-4775-bab7-e7bc4fa68438",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cc487-cd4b-4b1c-8cc1-809bfbd1e29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f7a24-7f00-43fc-a5e9-0aebada803f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e9096-0795-45d7-a83a-f9e740598000",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047a6a4-4a10-4716-8527-9f4a1804fbbc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd835c5-b725-4dbc-8a57-fab53a045dd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c04bca-8afa-4a0e-ab85-7143844988d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42bb76-3ca3-4f01-9676-fa1bf9cd0e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:19.260090Z",
     "iopub.status.busy": "2024-07-18T14:18:19.259714Z",
     "iopub.status.idle": "2024-07-18T14:18:19.799671Z",
     "shell.execute_reply": "2024-07-18T14:18:19.798397Z",
     "shell.execute_reply.started": "2024-07-18T14:18:19.260049Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['one_shot_temp_0.9']\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.9/top-p-0.6/ideas-1-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.9/top-p-0.6/ideas'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        # if doc_id==\"8396380d-e0b6-4b81-8fe9-0b99c611f9f3\":\n",
    "        #     print(str(truth_classification_vector))\n",
    "        #     print(str(truth_general_annotation_vector))\n",
    "        #     print(str(pred_classification_vector))\n",
    "        #     print(str(pred_general_annotation_vector))\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1af5e-bb40-468c-8ad3-e4dc60130abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:19.801889Z",
     "iopub.status.busy": "2024-07-18T14:18:19.801516Z",
     "iopub.status.idle": "2024-07-18T14:18:20.887467Z",
     "shell.execute_reply": "2024-07-18T14:18:20.886619Z",
     "shell.execute_reply.started": "2024-07-18T14:18:19.801847Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    # print(df_data_info.iloc[index])\n",
    "    # print(df_data_info['doc_id'][index])\n",
    "    # print(df_data_info[\"text\"][index])\n",
    "    # print(df_data_info[index:index+1])\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc523bad-793a-4525-a0c9-09eda598832c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:20.889379Z",
     "iopub.status.busy": "2024-07-18T14:18:20.888999Z",
     "iopub.status.idle": "2024-07-18T14:18:20.971922Z",
     "shell.execute_reply": "2024-07-18T14:18:20.971068Z",
     "shell.execute_reply.started": "2024-07-18T14:18:20.889336Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd6fbf-3f39-47eb-b142-d11b982d56ab",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62be74-7cdd-44c0-bf71-4af361c7f2cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:20.975850Z",
     "iopub.status.busy": "2024-07-18T14:18:20.975469Z",
     "iopub.status.idle": "2024-07-18T14:18:21.000429Z",
     "shell.execute_reply": "2024-07-18T14:18:20.999498Z",
     "shell.execute_reply.started": "2024-07-18T14:18:20.975821Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ab13e-e39a-4a78-8c2c-4587881fdda6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.002834Z",
     "iopub.status.busy": "2024-07-18T14:18:21.002375Z",
     "iopub.status.idle": "2024-07-18T14:18:21.098401Z",
     "shell.execute_reply": "2024-07-18T14:18:21.097483Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.002792Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc51dd-c5ad-4712-94fb-9ac5aa8aaabe",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e16de-68a9-4b06-846f-e7bb2fc0bb28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.101875Z",
     "iopub.status.busy": "2024-07-18T14:18:21.101480Z",
     "iopub.status.idle": "2024-07-18T14:18:21.196679Z",
     "shell.execute_reply": "2024-07-18T14:18:21.195699Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.101843Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6fd24-dd1c-4174-836e-0f278f6c5ade",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.199631Z",
     "iopub.status.busy": "2024-07-18T14:18:21.198904Z",
     "iopub.status.idle": "2024-07-18T14:18:21.288562Z",
     "shell.execute_reply": "2024-07-18T14:18:21.287650Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.199518Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae6b9a-b189-41a9-841f-d6f200b0fe05",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5f829-c257-4ed5-a4f1-2dc57988c73c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.291396Z",
     "iopub.status.busy": "2024-07-18T14:18:21.290429Z",
     "iopub.status.idle": "2024-07-18T14:18:21.379134Z",
     "shell.execute_reply": "2024-07-18T14:18:21.378218Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.291354Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ee524-f11b-448b-864f-6dc68b570de4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.381408Z",
     "iopub.status.busy": "2024-07-18T14:18:21.380924Z",
     "iopub.status.idle": "2024-07-18T14:18:21.476591Z",
     "shell.execute_reply": "2024-07-18T14:18:21.475314Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.381365Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36332c1-5f29-4899-9f31-86cb97500204",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c7917-4e6c-4886-ab86-e44a7f70e901",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.478612Z",
     "iopub.status.busy": "2024-07-18T14:18:21.478084Z",
     "iopub.status.idle": "2024-07-18T14:18:21.568087Z",
     "shell.execute_reply": "2024-07-18T14:18:21.566436Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.478566Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "\n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3f3a9-9788-4cc3-ab10-a64e7bfa45a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.570675Z",
     "iopub.status.busy": "2024-07-18T14:18:21.569759Z",
     "iopub.status.idle": "2024-07-18T14:18:21.661502Z",
     "shell.execute_reply": "2024-07-18T14:18:21.660402Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.570626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eedabe0-d13a-4725-8468-fc80f59d00df",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ddd60-8844-4e82-a32b-989f49566097",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.663696Z",
     "iopub.status.busy": "2024-07-18T14:18:21.663289Z",
     "iopub.status.idle": "2024-07-18T14:18:21.777164Z",
     "shell.execute_reply": "2024-07-18T14:18:21.776131Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.663652Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e922f6-8eab-4bae-8319-20de9705ba96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.786014Z",
     "iopub.status.busy": "2024-07-18T14:18:21.785162Z",
     "iopub.status.idle": "2024-07-18T14:18:21.872173Z",
     "shell.execute_reply": "2024-07-18T14:18:21.871254Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.785980Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11e381-b90d-42c4-a6eb-da2380bda56f",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd86d0-07dc-4f1f-8f03-0f967af34b09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.877831Z",
     "iopub.status.busy": "2024-07-18T14:18:21.873402Z",
     "iopub.status.idle": "2024-07-18T14:18:21.967541Z",
     "shell.execute_reply": "2024-07-18T14:18:21.966144Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.877790Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222787c-e65c-4262-a37c-cf883b12400b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.969735Z",
     "iopub.status.busy": "2024-07-18T14:18:21.969164Z",
     "iopub.status.idle": "2024-07-18T14:18:22.057882Z",
     "shell.execute_reply": "2024-07-18T14:18:22.057079Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.969693Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bc9cd-868e-448a-8fc2-d40913f88fba",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - one shot temp 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce973b-0ecd-4045-839c-e91ef508203b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:22.061527Z",
     "iopub.status.busy": "2024-07-18T14:18:22.060549Z",
     "iopub.status.idle": "2024-07-18T14:18:22.653160Z",
     "shell.execute_reply": "2024-07-18T14:18:22.652349Z",
     "shell.execute_reply.started": "2024-07-18T14:18:22.061421Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "df_data_info = pd.read_csv('test_data_info_no_short.csv')\n",
    "row_metrics = ['one_shot_temp_0']\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-1-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-1-shot'\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "    \n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        \n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c7aa5-c957-46d0-a925-eaf1e8e7b148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:22.655105Z",
     "iopub.status.busy": "2024-07-18T14:18:22.654674Z",
     "iopub.status.idle": "2024-07-18T14:18:23.656816Z",
     "shell.execute_reply": "2024-07-18T14:18:23.655500Z",
     "shell.execute_reply.started": "2024-07-18T14:18:22.655064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbec646-472a-4910-a0f5-915a4e82bfce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:23.660616Z",
     "iopub.status.busy": "2024-07-18T14:18:23.659430Z",
     "iopub.status.idle": "2024-07-18T14:18:23.743975Z",
     "shell.execute_reply": "2024-07-18T14:18:23.743006Z",
     "shell.execute_reply.started": "2024-07-18T14:18:23.660480Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb7747-afa4-4eef-8ed2-11b9ea515da4",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a64e4-fe8f-40e0-bb74-0bceae2f1eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:23.747843Z",
     "iopub.status.busy": "2024-07-18T14:18:23.747427Z",
     "iopub.status.idle": "2024-07-18T14:18:23.765976Z",
     "shell.execute_reply": "2024-07-18T14:18:23.764140Z",
     "shell.execute_reply.started": "2024-07-18T14:18:23.747811Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4180a7-80f3-4343-9730-f0d87b64985c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:23.769203Z",
     "iopub.status.busy": "2024-07-18T14:18:23.768478Z",
     "iopub.status.idle": "2024-07-18T14:18:23.855660Z",
     "shell.execute_reply": "2024-07-18T14:18:23.854145Z",
     "shell.execute_reply.started": "2024-07-18T14:18:23.769155Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb76688-5b9f-48d8-8a32-0136771e77f5",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7d324-39de-4232-af76-e4aa1f090f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:23.859956Z",
     "iopub.status.busy": "2024-07-18T14:18:23.859437Z",
     "iopub.status.idle": "2024-07-18T14:18:23.946406Z",
     "shell.execute_reply": "2024-07-18T14:18:23.945342Z",
     "shell.execute_reply.started": "2024-07-18T14:18:23.859863Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505722b-2abe-4251-b515-b1069df7ec78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:23.949360Z",
     "iopub.status.busy": "2024-07-18T14:18:23.948772Z",
     "iopub.status.idle": "2024-07-18T14:18:24.036037Z",
     "shell.execute_reply": "2024-07-18T14:18:24.032566Z",
     "shell.execute_reply.started": "2024-07-18T14:18:23.949289Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b891322-dd13-452d-bf6a-d14b2d76f7e0",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb82108-33bf-4554-9c0d-2b3661754559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.046423Z",
     "iopub.status.busy": "2024-07-18T14:18:24.042351Z",
     "iopub.status.idle": "2024-07-18T14:18:24.128975Z",
     "shell.execute_reply": "2024-07-18T14:18:24.127970Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.046378Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291273ee-85e6-42d6-9fac-335709fbae2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.131634Z",
     "iopub.status.busy": "2024-07-18T14:18:24.131161Z",
     "iopub.status.idle": "2024-07-18T14:18:24.216297Z",
     "shell.execute_reply": "2024-07-18T14:18:24.215053Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.131504Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67e453-eb7a-4057-8e13-4bf22f5618ba",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3395c4-9654-41bb-b355-a62e0d8e91f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.218612Z",
     "iopub.status.busy": "2024-07-18T14:18:24.218227Z",
     "iopub.status.idle": "2024-07-18T14:18:24.307776Z",
     "shell.execute_reply": "2024-07-18T14:18:24.306736Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.218569Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3f6d9-da33-43d6-a69b-f762b7aa32ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.310608Z",
     "iopub.status.busy": "2024-07-18T14:18:24.309646Z",
     "iopub.status.idle": "2024-07-18T14:18:24.392461Z",
     "shell.execute_reply": "2024-07-18T14:18:24.391818Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.310560Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38489b92-ad4b-4ff0-80f4-3720f3a62bb1",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863d14c-2589-42d3-a90b-13f3053a0603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.393656Z",
     "iopub.status.busy": "2024-07-18T14:18:24.393302Z",
     "iopub.status.idle": "2024-07-18T14:18:24.488492Z",
     "shell.execute_reply": "2024-07-18T14:18:24.487416Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.393628Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8699a2f-ed33-4c8b-8e2f-7be4dbacc9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.490698Z",
     "iopub.status.busy": "2024-07-18T14:18:24.490066Z",
     "iopub.status.idle": "2024-07-18T14:18:24.573875Z",
     "shell.execute_reply": "2024-07-18T14:18:24.572773Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.490654Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057bc0e-7349-43e7-b1d2-2cdae9e2166d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acacfa-3689-4410-a7d7-a943f901b777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.575996Z",
     "iopub.status.busy": "2024-07-18T14:18:24.575591Z",
     "iopub.status.idle": "2024-07-18T14:18:24.658454Z",
     "shell.execute_reply": "2024-07-18T14:18:24.657507Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.575953Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e09114-4f71-45b2-abfa-51a6ce6a2fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.660944Z",
     "iopub.status.busy": "2024-07-18T14:18:24.660226Z",
     "iopub.status.idle": "2024-07-18T14:18:24.749158Z",
     "shell.execute_reply": "2024-07-18T14:18:24.748205Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.660900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a5148-fc07-4379-b52f-178ec0357fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - 2 shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fa3d7-ecdb-4656-8eca-17fcda0939fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.754487Z",
     "iopub.status.busy": "2024-07-18T14:18:24.753124Z",
     "iopub.status.idle": "2024-07-18T14:18:25.254361Z",
     "shell.execute_reply": "2024-07-18T14:18:25.253630Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.754436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['two_shot_temp_0']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.9/top-p-0.6/ideas-2-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.9/top-p-0.6/ideas-2-shot'\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "    \n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        \n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc974af-a82f-4e27-ae6b-58085551a261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:25.255663Z",
     "iopub.status.busy": "2024-07-18T14:18:25.255432Z",
     "iopub.status.idle": "2024-07-18T14:18:26.861814Z",
     "shell.execute_reply": "2024-07-18T14:18:26.860853Z",
     "shell.execute_reply.started": "2024-07-18T14:18:25.255638Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decde5d7-9fa9-4123-bca1-62d83ac5c074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:26.872232Z",
     "iopub.status.busy": "2024-07-18T14:18:26.867245Z",
     "iopub.status.idle": "2024-07-18T14:18:26.966900Z",
     "shell.execute_reply": "2024-07-18T14:18:26.965952Z",
     "shell.execute_reply.started": "2024-07-18T14:18:26.872170Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22daa84-bcfb-484e-abf5-a4900b8a7798",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad1642-5ab3-4440-89a5-b83ab2f6d66e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:26.969607Z",
     "iopub.status.busy": "2024-07-18T14:18:26.968704Z",
     "iopub.status.idle": "2024-07-18T14:18:26.988867Z",
     "shell.execute_reply": "2024-07-18T14:18:26.987816Z",
     "shell.execute_reply.started": "2024-07-18T14:18:26.969562Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4398f-e2f0-4bba-8b2d-cd68cb5724f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:26.993405Z",
     "iopub.status.busy": "2024-07-18T14:18:26.991786Z",
     "iopub.status.idle": "2024-07-18T14:18:27.069489Z",
     "shell.execute_reply": "2024-07-18T14:18:27.068317Z",
     "shell.execute_reply.started": "2024-07-18T14:18:26.993357Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17cef1-a1bf-4ebf-ab17-5c0f50144d2f",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d642739-9f78-460f-9f19-a91dc99ee681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.071341Z",
     "iopub.status.busy": "2024-07-18T14:18:27.070729Z",
     "iopub.status.idle": "2024-07-18T14:18:27.152251Z",
     "shell.execute_reply": "2024-07-18T14:18:27.151350Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.071294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef5ad9-7084-40b5-bfb5-269dff823dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.154351Z",
     "iopub.status.busy": "2024-07-18T14:18:27.153264Z",
     "iopub.status.idle": "2024-07-18T14:18:27.236165Z",
     "shell.execute_reply": "2024-07-18T14:18:27.235220Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.154308Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb3c05-d60c-4832-acce-6ef0fe64e9a0",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44232568-6658-4913-9f10-36a18ce2ab74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.238557Z",
     "iopub.status.busy": "2024-07-18T14:18:27.237757Z",
     "iopub.status.idle": "2024-07-18T14:18:27.323998Z",
     "shell.execute_reply": "2024-07-18T14:18:27.322314Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.238512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f821f-0182-4ec6-ad5c-dbb83bf3811c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.325652Z",
     "iopub.status.busy": "2024-07-18T14:18:27.325134Z",
     "iopub.status.idle": "2024-07-18T14:18:27.412575Z",
     "shell.execute_reply": "2024-07-18T14:18:27.411551Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.325609Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5fccb-634b-4aaa-910c-cccc345bd4a9",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ed5c2-38f7-4b03-9190-fe49659113d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.414599Z",
     "iopub.status.busy": "2024-07-18T14:18:27.413868Z",
     "iopub.status.idle": "2024-07-18T14:18:27.499386Z",
     "shell.execute_reply": "2024-07-18T14:18:27.497979Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.414555Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6567ec2b-4dfa-4dcc-acdc-d8ce73f0aac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.501504Z",
     "iopub.status.busy": "2024-07-18T14:18:27.501133Z",
     "iopub.status.idle": "2024-07-18T14:18:27.586244Z",
     "shell.execute_reply": "2024-07-18T14:18:27.584684Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.501464Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5a5df-2b65-465a-821f-849400864c53",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10b0dc-52f0-48d5-82ca-90253b85af79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.589570Z",
     "iopub.status.busy": "2024-07-18T14:18:27.588761Z",
     "iopub.status.idle": "2024-07-18T14:18:27.671686Z",
     "shell.execute_reply": "2024-07-18T14:18:27.670781Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.589525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06ca75-c2b9-473b-a9dd-c71512534faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.673424Z",
     "iopub.status.busy": "2024-07-18T14:18:27.673000Z",
     "iopub.status.idle": "2024-07-18T14:18:27.759560Z",
     "shell.execute_reply": "2024-07-18T14:18:27.758606Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.673383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e072a-f9bc-41a5-b025-9850020d802d",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9987a6-5644-495b-9107-679df5e749f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.761022Z",
     "iopub.status.busy": "2024-07-18T14:18:27.760642Z",
     "iopub.status.idle": "2024-07-18T14:18:27.847382Z",
     "shell.execute_reply": "2024-07-18T14:18:27.846290Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.760983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e4c5c-a6f5-4feb-bba6-d9debead6684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.849537Z",
     "iopub.status.busy": "2024-07-18T14:18:27.848965Z",
     "iopub.status.idle": "2024-07-18T14:18:27.934017Z",
     "shell.execute_reply": "2024-07-18T14:18:27.933149Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.849492Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b63614-f79f-454a-9021-391fa639c923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 1 shot temp 0 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482a376-9e84-40db-b520-cc3ed30dec85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['one_shot_tf_idf']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-1-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-1-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "print('Complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcb648-2c9d-4879-9f84-d2de5b1e046e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775ba4c-6c51-4e85-bd59-936d9beb0064",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195b663-53a9-43d9-962e-a8ea2511ad19",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889026de-9008-4bf1-980a-8bba483d52c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed862b-da0f-4ae7-8bc3-1ad9ef7b78ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108dc1f-7273-4284-aea2-e308704405fd",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c34d4-8ce2-4dcc-96b7-8e449e3d201a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb637b25-12fc-4c19-a5bb-4ec87a1c9043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc951a5-9ff6-484c-88f3-5c020ecda05d",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e23d9-a897-49b9-8aae-9397b51268b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992768d6-695d-4273-a8f8-0ff8a3d6edc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ce79d-0efb-4047-b8d1-cfbf36ebc171",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c953d-857c-4412-b6df-059992b89928",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a85e08-6647-4b4d-bf34-f475361e8b9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4854e2-7e6f-414b-b265-ef87122f819c",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43fd0c-ccd4-4f68-ba3c-0ec2346d64f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10103682-7f3d-4225-9da9-b9271dfb0aac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed4d42-c9dd-40d2-bbb2-2f7e93b3d583",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00ec24-bb4e-49ec-9afe-e4d3d05cf905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36dbee6-ae0d-4285-8e85-2b3b79731b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297bc86-295f-4aa4-84b7-ac27923683b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 2 shot temp 0 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c0a0a-2ed4-4dc8-b0c6-fd87acac0d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['two_shot_tf_idf']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-2-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-2-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee839b1-5d26-4caf-b465-4095df33bdfb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb65ae-ff6d-4390-836a-2a6269ba250e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc11a3-7649-492b-a506-f5ea135bdd0e",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e635c6-e8a7-4e00-9c14-3f35cc0cffbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443df34a-c606-4a08-9a0d-d3d72d7909ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad92aa-9428-4971-8a11-eae8572e767d",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d3ed3-463e-4c47-a1f5-76cfea6b666e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff9059-a71b-465a-ada3-88f32e95cf57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee3a67-2dc4-4557-ace8-6eccf2f1c92c",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131d075-3796-4838-8c05-ab14b9017923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1b745-7523-43fb-a477-a1895675783a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f447261-a8ec-4d3a-bf1a-89b0575287f0",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310c8f4-4380-4076-a226-2b88ed6a1011",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac9075-8724-4fa9-9d78-3d3b5cbde81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9fd24-16a2-4797-a8ac-d19dfd1685be",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e78fa4-570a-4c55-af46-12b97167ac63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a4c85-ff4c-499e-853d-112d123a5a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ec841-af4d-4a24-a68e-c25ab24b3219",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8733d-be76-4bf6-a505-5e78448b8c38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d73f1-18b1-4fd4-999c-54350b5e88bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc1d11-9314-483a-9b14-4012a3adeae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 3 shot temp 0 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e098eb1d-4459-4c8d-a748-4faf28ceca1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['three_shot_tf_idf']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b1875-083f-476d-ad51-68a5db1c88dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c062ef0-b7ce-41ec-8770-876c4c829761",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f73c3-cded-46aa-8998-1c6808b1aead",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175005f8-f215-4c08-b7de-8a3c28c4b405",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ea8c8-1ec3-4f58-aea4-6e895523d31b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98054456-bad3-4761-919e-45e85fb1d508",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93074427-0622-49cd-a3ba-46430882ed25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575aca04-504e-497e-855c-4632ca54fb80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39fe3c-ef1c-400a-856a-61de88374175",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2000e-2974-4ab6-b90a-2e0c47cfa96b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff2686-de39-445e-b746-e82af1e334bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbb94b-1ee4-4b6f-bbd1-9784c9c0a406",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e05e19-6a6c-4c72-824a-b521da42d153",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7f3cb-ff5d-49c1-ab3d-c4c1241a216f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7fa37-4ab6-41ef-8080-465fa5c21e6f",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f117e-8579-4849-962b-359e04f56e2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9160e5e-737a-4c9b-b00b-d8e1b6a4f3e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bac705-a8a3-4179-8a05-c6796c027add",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93045935-e10f-40e7-aa1a-169ff6ecd375",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a18a53-9d32-4ed2-8335-e263363a6062",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfe9c8-0d61-45d1-a281-08f0785237a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 4 shot temp 0 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5741c-b240-4e6d-8243-5c409c5a320e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['four_shot_tf_id']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-4-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-4-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c8a37-1ef1-468b-aca4-cf6d65ccefb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4188b8c-00fb-42db-812b-c01653db1b7c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ef2ca-0110-4fb2-bab5-56b8024f1b35",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718fd748-aae2-46e3-803f-7f975e6ec860",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf436bf-e849-473a-85ea-0ff4c135e090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acab67f1-faee-4d82-8942-3db995264c81",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffde05c0-a847-428d-bb4c-a3318e8656c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b3100-a2ee-4a24-adb3-12680fe3bb3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2b8655-6282-4f2a-a382-6e981bbd6359",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38738354-ce0d-48e5-8f26-7677c66cc147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f840a2a8-01d9-4adb-ba9c-13b0a8bed68d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b77eaff-152f-4b6c-8189-f87943e70f05",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7248180-c59e-4f31-9f88-e0466b71abe1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25833aa-16d3-4765-811b-26069a4b3745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292aea3d-d1a8-455f-a39a-f85de5a9ebec",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1977f8fc-1a93-4a14-83a9-36443226c9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f583467-8e77-4020-b9a5-ce49ab3dc8fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e52334-ddec-4dc7-857f-e7f0e6b3923e",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7015e-1f4f-4636-8f74-4c923f1dd6c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6879ef06-4bda-48ab-ba7f-92fff1faa14b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44077ab5-54a6-478c-a0d0-615f0217ecb0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Metrics for Experiment 2 - 10 shot temp 0.0 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148c40a-deef-4970-8e18-ea41a41a8a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['ten_shot_tf_id']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-10-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-10-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f468b-34ee-44e3-9d9e-a7880b16c386",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d0d21-bdcc-456b-802b-bd9cb0584e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac2a534-c3c3-4bcc-94ff-a20ac4b01799",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b59c26d-8bb9-4e06-b950-13776749e559",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2789bd-7f0c-4d92-b31f-308b0b6e854f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95e3cf82-8530-45f8-aad5-fa6dadab617c",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ea11c0-0f7b-4474-9f1c-e9c451b148e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69ea55-9e87-4d12-9f1a-f29f3b5f1ef6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb633d0b-d74c-4dd0-a95e-326449b8d60c",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe3805a-afe5-4108-8996-67d913472a91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204836f1-715b-4d5e-9b2e-4b95d2e30a3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af974ad4-bc2b-4d80-be8b-0c6a67ba7b50",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e02cbad-167d-423f-ae80-ca8eb5ea7fdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1fa714-1ecb-41e5-9d96-6eedd370bc75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069901e1-f084-4e3b-9a53-ae2fd4481f65",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c79a7-66e6-4226-b655-8a0b35e18d44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff110c-9a63-4d16-af0a-6b9f01851abe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72f7887-a994-41ba-933c-63076f768413",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c9f2d-cd8f-4122-b3dc-9220af00dbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bf4f1e-cc0c-4de3-b368-7252344635ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062259a3-30d8-4a8b-b5b5-175f12beab02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c0d8237-1e96-426c-ad89-8529310ba56f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 3 - 1 shot temp 0 (TF-IDF ony annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32032c4-a145-4999-834c-eaf0ec35f8f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['one_shot_tf_idf_custom']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-1-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-1-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "print('Complete!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b83d1b-bbf1-4b5d-89b9-e1dfa16f809e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad3b181-1201-42ae-8bf3-8cac13f11b81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de5479-bebb-436f-8f9f-745cc97b99f4",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde7a5f-cdcb-41ae-a4c0-16ec60dd704d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd61de5-7d92-49d1-8dee-3fd0f3ed2ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c444a6c9-958d-4135-af45-2dd21774a025",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7d3098-7782-4b96-b546-5860e5315ab7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7f9d51-a642-4724-9ab1-25c0fba8c24d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980b9cce-4915-431c-b21b-aa8e40e600ed",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c9ce61-1c57-438f-a14d-d51b3f5ac7ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2f2c8c-c2d7-4487-8604-8135e7e613a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e6fd7-85a1-490a-bae3-b9fcc9a85830",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cd481f-2394-4b2e-8a95-9fe8e4d71a85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a94ba2-b609-4cb2-8060-e96e7193e793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8872507-31af-43bc-9f6b-36d275276c6c",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc6855e-eb5b-4839-a51d-cc59bdb82e39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62e5bb6-f7e3-494d-8df5-064632c7d740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b68cd-bccd-4c50-9a55-e26d688ea746",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2429c-ca18-4043-945a-457ac0d4ed9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6d8552-ae81-49ea-aeb5-c9ed8c6c402c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca6dc85-93bd-46f9-b8ee-a23c1a698054",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 3 - 2 shot temp 0 (TF-IDF ony annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73b08c4b-49c4-491b-b4ae-e6cbbd4627f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['two_shot_tf_idf_custom']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-2-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-2-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754a37b-37a3-4215-a704-70b3cf099028",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc651eb-0556-4ecc-8a24-8a74810cc411",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a3ea7e-56c6-478f-98c9-982d97ab431f",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60348edc-1846-4b94-97dc-77da943d6ec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7a43c5-ac87-42e1-a5bd-fbf85e68160a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3245af3-bcfa-4cd1-a943-21bd5a6aef23",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950fce1a-2a1d-4694-91e0-5f89af4f7620",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36a3a97-b604-4ba0-b987-01018b16c4de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e013be-46a1-4509-a91c-99a013a9153a",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bd6e33-3caf-40c3-b867-ec4d3196da7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb23220-e847-476d-99e9-9fb00c0046a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f0c2609-4a1b-4281-b78e-f2095c20adf5",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62170b8f-8914-4cbb-b6d4-418893439284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d931eded-5ea1-4681-a615-1b9c011d470b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0ca2cb-e6bf-4231-a4e6-effc34b06e01",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1d5568-b329-449a-9a44-2eb17975a617",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d5127-26b2-43f5-a1ad-7dae12f56287",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cd2eb9-db8a-40be-b076-4cf1efabdaa3",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b50b5-0121-4318-b3e3-ceaf9a61e466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e2092e-5f71-447a-9177-d4caaf4179c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134f5de0-0917-4787-8e69-85e730621bd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 3 - 3 shot temp 0 (TF-IDF ony annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941fa89f-a128-4c7f-aa80-bc5d9bffe6c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['three_shot_tf_idf_custom']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546bd2f-20b2-4d47-839f-0b9d2ee2618f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0dece-915a-497c-ad3e-2806b3bbc539",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45bfede1-a243-4bd0-9d76-816ee6dc7050",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0bb1e-2750-4c08-9999-6c6ee0a4d641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2406c12-1b50-4d06-b9e1-61840f3afee9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7505cd-75d8-4304-9874-e8f8555aef8a",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cfc983-fbd7-4ae0-a42d-41ebc8227edb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fab4638-035c-476e-8ec8-2506afb883d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04376f1c-50f9-4d6a-a7a8-2a9029b48702",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e917e5eb-ff4b-44c0-bd6b-067b30657e83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a969a932-d2ad-493b-a9a1-863411cdf446",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f84bf5a-75ad-4183-b43b-061f1e7b5a57",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d88eeb-0a99-4e31-895c-d7a8d4e97987",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8813c67b-0b01-4b74-bfec-f82b7d22f22f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f789624-0513-43a4-a6d6-90d71dd3e2a4",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee69a64-52ab-4d5d-93cb-572c4ff05d89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6caaee-564b-4dda-a51e-2b60690f6349",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be8852b-1a65-4df3-98d0-e3a95d15c47d",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f9cf5a-6d79-4015-83c4-2113231ec62c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b7669-34bc-41c5-9eea-2a310450a1b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34718243-ffe2-40c3-8eb4-c3a64ed722b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 3 - 4 shot temp 0 (TF-IDF ony annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c56538d-a701-4776-9a0b-489c4e8a123d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['four_shot_tf_id_custom']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-4-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-4-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be054a01-3823-4842-8652-0e552ade5316",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73419c7-2574-42b3-8624-5535ea1c3214",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa579291-6c50-4040-8bdf-3cb01ea0c404",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80a8618-67df-45b9-8f96-392cea40b9fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d57f49d-f1ca-4f91-af5c-623dcfcf0378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23690b1a-e4ad-4963-894e-9b8637b92c12",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c52fd-cea7-4348-a0e5-732b5f0e1a1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2016ad72-645c-4b7a-b944-91e8f3201b60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55336070-8f9c-42b3-bff9-37ede6164eaf",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4baa1884-879c-4817-a661-93d702c356e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b5e356-5038-455e-9327-c08ebcf50c33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b584b3-c3d0-4b9b-b1c0-e59bb5fec556",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f385741-105f-41bb-bb20-afb6ab3f3a99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09813990-02a9-46c7-881e-2a118145279f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5981ef-2458-4c0d-8eff-1dea8554888c",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1438b4-0c71-4ebe-b76a-06474e126bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82cf85c-053d-4d85-b867-7cd3a4f1208a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442900e3-12b5-4d2b-b43d-0b4efd9b7850",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e4184-d9fc-4b0a-b65e-4983a545d341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6bc6f0-a282-4788-906f-50a1d221a7a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b1158d-8625-4733-b2d9-cce8ba796f18",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 3 - 10 shot temp 0 (TF-IDF ony annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d443742d-2bfc-4683-94a7-31135f1df610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['ten_shot_tf_idf_custom']\n",
    "\n",
    "path = f'../llama-outputs-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-10-shot'\n",
    "metrics_path = f'../metrics-result-augmented/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-10-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede917e9-c607-4712-89ca-8cedc72125bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c7da8a-9714-4847-99e5-df6d2d346672",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2280450-f169-4825-8a45-3942185bbab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ef49849-ddf1-4a32-9db6-5e5801420c3a",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7f43c7-a86a-4579-8e85-5705f6e1da93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a8a96b-2d46-4536-8ff7-669225a72f2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3229e72d-3d33-4f27-9926-39d51264216d",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfb155d-45ee-4ee9-844b-e93e223f33ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd451772-9288-407c-9bb3-5b9e23db70b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999855cf-d99c-4891-aa86-a49e4980a7e6",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ac2ea6-c7fe-475b-ab3a-4f1cc4fbe750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871dd1d2-5d23-41a9-8732-e1a32ac32f25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411bafeb-6584-43a0-b57f-aabef9b9eaee",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8b78ee-cec2-4b51-84f4-ca52988510f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528ee56d-885b-45cb-9baa-f545ce2f841e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd08cb4-1789-4b12-af2d-7fb117d69d15",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf1eac-1481-48ce-b3ef-4a37137e54f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe753c17-5bae-4024-bf8f-1deecaa4a639",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192eaa2d-869e-4118-bffc-73c597c9d2c0",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27102730-f9a8-4159-9b50-9818af868a7d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7679a-0786-4231-b0dc-11acf50ed8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e3a67a-0bad-4221-9ea6-ad753410a9dd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save and compress metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2fb9b2-754f-45f5-a4b4-d1beb35fc150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40a13b4-f752-43fe-9a96-b7fcac9d2f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "#Create a DataFrame object\n",
    "df_metrics = pd.DataFrame(rows_metrics_report,\n",
    "                  columns = ['approach','class_weight' , 'class_mean', 'ann_weight' , 'ann_mean','precision_weight','precision_mean','recall_weight','recall_mean','f1_weight','f1_mean','global_weight','global_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c680f3-b0ed-4c86-aa31-76b3bf552c84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_metrics.to_csv('df_metrics_no_short_augmented.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecea07-52cd-4839-b909-d6e87d9ad0ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "# make_tarfile('metrics_result.tar.gz','metrics-result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf061f2-cc6d-4e80-8fdf-cb08f3e61248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T18:07:47.222295Z",
     "iopub.status.busy": "2024-09-10T18:07:47.221921Z",
     "iopub.status.idle": "2024-09-10T18:08:33.315075Z",
     "shell.execute_reply": "2024-09-10T18:08:33.312381Z",
     "shell.execute_reply.started": "2024-09-10T18:07:47.222254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('llama-outputs-augmented.tar.gz','llama-outputs-augmented')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245661e1-e536-4c53-964e-0bb2519d4f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_tarfile('metrics-result-augmented.tar.gz','../metrics-result-augmented')"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
