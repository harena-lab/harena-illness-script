{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276c8d1-85d7-4046-875b-69029f4fbafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e328da-ff80-4652-9611-498c4c834e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import re\n",
    "import ast\n",
    "\n",
    "def get_examples(file_path):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        data = [line for line in reader]\n",
    "    return data\n",
    "def get_from_annotated_dataset(annotated_dataset,_id):\n",
    "    for doc in annotated_dataset:\n",
    "        if doc['doc_id'] == _id:\n",
    "            return doc\n",
    "annotated_dataset = get_examples('teste-progresso/annotations-medical_specialist-dpoc-bio-composed-multiple.jsonl')\n",
    "df_data_info = pd.read_csv('test_data_info_no_short.csv')\n",
    "\n",
    "#REMOVE ADDITIONAL BAD DATA\n",
    "# df_data_info.drop(df_data_info[df_data_info['doc_id'] == '8396380d-e0b6-4b81-8fe9-0b99c611f9f3'].index, inplace=True)\n",
    "# df_data_info.reset_index(drop=True,inplace=True)\n",
    "def replace_substring(string, start, end, replacement):\n",
    "    # Check if start and end are valid indices for the string\n",
    "    if start < 0 or end > len(string) or start > end:\n",
    "        return \"Invalid start or end index\"\n",
    "\n",
    "    # Replace the substring from start to end with the replacement string\n",
    "    new_string = string[:start] + replacement + string[end:]\n",
    "\n",
    "    return new_string\n",
    "def add_quotes(item):\n",
    "    if (not (item.startswith('\"') and item.endswith('\"')) and not (item.startswith(\"'\") and item.endswith(\"'\"))):\n",
    "        if (item.startswith('\"') and not item.endswith('\"')) or (item.startswith(\"'\") and not item.endswith(\"'\")):\n",
    "            item = item.replace('\"', '\\\\\"').replace(\"'\", \"\\\\'\")\n",
    "    return item\n",
    "def fix_quotes(text):\n",
    "    # Regular expression to find lists and their items   \n",
    "    pattern = r\"\"\"\\[\\s*(([\"'][^\"']+[\"'])(?:\\s+[^,]+)?)\\s*,\\s*(\\[\\s*[\"'][^\"']+?[\"'](?:\\s*,\\s*[\"'][^\"']+?[\"'])*\\s*\\])\\s*\\]\"\"\"\n",
    "    annotation_list = {\"annotations\":[]}\n",
    "    for match in re.finditer(pattern, text):        \n",
    "        main_text = add_quotes(match.group(1))\n",
    "        if main_text.startswith('\"') and main_text.endswith('\"') or (main_text.startswith(\"'\") and main_text.endswith(\"'\")):\n",
    "            main_text = main_text[1:-1]\n",
    "        current_list = [str(match.group(1)), ast.literal_eval(match.group(3))]\n",
    "        current_list[0] = current_list[0][1:-1]\n",
    "        annotation_list[\"annotations\"].append(current_list)\n",
    "    annotation_list = str(annotation_list)\n",
    "    return annotation_list\n",
    "\n",
    "def select_after_first_brace(string):\n",
    "    pattern = r\"(|[\\'\\\"])(annotations)([\\'\\\"]|)\"\n",
    "    \n",
    "    matches = re.search(pattern, string)\n",
    "    if matches != None:\n",
    "        string = replace_substring(string, matches.span()[0], matches.span()[1], '\"annotations\"')\n",
    "    string.replace(\"]\\']\",']]')\n",
    "    string.replace(\"]\\\"]\",']]')\n",
    "    # print('STRING AFTER FIRST REGEX:', string)\n",
    "    pattern = r'\"annotations\":\\[.*\\]\\]'\n",
    "    match = re.search(pattern, string)\n",
    "    if match != None:\n",
    "        # string = replace_substring(string, match.span()[0], match.span()[1], '\"annotations\":[')\n",
    "        string = match.group(0)\n",
    "        # print('STRING AFTER SECOND REGEX:', string)\n",
    "\n",
    "    brace_index = string.find('\"annotations\"')\n",
    "    string = '{'+string[brace_index:]\n",
    "    # print('trying to select correct part of models output')\n",
    "    # print(brace_index)\n",
    "    # print(string)\n",
    "    if brace_index == -1:\n",
    "        return '{\"annotations\":[]}'\n",
    "    else:\n",
    "        if string[-3:] == \"']]\":\n",
    "            string = string[:-3]+\"']]]\"\n",
    "        if string.find(\"}\") == -1:\n",
    "            end_annotation_index = string.find(']]]')\n",
    "            if string[-1:] != ']' and end_annotation_index != -1:                \n",
    "                string = string[:end_annotation_index+3]+'}'\n",
    "            elif string[-1:] != ']' and end_annotation_index == -1:\n",
    "                end_annotation_index = string.find(']] ]')\n",
    "                if end_annotation_index != -1:\n",
    "                    string = string[:end_annotation_index+4]+'}'\n",
    "                else:\n",
    "                    string = string+']}'\n",
    "            elif string[-7:].count(']') < 3:\n",
    "                string = string+']}'\n",
    "            else:\n",
    "                string = string+'}'\n",
    "        else:\n",
    "            string = string[:string.find(\"}\")+1]\n",
    "        # print('results...')\n",
    "        # print(string)\n",
    "\n",
    "\n",
    "        string = fix_quotes(string)\n",
    "        return string\n",
    "# prediction_annotation = eval(model_response[0])\n",
    "def prediction_to_labels(prediction_labels, data_info):\n",
    "    if type(prediction_labels) == list:\n",
    "        ze = prediction_labels[0]\n",
    "        prediction_labels = ze\n",
    "    prediction_labels = select_after_first_brace(prediction_labels)\n",
    "    prediction_annotation = ast.literal_eval(prediction_labels)\n",
    "    full_text = data_info['text']\n",
    "    text_tokenized = data_info['labels']\n",
    "    categorized_prediction = annotation_to_tokens(full_text, text_tokenized, prediction_annotation)\n",
    "    labels = extract_labels_from_prediction(categorized_prediction)\n",
    "    return labels\n",
    "def truth_to_labels(data_info):\n",
    "    labels = extract_labels_from_truth(data_info['labels'])\n",
    "    return labels\n",
    "def extract_labels_from_truth (data_info):\n",
    "    text_tokenized = data_info\n",
    "    categories = []\n",
    "    for token in text_tokenized:\n",
    "        if token[4] != None:\n",
    "            categories.append(list(token[4].keys()))\n",
    "        else:\n",
    "            categories.append('0')\n",
    "    return categories\n",
    "def annotation_to_tokens (full_text, text_tokenized, prediction_annotation):\n",
    "    clean_text_tokenized = [[token[0],token[1],token[2]] for token in text_tokenized]\n",
    "    annotations = prediction_annotation['annotations']\n",
    "    for annotation in annotations:\n",
    "        # print('============= new annotation', annotation[0])\n",
    "        start_pos = full_text.find(annotation[0])\n",
    "        end_pos = len(annotation[0])+start_pos-1\n",
    "        # print(f'end pos is {len(annotation)} + {start_pos} - 1 = {end_pos}')\n",
    "        categorizing = False\n",
    "        \n",
    "        for token in clean_text_tokenized:\n",
    "            # print(f'token pos {token[1]} annotation pos {start_pos} token {token[0]}')\n",
    "            if token[1] == start_pos:\n",
    "                # print('starting categorization...')\n",
    "                # print(f'start pos {start_pos} end pos {end_pos} token {token[0]}')\n",
    "                categorizing = True\n",
    "            if categorizing:\n",
    "                #adds category to token\n",
    "                # print(f'adding category {annotation[1]} to token {token[0]}')\n",
    "                token.append(annotation[1])\n",
    "                if token[2] == end_pos:\n",
    "                    # print(f'ending categorization at {token[0]}...')\n",
    "                    categorizing = False\n",
    "                    break\n",
    "            \n",
    "            # print(token)\n",
    "    return clean_text_tokenized\n",
    "def extract_labels_from_prediction (categorized_prediction):\n",
    "    labels = []\n",
    "    for token in categorized_prediction:\n",
    "        if len(token) > 3:\n",
    "            labels.append(token[3])\n",
    "        else:\n",
    "            labels.append('0')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0692ed-4f0f-48b2-b4ac-3a08cade757a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert labels of a annotation to binary vector\n",
    "cats = {'pathophysiology':0,\n",
    "        'etiology':1,\n",
    "        'epidemiology':2,\n",
    "        'history':3,\n",
    "        'physical':4,\n",
    "        'exams':5,\n",
    "        'differential':6,\n",
    "        'therapeutic':7}\n",
    "\n",
    "def label2binary(labels):\n",
    "    vet = [0] * 8\n",
    "    # print(vet)\n",
    "    for label in labels:\n",
    "        if label in list(cats.keys()):\n",
    "            vet[cats[label]] = 1\n",
    "    return vet\n",
    "\n",
    "# Convert all the text - divided in tokens - labels to binary vectors\n",
    "def preprocess_classification(classi_data):\n",
    "    # print('preprocess classification ======= ',classi_data)\n",
    "    for index in range(len(classi_data)):\n",
    "        classi_data[index] = label2binary(classi_data[index])\n",
    "    return classi_data\n",
    "# Convert all the text - divided in tokens - labels to boolean\n",
    "def preprocess_annotation(ann_data):\n",
    "    for index in range(len(ann_data)):\n",
    "        ann_data[index] = 1 if sum(label2binary(ann_data[index])) >= 1 else 0\n",
    "    return ann_data\n",
    "\n",
    "def get_substrings_from_text(text):\n",
    "    substring = text.split(\"\\n\")\n",
    "    return substring\n",
    "\n",
    "def find_complete_text_from_substring(substring, texts):\n",
    "    complete_text = None\n",
    "    count = 0\n",
    "    for text in texts:\n",
    "        if substring[0] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "        elif substring[0][:100] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "        elif substring[0].split('.')[0] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "    if count == 1:\n",
    "        return complete_text\n",
    "    elif count > 1 and len(substring) > 1:\n",
    "        substring.pop(0)\n",
    "        find_complete_text_from_substring(substring, texts)\n",
    "    else:\n",
    "        print('not FOUND', substring[0][:60])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f40c0-bb96-458a-ae3c-f6c374a6d1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b78f1-9416-45a3-b034-4f3eda7d46d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 5 - Random Examples (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c440fe-f29d-4983-a9d7-45cbc962e909",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('../annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_random']\n",
    "    path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-{i_shot}-shot'\n",
    "    metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "        \n",
    "    exisiting_ids = []\n",
    "    df_data_info['truth'] = ''\n",
    "    df_data_info['truth_annotation'] = ''\n",
    "    df_data_info['prediction'] = ''\n",
    "    df_data_info['prediction_annotation'] = ''\n",
    "    for name in file_names:\n",
    "        # print(name)\n",
    "        llama_annotated = ''\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "            truth_labels = truth_to_labels(truth_data)\n",
    "            # print(doc_id)\n",
    "            pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "            pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "            # print(pred_classification_vector)\n",
    "            # print(\"==========\")\n",
    "            truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "            # print(truth_classification_vector)\n",
    "            # print('######3')\n",
    "            truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "            print(truth_general_annotation_vector)\n",
    "            \n",
    "    \n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "    df_data_info['truth'] = list(df_data_info['truth'])\n",
    "    df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "    df_data_info['prediction'] = df_data_info['prediction']\n",
    "    df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "    # df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "    \n",
    "    aux = []\n",
    "    aux2 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for index in tqdm(range(len(df_data_info))):\n",
    "        truth = eval(df_data_info[\"truth\"][index])\n",
    "        prediction = eval(df_data_info[\"prediction\"][index])\n",
    "        truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "        prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "        value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "        aux.append([value,len(truth)])\n",
    "        value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "        aux2.append([value2,len(truth)])\n",
    "        \n",
    "        # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "    \n",
    "        # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "        precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        precision.append([precision_now,len(truth)])\n",
    "    \n",
    "        recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        recall.append([recall_now,len(truth)])\n",
    "    \n",
    "        f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        f1.append([f1_now,len(truth)])\n",
    "    \n",
    "    df_data_info[\"label_score\"] = aux\n",
    "    df_data_info[\"annotation_score\"] = aux2\n",
    "    df_data_info[\"precision\"] = precision\n",
    "    df_data_info[\"recall\"] = recall\n",
    "    df_data_info[\"f1\"] = f1\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n",
    "\n",
    "    #Ponderada\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "    \n",
    "    label_score_weight = score/count\n",
    "    row_metrics.append(label_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    label_score_mean = score/count\n",
    "    row_metrics.append(label_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    annotation_score_weight = score/count\n",
    "    row_metrics.append(annotation_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    annotation_score_mean = score/count\n",
    "    row_metrics.append(annotation_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    precision_weight = score/count\n",
    "    row_metrics.append(precision_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    precision_mean = score/count\n",
    "    row_metrics.append(precision_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    recall_weight = score/count\n",
    "    row_metrics.append(recall_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    recall_mean = score/count\n",
    "    row_metrics.append(recall_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    f1_weight = score/count\n",
    "    row_metrics.append(f1_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    f1_mean = score/count\n",
    "    row_metrics.append(f1_mean)\n",
    "    \n",
    "    weight_score = annotation_score_weight * label_score_weight\n",
    "    mean_score = annotation_score_mean * label_score_mean\n",
    "    \n",
    "    row_metrics.append(weight_score)\n",
    "    \n",
    "    row_metrics.append(mean_score)\n",
    "    rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a51686c",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 1 - 0-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3c4436",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "print('starting ',0,'shot ================================')\n",
    "df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "row_metrics = [f'0_shot']\n",
    "path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "    \n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    # print(name)\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        \n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n",
    "\n",
    "#Ponderada\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "row_metrics.append(annotation_score_weight)\n",
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "row_metrics.append(annotation_score_mean)\n",
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "row_metrics.append(precision_weight)\n",
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "row_metrics.append(precision_mean)\n",
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "row_metrics.append(recall_weight)\n",
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "row_metrics.append(recall_mean)\n",
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "row_metrics.append(f1_weight)\n",
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "row_metrics.append(f1_mean)\n",
    "\n",
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "\n",
    "row_metrics.append(mean_score)\n",
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d6a5f",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 1 - Static Examples (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6fecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_static_temp_0.0']\n",
    "    path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-{i_shot}-shot'\n",
    "    metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "        \n",
    "    exisiting_ids = []\n",
    "    df_data_info['truth'] = ''\n",
    "    df_data_info['truth_annotation'] = ''\n",
    "    df_data_info['prediction'] = ''\n",
    "    df_data_info['prediction_annotation'] = ''\n",
    "    for name in file_names:\n",
    "        # print(name)\n",
    "        llama_annotated = ''\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "            truth_labels = truth_to_labels(truth_data)\n",
    "            # print(doc_id)\n",
    "            pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "            pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "            # print(pred_classification_vector)\n",
    "            # print(\"==========\")\n",
    "            truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "            # print(truth_classification_vector)\n",
    "            # print('######3')\n",
    "            truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "            # print(truth_general_annotation_vector)\n",
    "            \n",
    "    \n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "    df_data_info['truth'] = list(df_data_info['truth'])\n",
    "    df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "    df_data_info['prediction'] = df_data_info['prediction']\n",
    "    df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "    # df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "    \n",
    "    aux = []\n",
    "    aux2 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for index in tqdm(range(len(df_data_info))):\n",
    "        truth = eval(df_data_info[\"truth\"][index])\n",
    "        prediction = eval(df_data_info[\"prediction\"][index])\n",
    "        truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "        prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "        value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "        aux.append([value,len(truth)])\n",
    "        value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "        aux2.append([value2,len(truth)])\n",
    "        \n",
    "        # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "    \n",
    "        # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "        precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        precision.append([precision_now,len(truth)])\n",
    "    \n",
    "        recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        recall.append([recall_now,len(truth)])\n",
    "    \n",
    "        f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        f1.append([f1_now,len(truth)])\n",
    "    \n",
    "    df_data_info[\"label_score\"] = aux\n",
    "    df_data_info[\"annotation_score\"] = aux2\n",
    "    df_data_info[\"precision\"] = precision\n",
    "    df_data_info[\"recall\"] = recall\n",
    "    df_data_info[\"f1\"] = f1\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n",
    "\n",
    "    #Ponderada\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "    \n",
    "    label_score_weight = score/count\n",
    "    row_metrics.append(label_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    label_score_mean = score/count\n",
    "    row_metrics.append(label_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    annotation_score_weight = score/count\n",
    "    row_metrics.append(annotation_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    annotation_score_mean = score/count\n",
    "    row_metrics.append(annotation_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    precision_weight = score/count\n",
    "    row_metrics.append(precision_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    precision_mean = score/count\n",
    "    row_metrics.append(precision_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    recall_weight = score/count\n",
    "    row_metrics.append(recall_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    recall_mean = score/count\n",
    "    row_metrics.append(recall_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    f1_weight = score/count\n",
    "    row_metrics.append(f1_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    f1_mean = score/count\n",
    "    row_metrics.append(f1_mean)\n",
    "    \n",
    "    weight_score = annotation_score_weight * label_score_weight\n",
    "    mean_score = annotation_score_mean * label_score_mean\n",
    "    \n",
    "    row_metrics.append(weight_score)\n",
    "    \n",
    "    row_metrics.append(mean_score)\n",
    "    rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7d71b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1936b888",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 2 - TF-IDF (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4af837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_tf_id']\n",
    "\n",
    "    path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-{i_shot}-shot'\n",
    "    metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "        \n",
    "    exisiting_ids = []\n",
    "    df_data_info['truth'] = ''\n",
    "    df_data_info['truth_annotation'] = ''\n",
    "    df_data_info['prediction'] = ''\n",
    "    df_data_info['prediction_annotation'] = ''\n",
    "    for name in file_names:\n",
    "        # print(name)\n",
    "        llama_annotated = ''\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "            truth_labels = truth_to_labels(truth_data)\n",
    "            # print(doc_id)\n",
    "            pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "            pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "            # print(pred_classification_vector)\n",
    "            # print(\"==========\")\n",
    "            truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "            # print(truth_classification_vector)\n",
    "            # print('######3')\n",
    "            truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "            # print(truth_general_annotation_vector)\n",
    "            \n",
    "    \n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "    df_data_info['truth'] = list(df_data_info['truth'])\n",
    "    df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "    df_data_info['prediction'] = df_data_info['prediction']\n",
    "    df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "    # df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "    \n",
    "    aux = []\n",
    "    aux2 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for index in tqdm(range(len(df_data_info))):\n",
    "        truth = eval(df_data_info[\"truth\"][index])\n",
    "        prediction = eval(df_data_info[\"prediction\"][index])\n",
    "        truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "        prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "        value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "        aux.append([value,len(truth)])\n",
    "        value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "        aux2.append([value2,len(truth)])\n",
    "        \n",
    "        # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "    \n",
    "        # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "        precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        precision.append([precision_now,len(truth)])\n",
    "    \n",
    "        recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        recall.append([recall_now,len(truth)])\n",
    "    \n",
    "        f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        f1.append([f1_now,len(truth)])\n",
    "    \n",
    "    df_data_info[\"label_score\"] = aux\n",
    "    df_data_info[\"annotation_score\"] = aux2\n",
    "    df_data_info[\"precision\"] = precision\n",
    "    df_data_info[\"recall\"] = recall\n",
    "    df_data_info[\"f1\"] = f1\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n",
    "\n",
    "    #Ponderada\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "    \n",
    "    label_score_weight = score/count\n",
    "    row_metrics.append(label_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    label_score_mean = score/count\n",
    "    row_metrics.append(label_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    annotation_score_weight = score/count\n",
    "    row_metrics.append(annotation_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    annotation_score_mean = score/count\n",
    "    row_metrics.append(annotation_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    precision_weight = score/count\n",
    "    row_metrics.append(precision_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    precision_mean = score/count\n",
    "    row_metrics.append(precision_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    recall_weight = score/count\n",
    "    row_metrics.append(recall_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    recall_mean = score/count\n",
    "    row_metrics.append(recall_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    f1_weight = score/count\n",
    "    row_metrics.append(f1_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    f1_mean = score/count\n",
    "    row_metrics.append(f1_mean)\n",
    "    \n",
    "    weight_score = annotation_score_weight * label_score_weight\n",
    "    mean_score = annotation_score_mean * label_score_mean\n",
    "    \n",
    "    row_metrics.append(weight_score)\n",
    "    \n",
    "    row_metrics.append(mean_score)\n",
    "    rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fd01b4",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 3 - TF-IDF Custom (similarity by text VS annotation) (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f35efa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_tf_id_custom']\n",
    "\n",
    "    path = f'../llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-{i_shot}-shot'\n",
    "    metrics_path = f'../metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "        \n",
    "    exisiting_ids = []\n",
    "    df_data_info['truth'] = ''\n",
    "    df_data_info['truth_annotation'] = ''\n",
    "    df_data_info['prediction'] = ''\n",
    "    df_data_info['prediction_annotation'] = ''\n",
    "    for name in file_names:\n",
    "        # print(name)\n",
    "        llama_annotated = ''\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "            truth_labels = truth_to_labels(truth_data)\n",
    "            # print(doc_id)\n",
    "            pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "            pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "            # print(pred_classification_vector)\n",
    "            # print(\"==========\")\n",
    "            truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "            # print(truth_classification_vector)\n",
    "            # print('######3')\n",
    "            truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "            # print(truth_general_annotation_vector)\n",
    "            \n",
    "    \n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "    df_data_info['truth'] = list(df_data_info['truth'])\n",
    "    df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "    df_data_info['prediction'] = df_data_info['prediction']\n",
    "    df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "    # df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "    \n",
    "    aux = []\n",
    "    aux2 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for index in tqdm(range(len(df_data_info))):\n",
    "        truth = eval(df_data_info[\"truth\"][index])\n",
    "        prediction = eval(df_data_info[\"prediction\"][index])\n",
    "        truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "        prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "        value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "        aux.append([value,len(truth)])\n",
    "        value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "        aux2.append([value2,len(truth)])\n",
    "        \n",
    "        # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "    \n",
    "        # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "        precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        precision.append([precision_now,len(truth)])\n",
    "    \n",
    "        recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        recall.append([recall_now,len(truth)])\n",
    "    \n",
    "        f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        f1.append([f1_now,len(truth)])\n",
    "    \n",
    "    df_data_info[\"label_score\"] = aux\n",
    "    df_data_info[\"annotation_score\"] = aux2\n",
    "    df_data_info[\"precision\"] = precision\n",
    "    df_data_info[\"recall\"] = recall\n",
    "    df_data_info[\"f1\"] = f1\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n",
    "\n",
    "    #Ponderada\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "    \n",
    "    label_score_weight = score/count\n",
    "    row_metrics.append(label_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    label_score_mean = score/count\n",
    "    row_metrics.append(label_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    annotation_score_weight = score/count\n",
    "    row_metrics.append(annotation_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    annotation_score_mean = score/count\n",
    "    row_metrics.append(annotation_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    precision_weight = score/count\n",
    "    row_metrics.append(precision_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    precision_mean = score/count\n",
    "    row_metrics.append(precision_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    recall_weight = score/count\n",
    "    row_metrics.append(recall_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    recall_mean = score/count\n",
    "    row_metrics.append(recall_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    f1_weight = score/count\n",
    "    row_metrics.append(f1_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    f1_mean = score/count\n",
    "    row_metrics.append(f1_mean)\n",
    "    \n",
    "    weight_score = annotation_score_weight * label_score_weight\n",
    "    mean_score = annotation_score_mean * label_score_mean\n",
    "    \n",
    "    row_metrics.append(weight_score)\n",
    "    \n",
    "    row_metrics.append(mean_score)\n",
    "    rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57357656-8f35-477d-abd8-a0fc9c9f1826",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save and compress metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394fc04-78be-40e8-b8c0-c9cb6cf58cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.747975Z",
     "iopub.status.busy": "2024-07-18T14:18:49.747396Z",
     "iopub.status.idle": "2024-07-18T14:18:49.835540Z",
     "shell.execute_reply": "2024-07-18T14:18:49.834452Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.747929Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448eb28-fea4-443b-a980-2bf8b01c842a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.839204Z",
     "iopub.status.busy": "2024-07-18T14:18:49.838571Z",
     "iopub.status.idle": "2024-07-18T14:18:49.923444Z",
     "shell.execute_reply": "2024-07-18T14:18:49.922646Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.839142Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "#Create a DataFrame object\n",
    "df_metrics = pd.DataFrame(rows_metrics_report,\n",
    "                  columns = ['experiment','class_weight' , 'class_mean', 'ann_weight' , 'ann_mean','precision_weight','precision_mean','recall_weight','recall_mean','f1_weight','f1_mean','global_weight','global_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fc2dd-ad58-4dc4-b46f-25c273e2e7d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.932276Z",
     "iopub.status.busy": "2024-07-18T14:18:49.928936Z",
     "iopub.status.idle": "2024-07-18T14:18:50.043173Z",
     "shell.execute_reply": "2024-07-18T14:18:50.041988Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.929664Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_metrics.to_csv('df_metrics_no_short.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecea07-52cd-4839-b909-d6e87d9ad0ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T18:07:41.022819Z",
     "iopub.status.busy": "2024-09-10T18:07:41.022127Z",
     "iopub.status.idle": "2024-09-10T18:07:47.220312Z",
     "shell.execute_reply": "2024-09-10T18:07:47.218159Z",
     "shell.execute_reply.started": "2024-09-10T18:07:41.022784Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "make_tarfile('metrics_result.tar.gz','../metrics-result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf061f2-cc6d-4e80-8fdf-cb08f3e61248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T18:07:47.222295Z",
     "iopub.status.busy": "2024-09-10T18:07:47.221921Z",
     "iopub.status.idle": "2024-09-10T18:08:33.315075Z",
     "shell.execute_reply": "2024-09-10T18:08:33.312381Z",
     "shell.execute_reply.started": "2024-09-10T18:07:47.222254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('../llama-outputs.tar.gz','../llama-outputs')"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
