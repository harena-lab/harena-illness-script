{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d276c8d1-85d7-4046-875b-69029f4fbafa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install jsonlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e328da-ff80-4652-9611-498c4c834e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import jaccard_score, precision_score, recall_score, f1_score\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def get_examples(file_path):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        data = [line for line in reader]\n",
    "    return data\n",
    "def get_from_annotated_dataset(annotated_dataset,_id):\n",
    "    for doc in annotated_dataset:\n",
    "        if doc['doc_id'] == _id:\n",
    "            return doc\n",
    "annotated_dataset = get_examples('teste-progresso/annotations-medical_specialist-dpoc-bio-composed-multiple.jsonl')\n",
    "df_data_info = pd.read_csv('test_data_info_no_short.csv')\n",
    "\n",
    "def escape_inner_quotes(text):\n",
    "    pattern = r\"\"\"(?<=:)\\s*[^\"']*[\"|'](.*?)(?<!\\\\)[\"|'](?=\\s*[,}])\"\"\"\n",
    "    escaped_text = re.sub(pattern, lambda m: \"'\" + m.group(1).replace('\"', '\\\\\"') + \"'\", text)\n",
    "    return escaped_text\n",
    "####### Function that extracts the entities and respective categories and transforms into a dictionary of annotations  #############\n",
    "def transform_augmented_data_to_pattern(data_info):\n",
    "    # print(data_info)\n",
    "    data_info = data_info.replace('```','')\n",
    "    data_info = re.sub(r'(?<![\"\\\\]):', r'\\\\:', data_info)\n",
    "    data_info = data_info.replace(\"\\n\", \" \").replace(\"  \", \" \")\n",
    "    closing_braces = data_info.find(\"}\")\n",
    "    # print(data_info[-3:])\n",
    "    # print(data_info[-4:])\n",
    "    # print(data_info[:closing_braces-1])\n",
    "    # print('========')\n",
    "    # print(data_info.find('\"}'))\n",
    "    # print(data_info.find('\" }'))\n",
    "    # print(closing_braces)\n",
    "    if closing_braces == -1:\n",
    "        # print('======= didnt found closing braces')\n",
    "        data_info = data_info[:len(data_info)] + '}'\n",
    "        # print('======= after insertion',data_info)\n",
    "        closing_braces = data_info.find(\"}\")\n",
    "    if (data_info.find('\"}') == -1 and data_info.find('\" }') == -1):\n",
    "        data_info = data_info[:closing_braces-1] + '\"}'\n",
    "    data_info = escape_inner_quotes(data_info)\n",
    "    # print('aaaaaaaaaaaaaa')\n",
    "    # print(data_info)\n",
    "    data_info = ast.literal_eval(data_info)\n",
    "    # data_info = clean_and_parse_string(data_info)\n",
    "    # print(type(data_info))\n",
    "    \n",
    "    pattern = r'\\[([^\\[\\]]+)\\s*\\|\\s*([^\\[\\]]+)\\]'\n",
    "    matches = re.findall(pattern, data_info['annotations'])\n",
    "    annotations = {\"annotations\": [[match[0], [val.strip() for val in match[1].split(',')]] for match in matches]}\n",
    "    return annotations\n",
    "\n",
    "#REMOVE ADDITIONAL BAD DATA\n",
    "# df_data_info.drop(df_data_info[df_data_info['doc_id'] == '8396380d-e0b6-4b81-8fe9-0b99c611f9f3'].index, inplace=True)\n",
    "# df_data_info.reset_index(drop=True,inplace=True)\n",
    "def replace_substring(string, start, end, replacement):\n",
    "    # Check if start and end are valid indices for the string\n",
    "    if start < 0 or end > len(string) or start > end:\n",
    "        return \"Invalid start or end index\"\n",
    "\n",
    "    # Replace the substring from start to end with the replacement string\n",
    "    new_string = string[:start] + replacement + string[end:]\n",
    "\n",
    "    return new_string\n",
    "def select_after_first_brace(string):\n",
    "    pattern = r\"(|[\\'\\\"])(annotations)([\\'\\\"]|)\"\n",
    "    \n",
    "    matches = re.search(pattern, string)\n",
    "    if matches != None:\n",
    "        string = replace_substring(string, matches.span()[0], matches.span()[1], '\"annotations\"')\n",
    "    string.replace(\"]\\']\",']]')\n",
    "    string.replace(\"]\\\"]\",']]')\n",
    "    # print('STRING AFTER FIRST REGEX:', string)\n",
    "    pattern = r'\"annotations\"\\s*=\\s*\\['\n",
    "    match = re.search(pattern, string)\n",
    "    if match != None:\n",
    "        string = replace_substring(string, match.span()[0], match.span()[1], '\"annotations\":[')\n",
    "        # print('STRING AFTER SECOND REGEX:', string)\n",
    "\n",
    "    brace_index = string.find('\"annotations\"')\n",
    "    string = '{'+string[brace_index:]\n",
    "    # print('trying to select correct part of models output')\n",
    "    # print(brace_index)\n",
    "    # print(string)\n",
    "    if string[-3:] == \"']]\":\n",
    "        string = string[:-3]+\"']]]\"\n",
    "    if string.find(\"}\") == -1:\n",
    "        end_annotation_index = string.find(']]]')\n",
    "\n",
    "        if string[-1:] != ']' and end_annotation_index != -1:\n",
    "            string = string[:end_annotation_index+3]+'}'\n",
    "        elif string[-1:] != ']' and end_annotation_index == -1:\n",
    "            end_annotation_index = string.find(']] ]')\n",
    "\n",
    "            string = string[:end_annotation_index+4]+'}'\n",
    "        else:\n",
    "            string = string+'}'\n",
    "    else:\n",
    "        string = string[:string.find(\"}\")+1]\n",
    "    # print('results...')\n",
    "    # print(string)\n",
    "    return string\n",
    "# prediction_annotation = eval(model_response[0])\n",
    "def prediction_to_labels(prediction_labels, data_info):\n",
    "    prediction_labels = transform_augmented_data_to_pattern(prediction_labels)\n",
    "    if type(prediction_labels) == list:\n",
    "        ze = prediction_labels[0]\n",
    "        prediction_labels = ze\n",
    "    prediction_labels = select_after_first_brace(str(prediction_labels))\n",
    "    prediction_annotation = eval(prediction_labels)\n",
    "    full_text = data_info['text']\n",
    "    text_tokenized = data_info['labels']\n",
    "    categorized_prediction = annotation_to_tokens(full_text, text_tokenized, prediction_annotation)\n",
    "    labels = extract_labels_from_prediction(categorized_prediction)\n",
    "    return labels\n",
    "def truth_to_labels(data_info):\n",
    "    labels = extract_labels_from_truth(data_info['labels'])\n",
    "    return labels\n",
    "def extract_labels_from_truth (data_info):\n",
    "    text_tokenized = data_info\n",
    "    categories = []\n",
    "    for token in text_tokenized:\n",
    "        if token[4] != None:\n",
    "            categories.append(list(token[4].keys()))\n",
    "        else:\n",
    "            categories.append('0')\n",
    "    return categories\n",
    "def annotation_to_tokens (full_text, text_tokenized, prediction_annotation):\n",
    "    clean_text_tokenized = [[token[0],token[1],token[2]] for token in text_tokenized]\n",
    "    annotations = prediction_annotation['annotations']\n",
    "    for annotation in annotations:\n",
    "        # print('============= new annotation', annotation[0])\n",
    "        start_pos = full_text.find(annotation[0])\n",
    "        end_pos = len(annotation[0])+start_pos-1\n",
    "        # print(f'end pos is {len(annotation)} + {start_pos} - 1 = {end_pos}')\n",
    "        categorizing = False\n",
    "        \n",
    "        for token in clean_text_tokenized:\n",
    "            # print(f'token pos {token[1]} annotation pos {start_pos} token {token[0]}')\n",
    "            if token[1] == start_pos:\n",
    "                # print('starting categorization...')\n",
    "                # print(f'start pos {start_pos} end pos {end_pos} token {token[0]}')\n",
    "                categorizing = True\n",
    "            if categorizing:\n",
    "                #adds category to token\n",
    "                # print(f'adding category {annotation[1]} to token {token[0]}')\n",
    "                token.append(annotation[1])\n",
    "                if token[2] == end_pos:\n",
    "                    # print(f'ending categorization at {token[0]}...')\n",
    "                    categorizing = False\n",
    "                    break\n",
    "            \n",
    "            # print(token)\n",
    "    return clean_text_tokenized\n",
    "def extract_labels_from_prediction (categorized_prediction):\n",
    "    labels = []\n",
    "    for token in categorized_prediction:\n",
    "        if len(token) > 3:\n",
    "            labels.append(token[3])\n",
    "        else:\n",
    "            labels.append('0')\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0692ed-4f0f-48b2-b4ac-3a08cade757a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert labels of a annotation to binary vector\n",
    "cats = {'pathophysiology':0,\n",
    "        'etiology':1,\n",
    "        'epidemiology':2,\n",
    "        'history':3,\n",
    "        'physical':4,\n",
    "        'exams':5,\n",
    "        'differential':6,\n",
    "        'therapeutic':7}\n",
    "\n",
    "def label2binary(labels):\n",
    "    vet = [0] * 8\n",
    "    # print(vet)\n",
    "    for label in labels:\n",
    "        if label in list(cats.keys()):\n",
    "            vet[cats[label]] = 1\n",
    "    return vet\n",
    "\n",
    "# Convert all the text - divided in tokens - labels to binary vectors\n",
    "def preprocess_classification(classi_data):\n",
    "    # print('preprocess classification ======= ',classi_data)\n",
    "    for index in range(len(classi_data)):\n",
    "        classi_data[index] = label2binary(classi_data[index])\n",
    "    return classi_data\n",
    "# Convert all the text - divided in tokens - labels to boolean\n",
    "def preprocess_annotation(ann_data):\n",
    "    for index in range(len(ann_data)):\n",
    "        ann_data[index] = 1 if sum(label2binary(ann_data[index])) >= 1 else 0\n",
    "    return ann_data\n",
    "\n",
    "def get_substrings_from_text(text):\n",
    "    substring = text.split(\"\\n\")\n",
    "    return substring\n",
    "\n",
    "def find_complete_text_from_substring(substring, texts):\n",
    "    complete_text = None\n",
    "    count = 0\n",
    "    for text in texts:\n",
    "        if substring[0] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "        elif substring[0][:100] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "        elif substring[0].split('.')[0] in text['text']:\n",
    "            count += 1\n",
    "            complete_text = text\n",
    "    if count == 1:\n",
    "        return complete_text\n",
    "    elif count > 1 and len(substring) > 1:\n",
    "        substring.pop(0)\n",
    "        find_complete_text_from_substring(substring, texts)\n",
    "    else:\n",
    "        print('not FOUND', substring[0][:60])\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08f40c0-bb96-458a-ae3c-f6c374a6d1a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fd4661",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 1 - 0-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb0e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_random']\n",
    "    path = f'llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/random-shot-retrieval/ideas-random-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "        \n",
    "    exisiting_ids = []\n",
    "    df_data_info['truth'] = ''\n",
    "    df_data_info['truth_annotation'] = ''\n",
    "    df_data_info['prediction'] = ''\n",
    "    df_data_info['prediction_annotation'] = ''\n",
    "    for name in file_names:\n",
    "        # print(name)\n",
    "        llama_annotated = ''\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "            truth_labels = truth_to_labels(truth_data)\n",
    "            # print(doc_id)\n",
    "            pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "            pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "            # print(pred_classification_vector)\n",
    "            # print(\"==========\")\n",
    "            truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "            # print(truth_classification_vector)\n",
    "            # print('######3')\n",
    "            truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "            # print(truth_general_annotation_vector)\n",
    "            \n",
    "    \n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "    df_data_info['truth'] = list(df_data_info['truth'])\n",
    "    df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "    df_data_info['prediction'] = df_data_info['prediction']\n",
    "    df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "    # df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "    \n",
    "    aux = []\n",
    "    aux2 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for index in tqdm(range(len(df_data_info))):\n",
    "        truth = eval(df_data_info[\"truth\"][index])\n",
    "        prediction = eval(df_data_info[\"prediction\"][index])\n",
    "        truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "        prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "        value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "        aux.append([value,len(truth)])\n",
    "        value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "        aux2.append([value2,len(truth)])\n",
    "        \n",
    "        # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "    \n",
    "        # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "        precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        precision.append([precision_now,len(truth)])\n",
    "    \n",
    "        recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        recall.append([recall_now,len(truth)])\n",
    "    \n",
    "        f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        f1.append([f1_now,len(truth)])\n",
    "    \n",
    "    df_data_info[\"label_score\"] = aux\n",
    "    df_data_info[\"annotation_score\"] = aux2\n",
    "    df_data_info[\"precision\"] = precision\n",
    "    df_data_info[\"recall\"] = recall\n",
    "    df_data_info[\"f1\"] = f1\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n",
    "\n",
    "    #Ponderada\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "    \n",
    "    label_score_weight = score/count\n",
    "    row_metrics.append(label_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    label_score_mean = score/count\n",
    "    row_metrics.append(label_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    annotation_score_weight = score/count\n",
    "    row_metrics.append(annotation_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    annotation_score_mean = score/count\n",
    "    row_metrics.append(annotation_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    precision_weight = score/count\n",
    "    row_metrics.append(precision_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    precision_mean = score/count\n",
    "    row_metrics.append(precision_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    recall_weight = score/count\n",
    "    row_metrics.append(recall_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    recall_mean = score/count\n",
    "    row_metrics.append(recall_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    f1_weight = score/count\n",
    "    row_metrics.append(f1_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    f1_mean = score/count\n",
    "    row_metrics.append(f1_mean)\n",
    "    \n",
    "    weight_score = annotation_score_weight * label_score_weight\n",
    "    mean_score = annotation_score_mean * label_score_mean\n",
    "    \n",
    "    row_metrics.append(weight_score)\n",
    "    \n",
    "    row_metrics.append(mean_score)\n",
    "    rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f785e8a3",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 1 - zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4390df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['zero_shot_temp_0']\n",
    "path = f'llama-outputs/no-short-data/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "metrics_path = f'metrics-result/no-short-data/temp-0.0/top-p-0.6/ideas_zero_shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        # print(doc_id)\n",
    "        # if doc_id==\"8396380d-e0b6-4b81-8fe9-0b99c611f9f3\":\n",
    "        #     print(str(truth_classification_vector))\n",
    "        #     print(str(truth_general_annotation_vector))\n",
    "        #     print(str(pred_classification_vector))\n",
    "        #     print(str(pred_general_annotation_vector))\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6130758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    # print(df_data_info.iloc[index])\n",
    "    # print(df_data_info['doc_id'][index])\n",
    "    # print(df_data_info[\"text\"][index])\n",
    "    # print(df_data_info[index:index+1])\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880cf9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fd692e",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4358ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f14727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8c1de4",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844582ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01784630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013066ff",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd64e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c072a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f797d94",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3574c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_weight)\n",
    "    \n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f765a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6af2e7f",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88cb92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06907f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f47a13",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26b347",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74c8f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b251b2a1",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 1 - Static Examples (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf194a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_static_temp_0.0']\n",
    "    path = f'llama-outputs/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result/full-dataset/no-short-data/temp-0.0/top-p-0.6/ideas-static-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "        \n",
    "    exisiting_ids = []\n",
    "    df_data_info['truth'] = ''\n",
    "    df_data_info['truth_annotation'] = ''\n",
    "    df_data_info['prediction'] = ''\n",
    "    df_data_info['prediction_annotation'] = ''\n",
    "    for name in file_names:\n",
    "        # print(name)\n",
    "        llama_annotated = ''\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "            truth_labels = truth_to_labels(truth_data)\n",
    "            # print(doc_id)\n",
    "            pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "            pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "            # print(pred_classification_vector)\n",
    "            # print(\"==========\")\n",
    "            truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "            # print(truth_classification_vector)\n",
    "            # print('######3')\n",
    "            truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "            # print(truth_general_annotation_vector)\n",
    "            \n",
    "    \n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "    df_data_info['truth'] = list(df_data_info['truth'])\n",
    "    df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "    df_data_info['prediction'] = df_data_info['prediction']\n",
    "    df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "    # df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "    \n",
    "    aux = []\n",
    "    aux2 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for index in tqdm(range(len(df_data_info))):\n",
    "        truth = eval(df_data_info[\"truth\"][index])\n",
    "        prediction = eval(df_data_info[\"prediction\"][index])\n",
    "        truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "        prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "        value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "        aux.append([value,len(truth)])\n",
    "        value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "        aux2.append([value2,len(truth)])\n",
    "        \n",
    "        # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "    \n",
    "        # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "        precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        precision.append([precision_now,len(truth)])\n",
    "    \n",
    "        recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        recall.append([recall_now,len(truth)])\n",
    "    \n",
    "        f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        f1.append([f1_now,len(truth)])\n",
    "    \n",
    "    df_data_info[\"label_score\"] = aux\n",
    "    df_data_info[\"annotation_score\"] = aux2\n",
    "    df_data_info[\"precision\"] = precision\n",
    "    df_data_info[\"recall\"] = recall\n",
    "    df_data_info[\"f1\"] = f1\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n",
    "\n",
    "    #Ponderada\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "    \n",
    "    label_score_weight = score/count\n",
    "    row_metrics.append(label_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    label_score_mean = score/count\n",
    "    row_metrics.append(label_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    annotation_score_weight = score/count\n",
    "    row_metrics.append(annotation_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    annotation_score_mean = score/count\n",
    "    row_metrics.append(annotation_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    precision_weight = score/count\n",
    "    row_metrics.append(precision_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    precision_mean = score/count\n",
    "    row_metrics.append(precision_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    recall_weight = score/count\n",
    "    row_metrics.append(recall_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    recall_mean = score/count\n",
    "    row_metrics.append(recall_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    f1_weight = score/count\n",
    "    row_metrics.append(f1_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    f1_mean = score/count\n",
    "    row_metrics.append(f1_mean)\n",
    "    \n",
    "    weight_score = annotation_score_weight * label_score_weight\n",
    "    mean_score = annotation_score_mean * label_score_mean\n",
    "    \n",
    "    row_metrics.append(weight_score)\n",
    "    \n",
    "    row_metrics.append(mean_score)\n",
    "    rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ff1d73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ccfaa265",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 2 - TF-IDF (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29abb77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_tf_id']\n",
    "\n",
    "    path = f'llama-outputs/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "        \n",
    "    exisiting_ids = []\n",
    "    df_data_info['truth'] = ''\n",
    "    df_data_info['truth_annotation'] = ''\n",
    "    df_data_info['prediction'] = ''\n",
    "    df_data_info['prediction_annotation'] = ''\n",
    "    for name in file_names:\n",
    "        # print(name)\n",
    "        llama_annotated = ''\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "            truth_labels = truth_to_labels(truth_data)\n",
    "            # print(doc_id)\n",
    "            pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "            pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "            # print(pred_classification_vector)\n",
    "            # print(\"==========\")\n",
    "            truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "            # print(truth_classification_vector)\n",
    "            # print('######3')\n",
    "            truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "            # print(truth_general_annotation_vector)\n",
    "            \n",
    "    \n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "    df_data_info['truth'] = list(df_data_info['truth'])\n",
    "    df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "    df_data_info['prediction'] = df_data_info['prediction']\n",
    "    df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "    # df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "    \n",
    "    aux = []\n",
    "    aux2 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for index in tqdm(range(len(df_data_info))):\n",
    "        truth = eval(df_data_info[\"truth\"][index])\n",
    "        prediction = eval(df_data_info[\"prediction\"][index])\n",
    "        truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "        prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "        value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "        aux.append([value,len(truth)])\n",
    "        value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "        aux2.append([value2,len(truth)])\n",
    "        \n",
    "        # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "    \n",
    "        # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "        precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        precision.append([precision_now,len(truth)])\n",
    "    \n",
    "        recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        recall.append([recall_now,len(truth)])\n",
    "    \n",
    "        f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        f1.append([f1_now,len(truth)])\n",
    "    \n",
    "    df_data_info[\"label_score\"] = aux\n",
    "    df_data_info[\"annotation_score\"] = aux2\n",
    "    df_data_info[\"precision\"] = precision\n",
    "    df_data_info[\"recall\"] = recall\n",
    "    df_data_info[\"f1\"] = f1\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n",
    "\n",
    "    #Ponderada\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "    \n",
    "    label_score_weight = score/count\n",
    "    row_metrics.append(label_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    label_score_mean = score/count\n",
    "    row_metrics.append(label_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    annotation_score_weight = score/count\n",
    "    row_metrics.append(annotation_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    annotation_score_mean = score/count\n",
    "    row_metrics.append(annotation_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    precision_weight = score/count\n",
    "    row_metrics.append(precision_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    precision_mean = score/count\n",
    "    row_metrics.append(precision_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    recall_weight = score/count\n",
    "    row_metrics.append(recall_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    recall_mean = score/count\n",
    "    row_metrics.append(recall_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    f1_weight = score/count\n",
    "    row_metrics.append(f1_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    f1_mean = score/count\n",
    "    row_metrics.append(f1_mean)\n",
    "    \n",
    "    weight_score = annotation_score_weight * label_score_weight\n",
    "    mean_score = annotation_score_mean * label_score_mean\n",
    "    \n",
    "    row_metrics.append(weight_score)\n",
    "    \n",
    "    row_metrics.append(mean_score)\n",
    "    rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc04f96",
   "metadata": {},
   "source": [
    "### Metrics for Experiment 3 - TF-IDF Custom (similarity by text VS annotation) (1,2,3,4,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f3f247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "for i_shot in [1,2,3,4,10]:\n",
    "    print('starting ',i_shot,'shot ================================')\n",
    "    df_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "    row_metrics = [f'{i_shot}_shot_tf_id_custom']\n",
    "\n",
    "    path = f'llama-outputs/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-{i_shot}-shot'\n",
    "    metrics_path = f'metrics-result/no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-{i_shot}-shot'\n",
    "    file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "    if not os.path.exists(metrics_path):\n",
    "        os.makedirs(metrics_path)\n",
    "        \n",
    "    exisiting_ids = []\n",
    "    df_data_info['truth'] = ''\n",
    "    df_data_info['truth_annotation'] = ''\n",
    "    df_data_info['prediction'] = ''\n",
    "    df_data_info['prediction_annotation'] = ''\n",
    "    for name in file_names:\n",
    "        # print(name)\n",
    "        llama_annotated = ''\n",
    "        doc_id = name[-41:-5]\n",
    "        if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "            truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "            with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "                llama_annotated = json.load(file)\n",
    "            pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "            truth_labels = truth_to_labels(truth_data)\n",
    "            # print(doc_id)\n",
    "            pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "            pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "            # print(pred_classification_vector)\n",
    "            # print(\"==========\")\n",
    "            truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "            # print(truth_classification_vector)\n",
    "            # print('######3')\n",
    "            truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "            # print(truth_general_annotation_vector)\n",
    "            \n",
    "    \n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "            df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "    df_data_info['truth'] = list(df_data_info['truth'])\n",
    "    df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "    df_data_info['prediction'] = df_data_info['prediction']\n",
    "    df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "    # df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "    # df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n",
    "    \n",
    "    aux = []\n",
    "    aux2 = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    \n",
    "    for index in tqdm(range(len(df_data_info))):\n",
    "        truth = eval(df_data_info[\"truth\"][index])\n",
    "        prediction = eval(df_data_info[\"prediction\"][index])\n",
    "        truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "        prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "        value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "        aux.append([value,len(truth)])\n",
    "        value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "        aux2.append([value2,len(truth)])\n",
    "        \n",
    "        # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "    \n",
    "        # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "        precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        precision.append([precision_now,len(truth)])\n",
    "    \n",
    "        recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        recall.append([recall_now,len(truth)])\n",
    "    \n",
    "        f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "        f1.append([f1_now,len(truth)])\n",
    "    \n",
    "    df_data_info[\"label_score\"] = aux\n",
    "    df_data_info[\"annotation_score\"] = aux2\n",
    "    df_data_info[\"precision\"] = precision\n",
    "    df_data_info[\"recall\"] = recall\n",
    "    df_data_info[\"f1\"] = f1\n",
    "    df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n",
    "\n",
    "    #Ponderada\n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "    \n",
    "    label_score_weight = score/count\n",
    "    row_metrics.append(label_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"label_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    label_score_mean = score/count\n",
    "    row_metrics.append(label_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    annotation_score_weight = score/count\n",
    "    row_metrics.append(annotation_score_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"annotation_score\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    annotation_score_mean = score/count\n",
    "    row_metrics.append(annotation_score_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    precision_weight = score/count\n",
    "    row_metrics.append(precision_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"precision\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    precision_mean = score/count\n",
    "    row_metrics.append(precision_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    recall_weight = score/count\n",
    "    row_metrics.append(recall_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"recall\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    recall_mean = score/count\n",
    "    row_metrics.append(recall_mean)\n",
    "    # Ponderada\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0] * value[1]\n",
    "        count += value[1]\n",
    "        \n",
    "    f1_weight = score/count\n",
    "    row_metrics.append(f1_weight)\n",
    "    # Mean\n",
    "    \n",
    "    score = 0\n",
    "    count = 0\n",
    "    for value in df_data_info[\"f1\"]:\n",
    "        score += value[0]\n",
    "        count += 1\n",
    "    \n",
    "    f1_mean = score/count\n",
    "    row_metrics.append(f1_mean)\n",
    "    \n",
    "    weight_score = annotation_score_weight * label_score_weight\n",
    "    mean_score = annotation_score_mean * label_score_mean\n",
    "    \n",
    "    row_metrics.append(weight_score)\n",
    "    \n",
    "    row_metrics.append(mean_score)\n",
    "    rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24160765-7e51-499b-ba87-118f45cbdb15",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - zero shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ba2b8-aab5-43f4-880e-1ad06cc289a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['zero_shot_temp_0']\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-0-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.0/top-p-0.6/ideas_0_shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        # print(doc_id)\n",
    "        # if doc_id==\"8396380d-e0b6-4b81-8fe9-0b99c611f9f3\":\n",
    "        #     print(str(truth_classification_vector))\n",
    "        #     print(str(truth_general_annotation_vector))\n",
    "        #     print(str(pred_classification_vector))\n",
    "        #     print(str(pred_general_annotation_vector))\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9df299-1c5c-479f-8f0e-3c860b9814c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    # print(df_data_info.iloc[index])\n",
    "    # print(df_data_info['doc_id'][index])\n",
    "    # print(df_data_info[\"text\"][index])\n",
    "    # print(df_data_info[index:index+1])\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46e2af0-9602-4db8-840c-3cc4069f7b6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:18.168610Z",
     "iopub.status.busy": "2024-07-18T14:18:18.167803Z",
     "iopub.status.idle": "2024-07-18T14:18:18.253628Z",
     "shell.execute_reply": "2024-07-18T14:18:18.252718Z",
     "shell.execute_reply.started": "2024-07-18T14:18:18.168567Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394e8ad9-b729-471d-8b2f-319e182fc4b6",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6045f38d-97cf-4506-8bf0-f748a4401fa0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:18.256840Z",
     "iopub.status.busy": "2024-07-18T14:18:18.256232Z",
     "iopub.status.idle": "2024-07-18T14:18:18.272396Z",
     "shell.execute_reply": "2024-07-18T14:18:18.271440Z",
     "shell.execute_reply.started": "2024-07-18T14:18:18.256793Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beb53bf-40aa-49b6-92e4-653f9895257d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:18.275149Z",
     "iopub.status.busy": "2024-07-18T14:18:18.274317Z",
     "iopub.status.idle": "2024-07-18T14:18:18.363066Z",
     "shell.execute_reply": "2024-07-18T14:18:18.362086Z",
     "shell.execute_reply.started": "2024-07-18T14:18:18.275108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d8575d-2149-417d-8de4-4d82a9da5f8b",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda4fc07-1472-4618-b166-5ed2051df860",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:18.365419Z",
     "iopub.status.busy": "2024-07-18T14:18:18.364489Z",
     "iopub.status.idle": "2024-07-18T14:18:18.478093Z",
     "shell.execute_reply": "2024-07-18T14:18:18.476748Z",
     "shell.execute_reply.started": "2024-07-18T14:18:18.365363Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6831438-2ec3-4c53-b56f-0fce1716ee1a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:18.481562Z",
     "iopub.status.busy": "2024-07-18T14:18:18.480706Z",
     "iopub.status.idle": "2024-07-18T14:18:18.575446Z",
     "shell.execute_reply": "2024-07-18T14:18:18.574046Z",
     "shell.execute_reply.started": "2024-07-18T14:18:18.481514Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da868557-e0a1-4018-a78f-37ff77ffa2a3",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4349b8d-db60-4915-8aeb-7848e651e09f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:18.578970Z",
     "iopub.status.busy": "2024-07-18T14:18:18.578082Z",
     "iopub.status.idle": "2024-07-18T14:18:18.667944Z",
     "shell.execute_reply": "2024-07-18T14:18:18.666922Z",
     "shell.execute_reply.started": "2024-07-18T14:18:18.578924Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d52cf5-b119-441f-b5e0-7edefb398ff7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:18.670317Z",
     "iopub.status.busy": "2024-07-18T14:18:18.669718Z",
     "iopub.status.idle": "2024-07-18T14:18:18.745878Z",
     "shell.execute_reply": "2024-07-18T14:18:18.744993Z",
     "shell.execute_reply.started": "2024-07-18T14:18:18.670273Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e98536-f610-4493-b37e-9751e2f477ba",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9141e8-5b1d-4917-80a6-058326bdfc0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:18.750501Z",
     "iopub.status.busy": "2024-07-18T14:18:18.750175Z",
     "iopub.status.idle": "2024-07-18T14:18:18.836886Z",
     "shell.execute_reply": "2024-07-18T14:18:18.835961Z",
     "shell.execute_reply.started": "2024-07-18T14:18:18.750473Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_weight)\n",
    "    \n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe18813-346f-417d-8363-0d262d093620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:18.838931Z",
     "iopub.status.busy": "2024-07-18T14:18:18.838070Z",
     "iopub.status.idle": "2024-07-18T14:18:18.917170Z",
     "shell.execute_reply": "2024-07-18T14:18:18.916049Z",
     "shell.execute_reply.started": "2024-07-18T14:18:18.838899Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e23061d-a826-4775-bab7-e7bc4fa68438",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1cc487-cd4b-4b1c-8cc1-809bfbd1e29a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:18.919019Z",
     "iopub.status.busy": "2024-07-18T14:18:18.918543Z",
     "iopub.status.idle": "2024-07-18T14:18:19.005451Z",
     "shell.execute_reply": "2024-07-18T14:18:19.004433Z",
     "shell.execute_reply.started": "2024-07-18T14:18:18.918977Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2f7a24-7f00-43fc-a5e9-0aebada803f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:19.008003Z",
     "iopub.status.busy": "2024-07-18T14:18:19.007548Z",
     "iopub.status.idle": "2024-07-18T14:18:19.090087Z",
     "shell.execute_reply": "2024-07-18T14:18:19.089125Z",
     "shell.execute_reply.started": "2024-07-18T14:18:19.007959Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2e9096-0795-45d7-a83a-f9e740598000",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047a6a4-4a10-4716-8527-9f4a1804fbbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:19.092766Z",
     "iopub.status.busy": "2024-07-18T14:18:19.092242Z",
     "iopub.status.idle": "2024-07-18T14:18:19.175026Z",
     "shell.execute_reply": "2024-07-18T14:18:19.173912Z",
     "shell.execute_reply.started": "2024-07-18T14:18:19.092658Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd835c5-b725-4dbc-8a57-fab53a045dd3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:19.177630Z",
     "iopub.status.busy": "2024-07-18T14:18:19.176873Z",
     "iopub.status.idle": "2024-07-18T14:18:19.258139Z",
     "shell.execute_reply": "2024-07-18T14:18:19.257357Z",
     "shell.execute_reply.started": "2024-07-18T14:18:19.177517Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c04bca-8afa-4a0e-ab85-7143844988d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - one shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42bb76-3ca3-4f01-9676-fa1bf9cd0e35",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:19.260090Z",
     "iopub.status.busy": "2024-07-18T14:18:19.259714Z",
     "iopub.status.idle": "2024-07-18T14:18:19.799671Z",
     "shell.execute_reply": "2024-07-18T14:18:19.798397Z",
     "shell.execute_reply.started": "2024-07-18T14:18:19.260049Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['one_shot_temp_0.9']\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-1-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.9/top-p-0.6/ideas'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        # print(doc_id)\n",
    "        # if doc_id==\"8396380d-e0b6-4b81-8fe9-0b99c611f9f3\":\n",
    "        #     print(str(truth_classification_vector))\n",
    "        #     print(str(truth_general_annotation_vector))\n",
    "        #     print(str(pred_classification_vector))\n",
    "        #     print(str(pred_general_annotation_vector))\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf1af5e-bb40-468c-8ad3-e4dc60130abb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:19.801889Z",
     "iopub.status.busy": "2024-07-18T14:18:19.801516Z",
     "iopub.status.idle": "2024-07-18T14:18:20.887467Z",
     "shell.execute_reply": "2024-07-18T14:18:20.886619Z",
     "shell.execute_reply.started": "2024-07-18T14:18:19.801847Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    # print(df_data_info.iloc[index])\n",
    "    # print(df_data_info['doc_id'][index])\n",
    "    # print(df_data_info[\"text\"][index])\n",
    "    # print(df_data_info[index:index+1])\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc523bad-793a-4525-a0c9-09eda598832c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd6fbf-3f39-47eb-b142-d11b982d56ab",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62be74-7cdd-44c0-bf71-4af361c7f2cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1ab13e-e39a-4a78-8c2c-4587881fdda6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bc51dd-c5ad-4712-94fb-9ac5aa8aaabe",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078e16de-68a9-4b06-846f-e7bb2fc0bb28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6fd24-dd1c-4174-836e-0f278f6c5ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ae6b9a-b189-41a9-841f-d6f200b0fe05",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e5f829-c257-4ed5-a4f1-2dc57988c73c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ee524-f11b-448b-864f-6dc68b570de4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36332c1-5f29-4899-9f31-86cb97500204",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733c7917-4e6c-4886-ab86-e44a7f70e901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "\n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3f3a9-9788-4cc3-ab10-a64e7bfa45a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eedabe0-d13a-4725-8468-fc80f59d00df",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ddd60-8844-4e82-a32b-989f49566097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e922f6-8eab-4bae-8319-20de9705ba96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec11e381-b90d-42c4-a6eb-da2380bda56f",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bd86d0-07dc-4f1f-8f03-0f967af34b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e222787c-e65c-4262-a37c-cf883b12400b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:21.969735Z",
     "iopub.status.busy": "2024-07-18T14:18:21.969164Z",
     "iopub.status.idle": "2024-07-18T14:18:22.057882Z",
     "shell.execute_reply": "2024-07-18T14:18:22.057079Z",
     "shell.execute_reply.started": "2024-07-18T14:18:21.969693Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067bc9cd-868e-448a-8fc2-d40913f88fba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - one shot temp 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ce973b-0ecd-4045-839c-e91ef508203b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:22.061527Z",
     "iopub.status.busy": "2024-07-18T14:18:22.060549Z",
     "iopub.status.idle": "2024-07-18T14:18:22.653160Z",
     "shell.execute_reply": "2024-07-18T14:18:22.652349Z",
     "shell.execute_reply.started": "2024-07-18T14:18:22.061421Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "\n",
    "df_data_info = pd.read_csv('test_data_info_no_short.csv')\n",
    "row_metrics = ['one_shot_temp_0']\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-1-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-1-shot'\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "    \n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        \n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c7aa5-c957-46d0-a925-eaf1e8e7b148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:22.655105Z",
     "iopub.status.busy": "2024-07-18T14:18:22.654674Z",
     "iopub.status.idle": "2024-07-18T14:18:23.656816Z",
     "shell.execute_reply": "2024-07-18T14:18:23.655500Z",
     "shell.execute_reply.started": "2024-07-18T14:18:22.655064Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbec646-472a-4910-a0f5-915a4e82bfce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:23.660616Z",
     "iopub.status.busy": "2024-07-18T14:18:23.659430Z",
     "iopub.status.idle": "2024-07-18T14:18:23.743975Z",
     "shell.execute_reply": "2024-07-18T14:18:23.743006Z",
     "shell.execute_reply.started": "2024-07-18T14:18:23.660480Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eb7747-afa4-4eef-8ed2-11b9ea515da4",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589a64e4-fe8f-40e0-bb74-0bceae2f1eb9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:23.747843Z",
     "iopub.status.busy": "2024-07-18T14:18:23.747427Z",
     "iopub.status.idle": "2024-07-18T14:18:23.765976Z",
     "shell.execute_reply": "2024-07-18T14:18:23.764140Z",
     "shell.execute_reply.started": "2024-07-18T14:18:23.747811Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4180a7-80f3-4343-9730-f0d87b64985c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:23.769203Z",
     "iopub.status.busy": "2024-07-18T14:18:23.768478Z",
     "iopub.status.idle": "2024-07-18T14:18:23.855660Z",
     "shell.execute_reply": "2024-07-18T14:18:23.854145Z",
     "shell.execute_reply.started": "2024-07-18T14:18:23.769155Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb76688-5b9f-48d8-8a32-0136771e77f5",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c7d324-39de-4232-af76-e4aa1f090f62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:23.859956Z",
     "iopub.status.busy": "2024-07-18T14:18:23.859437Z",
     "iopub.status.idle": "2024-07-18T14:18:23.946406Z",
     "shell.execute_reply": "2024-07-18T14:18:23.945342Z",
     "shell.execute_reply.started": "2024-07-18T14:18:23.859863Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5505722b-2abe-4251-b515-b1069df7ec78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:23.949360Z",
     "iopub.status.busy": "2024-07-18T14:18:23.948772Z",
     "iopub.status.idle": "2024-07-18T14:18:24.036037Z",
     "shell.execute_reply": "2024-07-18T14:18:24.032566Z",
     "shell.execute_reply.started": "2024-07-18T14:18:23.949289Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b891322-dd13-452d-bf6a-d14b2d76f7e0",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb82108-33bf-4554-9c0d-2b3661754559",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.046423Z",
     "iopub.status.busy": "2024-07-18T14:18:24.042351Z",
     "iopub.status.idle": "2024-07-18T14:18:24.128975Z",
     "shell.execute_reply": "2024-07-18T14:18:24.127970Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.046378Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291273ee-85e6-42d6-9fac-335709fbae2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.131634Z",
     "iopub.status.busy": "2024-07-18T14:18:24.131161Z",
     "iopub.status.idle": "2024-07-18T14:18:24.216297Z",
     "shell.execute_reply": "2024-07-18T14:18:24.215053Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.131504Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67e453-eb7a-4057-8e13-4bf22f5618ba",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3395c4-9654-41bb-b355-a62e0d8e91f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.218612Z",
     "iopub.status.busy": "2024-07-18T14:18:24.218227Z",
     "iopub.status.idle": "2024-07-18T14:18:24.307776Z",
     "shell.execute_reply": "2024-07-18T14:18:24.306736Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.218569Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa3f6d9-da33-43d6-a69b-f762b7aa32ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.310608Z",
     "iopub.status.busy": "2024-07-18T14:18:24.309646Z",
     "iopub.status.idle": "2024-07-18T14:18:24.392461Z",
     "shell.execute_reply": "2024-07-18T14:18:24.391818Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.310560Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38489b92-ad4b-4ff0-80f4-3720f3a62bb1",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863d14c-2589-42d3-a90b-13f3053a0603",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.393656Z",
     "iopub.status.busy": "2024-07-18T14:18:24.393302Z",
     "iopub.status.idle": "2024-07-18T14:18:24.488492Z",
     "shell.execute_reply": "2024-07-18T14:18:24.487416Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.393628Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8699a2f-ed33-4c8b-8e2f-7be4dbacc9c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.490698Z",
     "iopub.status.busy": "2024-07-18T14:18:24.490066Z",
     "iopub.status.idle": "2024-07-18T14:18:24.573875Z",
     "shell.execute_reply": "2024-07-18T14:18:24.572773Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.490654Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b057bc0e-7349-43e7-b1d2-2cdae9e2166d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1acacfa-3689-4410-a7d7-a943f901b777",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.575996Z",
     "iopub.status.busy": "2024-07-18T14:18:24.575591Z",
     "iopub.status.idle": "2024-07-18T14:18:24.658454Z",
     "shell.execute_reply": "2024-07-18T14:18:24.657507Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.575953Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e09114-4f71-45b2-abfa-51a6ce6a2fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.660944Z",
     "iopub.status.busy": "2024-07-18T14:18:24.660226Z",
     "iopub.status.idle": "2024-07-18T14:18:24.749158Z",
     "shell.execute_reply": "2024-07-18T14:18:24.748205Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.660900Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a5148-fc07-4379-b52f-178ec0357fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 1 - 2 shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7fa3d7-ecdb-4656-8eca-17fcda0939fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:24.754487Z",
     "iopub.status.busy": "2024-07-18T14:18:24.753124Z",
     "iopub.status.idle": "2024-07-18T14:18:25.254361Z",
     "shell.execute_reply": "2024-07-18T14:18:25.253630Z",
     "shell.execute_reply.started": "2024-07-18T14:18:24.754436Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['two_shot_temp_0']\n",
    "\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-2-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-2-shot'\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "    \n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "        \n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc974af-a82f-4e27-ae6b-58085551a261",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:25.255663Z",
     "iopub.status.busy": "2024-07-18T14:18:25.255432Z",
     "iopub.status.idle": "2024-07-18T14:18:26.861814Z",
     "shell.execute_reply": "2024-07-18T14:18:26.860853Z",
     "shell.execute_reply.started": "2024-07-18T14:18:25.255638Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decde5d7-9fa9-4123-bca1-62d83ac5c074",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:26.872232Z",
     "iopub.status.busy": "2024-07-18T14:18:26.867245Z",
     "iopub.status.idle": "2024-07-18T14:18:26.966900Z",
     "shell.execute_reply": "2024-07-18T14:18:26.965952Z",
     "shell.execute_reply.started": "2024-07-18T14:18:26.872170Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22daa84-bcfb-484e-abf5-a4900b8a7798",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad1642-5ab3-4440-89a5-b83ab2f6d66e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:26.969607Z",
     "iopub.status.busy": "2024-07-18T14:18:26.968704Z",
     "iopub.status.idle": "2024-07-18T14:18:26.988867Z",
     "shell.execute_reply": "2024-07-18T14:18:26.987816Z",
     "shell.execute_reply.started": "2024-07-18T14:18:26.969562Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f4398f-e2f0-4bba-8b2d-cd68cb5724f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:26.993405Z",
     "iopub.status.busy": "2024-07-18T14:18:26.991786Z",
     "iopub.status.idle": "2024-07-18T14:18:27.069489Z",
     "shell.execute_reply": "2024-07-18T14:18:27.068317Z",
     "shell.execute_reply.started": "2024-07-18T14:18:26.993357Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c17cef1-a1bf-4ebf-ab17-5c0f50144d2f",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d642739-9f78-460f-9f19-a91dc99ee681",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.071341Z",
     "iopub.status.busy": "2024-07-18T14:18:27.070729Z",
     "iopub.status.idle": "2024-07-18T14:18:27.152251Z",
     "shell.execute_reply": "2024-07-18T14:18:27.151350Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.071294Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef5ad9-7084-40b5-bfb5-269dff823dd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.154351Z",
     "iopub.status.busy": "2024-07-18T14:18:27.153264Z",
     "iopub.status.idle": "2024-07-18T14:18:27.236165Z",
     "shell.execute_reply": "2024-07-18T14:18:27.235220Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.154308Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37cb3c05-d60c-4832-acce-6ef0fe64e9a0",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44232568-6658-4913-9f10-36a18ce2ab74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.238557Z",
     "iopub.status.busy": "2024-07-18T14:18:27.237757Z",
     "iopub.status.idle": "2024-07-18T14:18:27.323998Z",
     "shell.execute_reply": "2024-07-18T14:18:27.322314Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.238512Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f821f-0182-4ec6-ad5c-dbb83bf3811c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.325652Z",
     "iopub.status.busy": "2024-07-18T14:18:27.325134Z",
     "iopub.status.idle": "2024-07-18T14:18:27.412575Z",
     "shell.execute_reply": "2024-07-18T14:18:27.411551Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.325609Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c5fccb-634b-4aaa-910c-cccc345bd4a9",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942ed5c2-38f7-4b03-9190-fe49659113d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.414599Z",
     "iopub.status.busy": "2024-07-18T14:18:27.413868Z",
     "iopub.status.idle": "2024-07-18T14:18:27.499386Z",
     "shell.execute_reply": "2024-07-18T14:18:27.497979Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.414555Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6567ec2b-4dfa-4dcc-acdc-d8ce73f0aac1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.501504Z",
     "iopub.status.busy": "2024-07-18T14:18:27.501133Z",
     "iopub.status.idle": "2024-07-18T14:18:27.586244Z",
     "shell.execute_reply": "2024-07-18T14:18:27.584684Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.501464Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5a5df-2b65-465a-821f-849400864c53",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b10b0dc-52f0-48d5-82ca-90253b85af79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.589570Z",
     "iopub.status.busy": "2024-07-18T14:18:27.588761Z",
     "iopub.status.idle": "2024-07-18T14:18:27.671686Z",
     "shell.execute_reply": "2024-07-18T14:18:27.670781Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.589525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee06ca75-c2b9-473b-a9dd-c71512534faf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.673424Z",
     "iopub.status.busy": "2024-07-18T14:18:27.673000Z",
     "iopub.status.idle": "2024-07-18T14:18:27.759560Z",
     "shell.execute_reply": "2024-07-18T14:18:27.758606Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.673383Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70e072a-f9bc-41a5-b025-9850020d802d",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9987a6-5644-495b-9107-679df5e749f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.761022Z",
     "iopub.status.busy": "2024-07-18T14:18:27.760642Z",
     "iopub.status.idle": "2024-07-18T14:18:27.847382Z",
     "shell.execute_reply": "2024-07-18T14:18:27.846290Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.760983Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368e4c5c-a6f5-4feb-bba6-d9debead6684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.849537Z",
     "iopub.status.busy": "2024-07-18T14:18:27.848965Z",
     "iopub.status.idle": "2024-07-18T14:18:27.934017Z",
     "shell.execute_reply": "2024-07-18T14:18:27.933149Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.849492Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b63614-f79f-454a-9021-391fa639c923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 1 shot temp 0 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8482a376-9e84-40db-b520-cc3ed30dec85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:27.935377Z",
     "iopub.status.busy": "2024-07-18T14:18:27.935054Z",
     "iopub.status.idle": "2024-07-18T14:18:28.468764Z",
     "shell.execute_reply": "2024-07-18T14:18:28.467857Z",
     "shell.execute_reply.started": "2024-07-18T14:18:27.935338Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['one_shot_tf_idf']\n",
    "\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-1-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-1-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        print(doc_id)\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbcb648-2c9d-4879-9f84-d2de5b1e046e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:28.473874Z",
     "iopub.status.busy": "2024-07-18T14:18:28.473440Z",
     "iopub.status.idle": "2024-07-18T14:18:29.505801Z",
     "shell.execute_reply": "2024-07-18T14:18:29.504809Z",
     "shell.execute_reply.started": "2024-07-18T14:18:28.473843Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775ba4c-6c51-4e85-bd59-936d9beb0064",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:29.508757Z",
     "iopub.status.busy": "2024-07-18T14:18:29.508045Z",
     "iopub.status.idle": "2024-07-18T14:18:29.598200Z",
     "shell.execute_reply": "2024-07-18T14:18:29.597068Z",
     "shell.execute_reply.started": "2024-07-18T14:18:29.508711Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195b663-53a9-43d9-962e-a8ea2511ad19",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889026de-9008-4bf1-980a-8bba483d52c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:29.600221Z",
     "iopub.status.busy": "2024-07-18T14:18:29.599830Z",
     "iopub.status.idle": "2024-07-18T14:18:29.623953Z",
     "shell.execute_reply": "2024-07-18T14:18:29.622992Z",
     "shell.execute_reply.started": "2024-07-18T14:18:29.600179Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ed862b-da0f-4ae7-8bc3-1ad9ef7b78ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:29.629102Z",
     "iopub.status.busy": "2024-07-18T14:18:29.627906Z",
     "iopub.status.idle": "2024-07-18T14:18:29.711372Z",
     "shell.execute_reply": "2024-07-18T14:18:29.710194Z",
     "shell.execute_reply.started": "2024-07-18T14:18:29.629053Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f108dc1f-7273-4284-aea2-e308704405fd",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603c34d4-8ce2-4dcc-96b7-8e449e3d201a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:29.713987Z",
     "iopub.status.busy": "2024-07-18T14:18:29.713231Z",
     "iopub.status.idle": "2024-07-18T14:18:29.807758Z",
     "shell.execute_reply": "2024-07-18T14:18:29.806847Z",
     "shell.execute_reply.started": "2024-07-18T14:18:29.713846Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb637b25-12fc-4c19-a5bb-4ec87a1c9043",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:29.811445Z",
     "iopub.status.busy": "2024-07-18T14:18:29.810990Z",
     "iopub.status.idle": "2024-07-18T14:18:29.892061Z",
     "shell.execute_reply": "2024-07-18T14:18:29.891114Z",
     "shell.execute_reply.started": "2024-07-18T14:18:29.811413Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc951a5-9ff6-484c-88f3-5c020ecda05d",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980e23d9-a897-49b9-8aae-9397b51268b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:29.894379Z",
     "iopub.status.busy": "2024-07-18T14:18:29.893743Z",
     "iopub.status.idle": "2024-07-18T14:18:29.991287Z",
     "shell.execute_reply": "2024-07-18T14:18:29.990393Z",
     "shell.execute_reply.started": "2024-07-18T14:18:29.894334Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992768d6-695d-4273-a8f8-0ff8a3d6edc3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:29.995193Z",
     "iopub.status.busy": "2024-07-18T14:18:29.994230Z",
     "iopub.status.idle": "2024-07-18T14:18:30.077219Z",
     "shell.execute_reply": "2024-07-18T14:18:30.076149Z",
     "shell.execute_reply.started": "2024-07-18T14:18:29.995156Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8ce79d-0efb-4047-b8d1-cfbf36ebc171",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262c953d-857c-4412-b6df-059992b89928",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:30.079254Z",
     "iopub.status.busy": "2024-07-18T14:18:30.078718Z",
     "iopub.status.idle": "2024-07-18T14:18:30.164852Z",
     "shell.execute_reply": "2024-07-18T14:18:30.163841Z",
     "shell.execute_reply.started": "2024-07-18T14:18:30.079212Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a85e08-6647-4b4d-bf34-f475361e8b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:30.167076Z",
     "iopub.status.busy": "2024-07-18T14:18:30.166353Z",
     "iopub.status.idle": "2024-07-18T14:18:30.250872Z",
     "shell.execute_reply": "2024-07-18T14:18:30.249987Z",
     "shell.execute_reply.started": "2024-07-18T14:18:30.167017Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4854e2-7e6f-414b-b265-ef87122f819c",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a43fd0c-ccd4-4f68-ba3c-0ec2346d64f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:30.253400Z",
     "iopub.status.busy": "2024-07-18T14:18:30.253012Z",
     "iopub.status.idle": "2024-07-18T14:18:30.341538Z",
     "shell.execute_reply": "2024-07-18T14:18:30.340402Z",
     "shell.execute_reply.started": "2024-07-18T14:18:30.253357Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10103682-7f3d-4225-9da9-b9271dfb0aac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:30.343714Z",
     "iopub.status.busy": "2024-07-18T14:18:30.343325Z",
     "iopub.status.idle": "2024-07-18T14:18:30.434991Z",
     "shell.execute_reply": "2024-07-18T14:18:30.434006Z",
     "shell.execute_reply.started": "2024-07-18T14:18:30.343669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed4d42-c9dd-40d2-bbb2-2f7e93b3d583",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00ec24-bb4e-49ec-9afe-e4d3d05cf905",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:30.437255Z",
     "iopub.status.busy": "2024-07-18T14:18:30.436323Z",
     "iopub.status.idle": "2024-07-18T14:18:30.520344Z",
     "shell.execute_reply": "2024-07-18T14:18:30.519503Z",
     "shell.execute_reply.started": "2024-07-18T14:18:30.437209Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36dbee6-ae0d-4285-8e85-2b3b79731b52",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:30.524004Z",
     "iopub.status.busy": "2024-07-18T14:18:30.522887Z",
     "iopub.status.idle": "2024-07-18T14:18:30.601699Z",
     "shell.execute_reply": "2024-07-18T14:18:30.600879Z",
     "shell.execute_reply.started": "2024-07-18T14:18:30.523966Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cebf53-b9d5-4695-91a3-455e6b4165f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 1 shot temp 0.9 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0998e198-53e1-4151-bdfe-218f445cead5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:30.605032Z",
     "iopub.status.busy": "2024-07-18T14:18:30.604721Z",
     "iopub.status.idle": "2024-07-18T14:18:31.181422Z",
     "shell.execute_reply": "2024-07-18T14:18:31.180589Z",
     "shell.execute_reply.started": "2024-07-18T14:18:30.605002Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['one_shot_tf_idf_09_temp']\n",
    "\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-tf-idf-1-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-tf-idf-1-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83d3ee7-edd5-43af-a0c5-10ab31e79f6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:31.183393Z",
     "iopub.status.busy": "2024-07-18T14:18:31.182796Z",
     "iopub.status.idle": "2024-07-18T14:18:32.140435Z",
     "shell.execute_reply": "2024-07-18T14:18:32.139474Z",
     "shell.execute_reply.started": "2024-07-18T14:18:31.183348Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3512ac89-b7ee-4347-88a4-7714cb12ddbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.142360Z",
     "iopub.status.busy": "2024-07-18T14:18:32.141986Z",
     "iopub.status.idle": "2024-07-18T14:18:32.217190Z",
     "shell.execute_reply": "2024-07-18T14:18:32.216365Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.142318Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647819e4-7495-4a0c-9e56-12137a00b858",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98acb319-304d-4174-b92e-a1e02f3ea5e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.219184Z",
     "iopub.status.busy": "2024-07-18T14:18:32.218829Z",
     "iopub.status.idle": "2024-07-18T14:18:32.237264Z",
     "shell.execute_reply": "2024-07-18T14:18:32.236397Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.219143Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7533b359-2a4a-48db-b35f-cfccf0ff0a67",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.239314Z",
     "iopub.status.busy": "2024-07-18T14:18:32.238929Z",
     "iopub.status.idle": "2024-07-18T14:18:32.323158Z",
     "shell.execute_reply": "2024-07-18T14:18:32.322209Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.239273Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7d67bb-9fd8-4e75-b437-3982e8d5fc79",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf26489-e0d0-4731-a9cc-ab2940cc9fd9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.325302Z",
     "iopub.status.busy": "2024-07-18T14:18:32.324559Z",
     "iopub.status.idle": "2024-07-18T14:18:32.411499Z",
     "shell.execute_reply": "2024-07-18T14:18:32.410572Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.325271Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9581111-2858-4ed4-9c98-c5e73a9b4de8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.426526Z",
     "iopub.status.busy": "2024-07-18T14:18:32.426059Z",
     "iopub.status.idle": "2024-07-18T14:18:32.500984Z",
     "shell.execute_reply": "2024-07-18T14:18:32.499987Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.426492Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688126c9-9392-4d29-b479-dbf0e42f05ee",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a79c9f7-6fc7-49d6-a73c-4ac18a4bcb69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.503698Z",
     "iopub.status.busy": "2024-07-18T14:18:32.503012Z",
     "iopub.status.idle": "2024-07-18T14:18:32.588735Z",
     "shell.execute_reply": "2024-07-18T14:18:32.587467Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.503652Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33754eba-fc2b-4b6e-a266-9788d12cbfa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.591200Z",
     "iopub.status.busy": "2024-07-18T14:18:32.590580Z",
     "iopub.status.idle": "2024-07-18T14:18:32.668753Z",
     "shell.execute_reply": "2024-07-18T14:18:32.667581Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.591154Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248308c4-c4b5-422a-b896-b11d5ad5a185",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6905de-07b7-405d-95ea-997914ae0d28",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.671219Z",
     "iopub.status.busy": "2024-07-18T14:18:32.670541Z",
     "iopub.status.idle": "2024-07-18T14:18:32.753443Z",
     "shell.execute_reply": "2024-07-18T14:18:32.752508Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.671176Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a2558-dc47-4a58-a8d3-069e8d18389c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.755674Z",
     "iopub.status.busy": "2024-07-18T14:18:32.754859Z",
     "iopub.status.idle": "2024-07-18T14:18:32.835143Z",
     "shell.execute_reply": "2024-07-18T14:18:32.834214Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.755616Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303de0c5-f7d6-4200-ab38-0e78491e3731",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e23e5b-168b-44f1-a835-76e5baa2a951",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.837568Z",
     "iopub.status.busy": "2024-07-18T14:18:32.836768Z",
     "iopub.status.idle": "2024-07-18T14:18:32.923409Z",
     "shell.execute_reply": "2024-07-18T14:18:32.922531Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.837523Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eac86d6-590a-487c-bc12-b6ee963c94f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:32.925300Z",
     "iopub.status.busy": "2024-07-18T14:18:32.924961Z",
     "iopub.status.idle": "2024-07-18T14:18:33.016022Z",
     "shell.execute_reply": "2024-07-18T14:18:33.013199Z",
     "shell.execute_reply.started": "2024-07-18T14:18:32.925268Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d709149-9893-4943-95ea-7a3cd0102b40",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c032e9be-a101-42c8-a4f8-8a441015036b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:33.018730Z",
     "iopub.status.busy": "2024-07-18T14:18:33.018229Z",
     "iopub.status.idle": "2024-07-18T14:18:33.102215Z",
     "shell.execute_reply": "2024-07-18T14:18:33.099346Z",
     "shell.execute_reply.started": "2024-07-18T14:18:33.018696Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec82646-0684-4235-a169-b426c9212c94",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:33.104592Z",
     "iopub.status.busy": "2024-07-18T14:18:33.103879Z",
     "iopub.status.idle": "2024-07-18T14:18:33.181449Z",
     "shell.execute_reply": "2024-07-18T14:18:33.180675Z",
     "shell.execute_reply.started": "2024-07-18T14:18:33.104452Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9297bc86-295f-4aa4-84b7-ac27923683b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 2 shot temp 0 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c0a0a-2ed4-4dc8-b0c6-fd87acac0d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:33.185320Z",
     "iopub.status.busy": "2024-07-18T14:18:33.185001Z",
     "iopub.status.idle": "2024-07-18T14:18:33.681246Z",
     "shell.execute_reply": "2024-07-18T14:18:33.680424Z",
     "shell.execute_reply.started": "2024-07-18T14:18:33.185291Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['two_shot_tf_idf']\n",
    "\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-2-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-2-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee839b1-5d26-4caf-b465-4095df33bdfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:33.685002Z",
     "iopub.status.busy": "2024-07-18T14:18:33.684712Z",
     "iopub.status.idle": "2024-07-18T14:18:34.564240Z",
     "shell.execute_reply": "2024-07-18T14:18:34.563217Z",
     "shell.execute_reply.started": "2024-07-18T14:18:33.684972Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cb65ae-ff6d-4390-836a-2a6269ba250e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:34.566579Z",
     "iopub.status.busy": "2024-07-18T14:18:34.565562Z",
     "iopub.status.idle": "2024-07-18T14:18:34.648977Z",
     "shell.execute_reply": "2024-07-18T14:18:34.648081Z",
     "shell.execute_reply.started": "2024-07-18T14:18:34.566533Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dc11a3-7649-492b-a506-f5ea135bdd0e",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e635c6-e8a7-4e00-9c14-3f35cc0cffbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:34.651201Z",
     "iopub.status.busy": "2024-07-18T14:18:34.650534Z",
     "iopub.status.idle": "2024-07-18T14:18:34.665329Z",
     "shell.execute_reply": "2024-07-18T14:18:34.664343Z",
     "shell.execute_reply.started": "2024-07-18T14:18:34.651156Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443df34a-c606-4a08-9a0d-d3d72d7909ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:34.668965Z",
     "iopub.status.busy": "2024-07-18T14:18:34.667883Z",
     "iopub.status.idle": "2024-07-18T14:18:34.747336Z",
     "shell.execute_reply": "2024-07-18T14:18:34.746496Z",
     "shell.execute_reply.started": "2024-07-18T14:18:34.668918Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ad92aa-9428-4971-8a11-eae8572e767d",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071d3ed3-463e-4c47-a1f5-76cfea6b666e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:34.750652Z",
     "iopub.status.busy": "2024-07-18T14:18:34.750354Z",
     "iopub.status.idle": "2024-07-18T14:18:34.833627Z",
     "shell.execute_reply": "2024-07-18T14:18:34.832631Z",
     "shell.execute_reply.started": "2024-07-18T14:18:34.750624Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff9059-a71b-465a-ada3-88f32e95cf57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:34.836240Z",
     "iopub.status.busy": "2024-07-18T14:18:34.835386Z",
     "iopub.status.idle": "2024-07-18T14:18:34.917629Z",
     "shell.execute_reply": "2024-07-18T14:18:34.916710Z",
     "shell.execute_reply.started": "2024-07-18T14:18:34.836207Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ee3a67-2dc4-4557-ace8-6eccf2f1c92c",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a131d075-3796-4838-8c05-ab14b9017923",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:34.921282Z",
     "iopub.status.busy": "2024-07-18T14:18:34.919189Z",
     "iopub.status.idle": "2024-07-18T14:18:35.008790Z",
     "shell.execute_reply": "2024-07-18T14:18:35.006064Z",
     "shell.execute_reply.started": "2024-07-18T14:18:34.921247Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea1b745-7523-43fb-a477-a1895675783a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:35.011073Z",
     "iopub.status.busy": "2024-07-18T14:18:35.010672Z",
     "iopub.status.idle": "2024-07-18T14:18:35.094607Z",
     "shell.execute_reply": "2024-07-18T14:18:35.093643Z",
     "shell.execute_reply.started": "2024-07-18T14:18:35.011043Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f447261-a8ec-4d3a-bf1a-89b0575287f0",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7310c8f4-4380-4076-a226-2b88ed6a1011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:35.097407Z",
     "iopub.status.busy": "2024-07-18T14:18:35.095889Z",
     "iopub.status.idle": "2024-07-18T14:18:35.181444Z",
     "shell.execute_reply": "2024-07-18T14:18:35.180394Z",
     "shell.execute_reply.started": "2024-07-18T14:18:35.097359Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ac9075-8724-4fa9-9d78-3d3b5cbde81e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:35.183648Z",
     "iopub.status.busy": "2024-07-18T14:18:35.183262Z",
     "iopub.status.idle": "2024-07-18T14:18:35.284419Z",
     "shell.execute_reply": "2024-07-18T14:18:35.283520Z",
     "shell.execute_reply.started": "2024-07-18T14:18:35.183605Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd9fd24-16a2-4797-a8ac-d19dfd1685be",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e78fa4-570a-4c55-af46-12b97167ac63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:35.287662Z",
     "iopub.status.busy": "2024-07-18T14:18:35.287029Z",
     "iopub.status.idle": "2024-07-18T14:18:35.372239Z",
     "shell.execute_reply": "2024-07-18T14:18:35.371247Z",
     "shell.execute_reply.started": "2024-07-18T14:18:35.287618Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440a4c85-ff4c-499e-853d-112d123a5a3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:35.374761Z",
     "iopub.status.busy": "2024-07-18T14:18:35.373885Z",
     "iopub.status.idle": "2024-07-18T14:18:35.461646Z",
     "shell.execute_reply": "2024-07-18T14:18:35.460674Z",
     "shell.execute_reply.started": "2024-07-18T14:18:35.374675Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705ec841-af4d-4a24-a68e-c25ab24b3219",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a8733d-be76-4bf6-a505-5e78448b8c38",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:35.464063Z",
     "iopub.status.busy": "2024-07-18T14:18:35.463412Z",
     "iopub.status.idle": "2024-07-18T14:18:35.545650Z",
     "shell.execute_reply": "2024-07-18T14:18:35.544723Z",
     "shell.execute_reply.started": "2024-07-18T14:18:35.464017Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5d73f1-18b1-4fd4-999c-54350b5e88bc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:35.547892Z",
     "iopub.status.busy": "2024-07-18T14:18:35.547419Z",
     "iopub.status.idle": "2024-07-18T14:18:35.629771Z",
     "shell.execute_reply": "2024-07-18T14:18:35.628983Z",
     "shell.execute_reply.started": "2024-07-18T14:18:35.547851Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01882e78-6555-4bb2-8f64-3bc3edfff77f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 2 shot temp 0.9 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2639047-d7fe-49d0-b82d-715f844f47a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:35.631800Z",
     "iopub.status.busy": "2024-07-18T14:18:35.631189Z",
     "iopub.status.idle": "2024-07-18T14:18:36.065459Z",
     "shell.execute_reply": "2024-07-18T14:18:36.064394Z",
     "shell.execute_reply.started": "2024-07-18T14:18:35.631714Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['two_shot_tf_idf_09_temp']\n",
    "\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-tf-idf-2-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-tf-idf-2-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb49de7c-71fa-46c1-a724-d8010e6adabe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:36.067495Z",
     "iopub.status.busy": "2024-07-18T14:18:36.067125Z",
     "iopub.status.idle": "2024-07-18T14:18:37.101691Z",
     "shell.execute_reply": "2024-07-18T14:18:37.100566Z",
     "shell.execute_reply.started": "2024-07-18T14:18:36.067453Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bba9a5-e1e2-4958-b8a0-fcd69d5bb9a4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.104177Z",
     "iopub.status.busy": "2024-07-18T14:18:37.103650Z",
     "iopub.status.idle": "2024-07-18T14:18:37.165444Z",
     "shell.execute_reply": "2024-07-18T14:18:37.164622Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.104123Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47460a5a-8e14-4349-8df8-c7d2f1d86caf",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac15fe0-81b0-4a04-bea8-4d101d773e8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.170892Z",
     "iopub.status.busy": "2024-07-18T14:18:37.170590Z",
     "iopub.status.idle": "2024-07-18T14:18:37.201307Z",
     "shell.execute_reply": "2024-07-18T14:18:37.200403Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.170864Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d50124d-ff9d-42cd-883c-6ab6b04a69b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.202849Z",
     "iopub.status.busy": "2024-07-18T14:18:37.202560Z",
     "iopub.status.idle": "2024-07-18T14:18:37.291764Z",
     "shell.execute_reply": "2024-07-18T14:18:37.290451Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.202822Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69acf2f-78f2-40a4-882f-86c060605f18",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56787386-15a1-4418-afdc-87ad1bf09680",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.296848Z",
     "iopub.status.busy": "2024-07-18T14:18:37.295934Z",
     "iopub.status.idle": "2024-07-18T14:18:37.377354Z",
     "shell.execute_reply": "2024-07-18T14:18:37.376404Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.296800Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580d0011-5550-449c-90ad-c94094e668d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.379454Z",
     "iopub.status.busy": "2024-07-18T14:18:37.378866Z",
     "iopub.status.idle": "2024-07-18T14:18:37.456955Z",
     "shell.execute_reply": "2024-07-18T14:18:37.456079Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.379409Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abd6611-07cb-47e3-bf1e-831fb9da31ab",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32437cb2-b42c-4457-b647-a165335f8cd0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.458901Z",
     "iopub.status.busy": "2024-07-18T14:18:37.458199Z",
     "iopub.status.idle": "2024-07-18T14:18:37.541271Z",
     "shell.execute_reply": "2024-07-18T14:18:37.540453Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.458859Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2057900b-7a58-4b06-98c2-64294443811d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.545487Z",
     "iopub.status.busy": "2024-07-18T14:18:37.542395Z",
     "iopub.status.idle": "2024-07-18T14:18:37.623187Z",
     "shell.execute_reply": "2024-07-18T14:18:37.622221Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.545455Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90931ae-2aa0-4f29-95f1-9ce4ef05c7e7",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4647e4b-341c-4b3e-9832-b4e6e803b3b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.625481Z",
     "iopub.status.busy": "2024-07-18T14:18:37.625097Z",
     "iopub.status.idle": "2024-07-18T14:18:37.713664Z",
     "shell.execute_reply": "2024-07-18T14:18:37.712655Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.625438Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4b3053-3ae3-483b-bede-9a45145fb828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.719026Z",
     "iopub.status.busy": "2024-07-18T14:18:37.714919Z",
     "iopub.status.idle": "2024-07-18T14:18:37.801364Z",
     "shell.execute_reply": "2024-07-18T14:18:37.800386Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.718990Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f11d36-be9b-4466-8434-bb0600df2093",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b83244c-c141-4c22-a62b-2cfba782af83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.804204Z",
     "iopub.status.busy": "2024-07-18T14:18:37.802545Z",
     "iopub.status.idle": "2024-07-18T14:18:37.886513Z",
     "shell.execute_reply": "2024-07-18T14:18:37.885299Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.804143Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697b78be-15ff-4b1f-b0d2-25b37c17dba9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.888852Z",
     "iopub.status.busy": "2024-07-18T14:18:37.888305Z",
     "iopub.status.idle": "2024-07-18T14:18:37.974259Z",
     "shell.execute_reply": "2024-07-18T14:18:37.973210Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.888806Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65c9d9-dfae-459c-981f-d7ca0e85051d",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c0f4de-d377-40c2-a4bd-c5deac8881fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:37.976955Z",
     "iopub.status.busy": "2024-07-18T14:18:37.975905Z",
     "iopub.status.idle": "2024-07-18T14:18:38.059090Z",
     "shell.execute_reply": "2024-07-18T14:18:38.058162Z",
     "shell.execute_reply.started": "2024-07-18T14:18:37.976906Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4447894c-29a1-4d11-9325-b5510c3f40ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:38.061517Z",
     "iopub.status.busy": "2024-07-18T14:18:38.060840Z",
     "iopub.status.idle": "2024-07-18T14:18:38.146749Z",
     "shell.execute_reply": "2024-07-18T14:18:38.145822Z",
     "shell.execute_reply.started": "2024-07-18T14:18:38.061474Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18dc1d11-9314-483a-9b14-4012a3adeae0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 3 shot temp 0 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11f4613-d94e-4941-a801-da0bdfecac03",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:38.149550Z",
     "iopub.status.busy": "2024-07-18T14:18:38.148875Z",
     "iopub.status.idle": "2024-07-18T14:18:39.118081Z",
     "shell.execute_reply": "2024-07-18T14:18:39.117028Z",
     "shell.execute_reply.started": "2024-07-18T14:18:38.149507Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['three_shot_tf_idf']\n",
    "\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55b1875-083f-476d-ad51-68a5db1c88dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:39.119635Z",
     "iopub.status.busy": "2024-07-18T14:18:39.119372Z",
     "iopub.status.idle": "2024-07-18T14:18:40.040922Z",
     "shell.execute_reply": "2024-07-18T14:18:40.040033Z",
     "shell.execute_reply.started": "2024-07-18T14:18:39.119610Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c062ef0-b7ce-41ec-8770-876c4c829761",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.042711Z",
     "iopub.status.busy": "2024-07-18T14:18:40.042214Z",
     "iopub.status.idle": "2024-07-18T14:18:40.145615Z",
     "shell.execute_reply": "2024-07-18T14:18:40.144806Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.042667Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f73c3-cded-46aa-8998-1c6808b1aead",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175005f8-f215-4c08-b7de-8a3c28c4b405",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.147149Z",
     "iopub.status.busy": "2024-07-18T14:18:40.146729Z",
     "iopub.status.idle": "2024-07-18T14:18:40.153424Z",
     "shell.execute_reply": "2024-07-18T14:18:40.152518Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.147039Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "row_metrics.append(label_score_weight)\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534ea8c8-1ec3-4f58-aea4-6e895523d31b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.155287Z",
     "iopub.status.busy": "2024-07-18T14:18:40.154678Z",
     "iopub.status.idle": "2024-07-18T14:18:40.242745Z",
     "shell.execute_reply": "2024-07-18T14:18:40.241648Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.155198Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98054456-bad3-4761-919e-45e85fb1d508",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93074427-0622-49cd-a3ba-46430882ed25",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.245202Z",
     "iopub.status.busy": "2024-07-18T14:18:40.244158Z",
     "iopub.status.idle": "2024-07-18T14:18:40.328474Z",
     "shell.execute_reply": "2024-07-18T14:18:40.327313Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.245156Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575aca04-504e-497e-855c-4632ca54fb80",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.336210Z",
     "iopub.status.busy": "2024-07-18T14:18:40.330005Z",
     "iopub.status.idle": "2024-07-18T14:18:40.414610Z",
     "shell.execute_reply": "2024-07-18T14:18:40.413469Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.336168Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c39fe3c-ef1c-400a-856a-61de88374175",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2000e-2974-4ab6-b90a-2e0c47cfa96b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.416565Z",
     "iopub.status.busy": "2024-07-18T14:18:40.416048Z",
     "iopub.status.idle": "2024-07-18T14:18:40.501220Z",
     "shell.execute_reply": "2024-07-18T14:18:40.500384Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.416520Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ff2686-de39-445e-b746-e82af1e334bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.503173Z",
     "iopub.status.busy": "2024-07-18T14:18:40.502376Z",
     "iopub.status.idle": "2024-07-18T14:18:40.595142Z",
     "shell.execute_reply": "2024-07-18T14:18:40.594026Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.503139Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36dbb94b-1ee4-4b6f-bbd1-9784c9c0a406",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e05e19-6a6c-4c72-824a-b521da42d153",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.597118Z",
     "iopub.status.busy": "2024-07-18T14:18:40.596551Z",
     "iopub.status.idle": "2024-07-18T14:18:40.684212Z",
     "shell.execute_reply": "2024-07-18T14:18:40.683232Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.597052Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf7f3cb-ff5d-49c1-ab3d-c4c1241a216f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.687893Z",
     "iopub.status.busy": "2024-07-18T14:18:40.686208Z",
     "iopub.status.idle": "2024-07-18T14:18:40.773655Z",
     "shell.execute_reply": "2024-07-18T14:18:40.772439Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.687706Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f7fa37-4ab6-41ef-8080-465fa5c21e6f",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f117e-8579-4849-962b-359e04f56e2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.776043Z",
     "iopub.status.busy": "2024-07-18T14:18:40.775253Z",
     "iopub.status.idle": "2024-07-18T14:18:40.865383Z",
     "shell.execute_reply": "2024-07-18T14:18:40.864391Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.775997Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9160e5e-737a-4c9b-b00b-d8e1b6a4f3e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.868012Z",
     "iopub.status.busy": "2024-07-18T14:18:40.866584Z",
     "iopub.status.idle": "2024-07-18T14:18:40.967988Z",
     "shell.execute_reply": "2024-07-18T14:18:40.958772Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.867966Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96bac705-a8a3-4179-8a05-c6796c027add",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93045935-e10f-40e7-aa1a-169ff6ecd375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:40.971864Z",
     "iopub.status.busy": "2024-07-18T14:18:40.971071Z",
     "iopub.status.idle": "2024-07-18T14:18:41.060302Z",
     "shell.execute_reply": "2024-07-18T14:18:41.059211Z",
     "shell.execute_reply.started": "2024-07-18T14:18:40.971815Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a18a53-9d32-4ed2-8335-e263363a6062",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:41.062175Z",
     "iopub.status.busy": "2024-07-18T14:18:41.061794Z",
     "iopub.status.idle": "2024-07-18T14:18:41.142099Z",
     "shell.execute_reply": "2024-07-18T14:18:41.141157Z",
     "shell.execute_reply.started": "2024-07-18T14:18:41.062136Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79735888-ffc7-442e-8dcd-4e5ceefcc43c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 3 shot temp 0.9 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ada4b8-e924-4a1c-845c-c7d95721852c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:41.144718Z",
     "iopub.status.busy": "2024-07-18T14:18:41.143805Z",
     "iopub.status.idle": "2024-07-18T14:18:42.119149Z",
     "shell.execute_reply": "2024-07-18T14:18:42.118358Z",
     "shell.execute_reply.started": "2024-07-18T14:18:41.144669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['three_shot_tf_idf_09_temp']\n",
    "\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-tf-idf-3-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2972f94c-bf0a-4d7a-abb9-ee817cd84318",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:42.120782Z",
     "iopub.status.busy": "2024-07-18T14:18:42.120328Z",
     "iopub.status.idle": "2024-07-18T14:18:43.066253Z",
     "shell.execute_reply": "2024-07-18T14:18:43.065183Z",
     "shell.execute_reply.started": "2024-07-18T14:18:42.120747Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd3e4da-1019-4638-96c8-44b867999f6c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.068316Z",
     "iopub.status.busy": "2024-07-18T14:18:43.067752Z",
     "iopub.status.idle": "2024-07-18T14:18:43.157469Z",
     "shell.execute_reply": "2024-07-18T14:18:43.156570Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.068272Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffaef79-8fde-4828-a949-0a5e8e7da4ba",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccce22c-0b8c-4135-89fd-d8644b688fa9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.159447Z",
     "iopub.status.busy": "2024-07-18T14:18:43.158658Z",
     "iopub.status.idle": "2024-07-18T14:18:43.173437Z",
     "shell.execute_reply": "2024-07-18T14:18:43.172171Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.159413Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da7c0a8-a997-48a5-9013-a6d6207d2dc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.174701Z",
     "iopub.status.busy": "2024-07-18T14:18:43.174431Z",
     "iopub.status.idle": "2024-07-18T14:18:43.268415Z",
     "shell.execute_reply": "2024-07-18T14:18:43.267337Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.174674Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66b60b-6269-4073-9188-5ae034d9474e",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f5eb2-9c5f-492a-8685-7eb9d4f47a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.272436Z",
     "iopub.status.busy": "2024-07-18T14:18:43.272124Z",
     "iopub.status.idle": "2024-07-18T14:18:43.365690Z",
     "shell.execute_reply": "2024-07-18T14:18:43.362510Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.272407Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f15776c-3444-46cc-b1e5-394218beaaaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.372252Z",
     "iopub.status.busy": "2024-07-18T14:18:43.369643Z",
     "iopub.status.idle": "2024-07-18T14:18:43.455475Z",
     "shell.execute_reply": "2024-07-18T14:18:43.454457Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.372119Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fbae3b-974d-4eb9-bff7-08fcdf553d98",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdecebf-90d7-4ec7-a273-bb374b8ddc72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.457839Z",
     "iopub.status.busy": "2024-07-18T14:18:43.457453Z",
     "iopub.status.idle": "2024-07-18T14:18:43.542750Z",
     "shell.execute_reply": "2024-07-18T14:18:43.540500Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.457796Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a9222b-16b5-478c-ba8f-4407778cc364",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.545393Z",
     "iopub.status.busy": "2024-07-18T14:18:43.544991Z",
     "iopub.status.idle": "2024-07-18T14:18:43.643900Z",
     "shell.execute_reply": "2024-07-18T14:18:43.636628Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.545349Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d6ab7-44dd-4930-828b-f8fb1c664aea",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d1b4d-62ff-42c8-a19f-4e1b66ecba8b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.649307Z",
     "iopub.status.busy": "2024-07-18T14:18:43.645777Z",
     "iopub.status.idle": "2024-07-18T14:18:43.728733Z",
     "shell.execute_reply": "2024-07-18T14:18:43.727712Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.649266Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41e3f6b-b6db-49a3-81e5-917d3a8498a0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.742992Z",
     "iopub.status.busy": "2024-07-18T14:18:43.730216Z",
     "iopub.status.idle": "2024-07-18T14:18:43.818917Z",
     "shell.execute_reply": "2024-07-18T14:18:43.817969Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.742941Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e3e016-c5b9-4a83-b760-96e30aafc41d",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fd525-4d26-48bb-ae1d-21c6a4f8da1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.820946Z",
     "iopub.status.busy": "2024-07-18T14:18:43.820157Z",
     "iopub.status.idle": "2024-07-18T14:18:43.905656Z",
     "shell.execute_reply": "2024-07-18T14:18:43.904536Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.820914Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46edb332-e7e1-46c8-9b1e-7bae54ae65a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:43.908151Z",
     "iopub.status.busy": "2024-07-18T14:18:43.907767Z",
     "iopub.status.idle": "2024-07-18T14:18:44.003401Z",
     "shell.execute_reply": "2024-07-18T14:18:44.002202Z",
     "shell.execute_reply.started": "2024-07-18T14:18:43.908108Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c425af7-c736-4456-b8e5-621ca696473f",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc964fc-ab66-4558-a4c6-d037cb45a656",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:44.005961Z",
     "iopub.status.busy": "2024-07-18T14:18:44.005084Z",
     "iopub.status.idle": "2024-07-18T14:18:44.086467Z",
     "shell.execute_reply": "2024-07-18T14:18:44.085523Z",
     "shell.execute_reply.started": "2024-07-18T14:18:44.005914Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8048ddcb-3553-45b2-8f73-7cf28c7e8941",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:44.091180Z",
     "iopub.status.busy": "2024-07-18T14:18:44.087570Z",
     "iopub.status.idle": "2024-07-18T14:18:44.168544Z",
     "shell.execute_reply": "2024-07-18T14:18:44.167690Z",
     "shell.execute_reply.started": "2024-07-18T14:18:44.091144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ebfe9c8-0d61-45d1-a281-08f0785237a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 4 shot temp 0 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5741c-b240-4e6d-8243-5c409c5a320e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:44.173098Z",
     "iopub.status.busy": "2024-07-18T14:18:44.172472Z",
     "iopub.status.idle": "2024-07-18T14:18:45.062914Z",
     "shell.execute_reply": "2024-07-18T14:18:45.061132Z",
     "shell.execute_reply.started": "2024-07-18T14:18:44.173062Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['four_shot_tf_id']\n",
    "\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-4-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-4-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292c8a37-1ef1-468b-aca4-cf6d65ccefb0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:45.064858Z",
     "iopub.status.busy": "2024-07-18T14:18:45.064399Z",
     "iopub.status.idle": "2024-07-18T14:18:45.921640Z",
     "shell.execute_reply": "2024-07-18T14:18:45.920681Z",
     "shell.execute_reply.started": "2024-07-18T14:18:45.064813Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4188b8c-00fb-42db-812b-c01653db1b7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:45.923439Z",
     "iopub.status.busy": "2024-07-18T14:18:45.923015Z",
     "iopub.status.idle": "2024-07-18T14:18:46.021366Z",
     "shell.execute_reply": "2024-07-18T14:18:46.020572Z",
     "shell.execute_reply.started": "2024-07-18T14:18:45.923394Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff80200-8621-48a3-9cb7-306f505556e8",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c25677d-2634-49b9-8548-6bb4ee44e78c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.023084Z",
     "iopub.status.busy": "2024-07-18T14:18:46.022614Z",
     "iopub.status.idle": "2024-07-18T14:18:46.029453Z",
     "shell.execute_reply": "2024-07-18T14:18:46.028484Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.023028Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f70cebf-be63-43dd-a398-15068842b510",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.031465Z",
     "iopub.status.busy": "2024-07-18T14:18:46.030856Z",
     "iopub.status.idle": "2024-07-18T14:18:46.116496Z",
     "shell.execute_reply": "2024-07-18T14:18:46.115488Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.031422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130a65c9-55f9-43dc-9fb1-8a9e38806ae6",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d906a2c-9fce-4e77-8a97-c160a1c72d58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.118359Z",
     "iopub.status.busy": "2024-07-18T14:18:46.117975Z",
     "iopub.status.idle": "2024-07-18T14:18:46.196356Z",
     "shell.execute_reply": "2024-07-18T14:18:46.195493Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.118317Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c050312-b19a-4475-b80b-df52aa30b646",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.198116Z",
     "iopub.status.busy": "2024-07-18T14:18:46.197629Z",
     "iopub.status.idle": "2024-07-18T14:18:46.285915Z",
     "shell.execute_reply": "2024-07-18T14:18:46.285107Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.198076Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc2757-2998-420b-8a2f-dc01d0f54f96",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4afb31-2d8e-42d6-bdcf-962bdaeb45cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.289534Z",
     "iopub.status.busy": "2024-07-18T14:18:46.289222Z",
     "iopub.status.idle": "2024-07-18T14:18:46.375262Z",
     "shell.execute_reply": "2024-07-18T14:18:46.374318Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.289502Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a024e1f5-ec2a-4993-9d48-a6125e504d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.377068Z",
     "iopub.status.busy": "2024-07-18T14:18:46.376548Z",
     "iopub.status.idle": "2024-07-18T14:18:46.461919Z",
     "shell.execute_reply": "2024-07-18T14:18:46.460400Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.377024Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18806fbc-5ecc-41a1-8a35-95bc1fcb772b",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd25ef-2f0b-42b6-90b3-488bb16ba4d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.464001Z",
     "iopub.status.busy": "2024-07-18T14:18:46.463551Z",
     "iopub.status.idle": "2024-07-18T14:18:46.544454Z",
     "shell.execute_reply": "2024-07-18T14:18:46.543573Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.463958Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d81b278-bf2f-4a68-b3b2-e943a79dbfbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.547657Z",
     "iopub.status.busy": "2024-07-18T14:18:46.547291Z",
     "iopub.status.idle": "2024-07-18T14:18:46.633667Z",
     "shell.execute_reply": "2024-07-18T14:18:46.632694Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.547628Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088df1bc-0a16-4b03-b41d-a4ca84125947",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d82318-086c-4bd5-94f5-57602a0546d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.635616Z",
     "iopub.status.busy": "2024-07-18T14:18:46.635008Z",
     "iopub.status.idle": "2024-07-18T14:18:46.725393Z",
     "shell.execute_reply": "2024-07-18T14:18:46.724388Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.635575Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d170b4ae-7e3f-41f1-93bc-7eead8847058",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.729015Z",
     "iopub.status.busy": "2024-07-18T14:18:46.728269Z",
     "iopub.status.idle": "2024-07-18T14:18:46.805476Z",
     "shell.execute_reply": "2024-07-18T14:18:46.804484Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.728984Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e514b3-67e3-464a-9e2b-c0685654d0c8",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad875427-0910-4dec-96c5-79a4db35ec2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.807365Z",
     "iopub.status.busy": "2024-07-18T14:18:46.806840Z",
     "iopub.status.idle": "2024-07-18T14:18:46.901315Z",
     "shell.execute_reply": "2024-07-18T14:18:46.900502Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.807324Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f1501a-4f13-4e9d-970b-9336335f514b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.903127Z",
     "iopub.status.busy": "2024-07-18T14:18:46.902625Z",
     "iopub.status.idle": "2024-07-18T14:18:46.986788Z",
     "shell.execute_reply": "2024-07-18T14:18:46.986020Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.903085Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a679c0-df82-4d5b-8eeb-29f64430cb2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-09T18:20:21.164799Z",
     "iopub.status.busy": "2024-05-09T18:20:21.163980Z",
     "iopub.status.idle": "2024-05-09T18:20:21.168586Z",
     "shell.execute_reply": "2024-05-09T18:20:21.167539Z",
     "shell.execute_reply.started": "2024-05-09T18:20:21.164762Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Metrics for Experiment 2 - 4 shot temp 0.9 (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163729b8-5382-45ea-be38-8c5320484df7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:46.988732Z",
     "iopub.status.busy": "2024-07-18T14:18:46.987981Z",
     "iopub.status.idle": "2024-07-18T14:18:47.828478Z",
     "shell.execute_reply": "2024-07-18T14:18:47.827566Z",
     "shell.execute_reply.started": "2024-07-18T14:18:46.988492Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import ast\n",
    "row_metrics = ['four_shot_tf_idf_09_temp']\n",
    "\n",
    "path = f'llama-outputs-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-tf-idf-4-shot'\n",
    "metrics_path = f'metrics-result-augmented/no-short-data/temp-0.9/top-p-0.6/ideas-tf-idf-4-shot'\n",
    "\n",
    "if not os.path.exists(metrics_path):\n",
    "    os.makedirs(metrics_path)\n",
    "\n",
    "file_names = [file for file in os.listdir(path) if file.endswith('.json')]\n",
    "\n",
    "exisiting_ids = []\n",
    "df_data_info['truth'] = ''\n",
    "df_data_info['truth_annotation'] = ''\n",
    "df_data_info['prediction'] = ''\n",
    "df_data_info['prediction_annotation'] = ''\n",
    "for name in file_names:\n",
    "    llama_annotated = ''\n",
    "    doc_id = name[-41:-5]\n",
    "    print(doc_id)\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6'):\n",
    "        truth_data = get_from_annotated_dataset(annotated_dataset, doc_id)\n",
    "        with open(f'{path}/{name}', \"r\", encoding='utf8') as file:\n",
    "            llama_annotated = json.load(file)\n",
    "        pred_labels = prediction_to_labels(llama_annotated['response'],truth_data)\n",
    "        truth_labels = truth_to_labels(truth_data)\n",
    "        # print(doc_id)\n",
    "        pred_classification_vector = preprocess_classification(pred_labels.copy())\n",
    "        pred_general_annotation_vector = preprocess_annotation(pred_labels.copy())\n",
    "        # print(pred_classification_vector)\n",
    "        # print(\"==========\")\n",
    "        truth_classification_vector = preprocess_classification(truth_labels.copy())\n",
    "        # print(truth_classification_vector)\n",
    "        # print('######3')\n",
    "        truth_general_annotation_vector = preprocess_annotation(truth_labels.copy())\n",
    "        # print(truth_general_annotation_vector)\n",
    "\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth'] = str(truth_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'truth_annotation'] = str(truth_general_annotation_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction'] = str(pred_classification_vector)\n",
    "        df_data_info.loc[df_data_info['doc_id'] == doc_id, 'prediction_annotation'] = str(pred_general_annotation_vector)\n",
    "df_data_info['truth'] = list(df_data_info['truth'])\n",
    "df_data_info['truth_annotation'] = df_data_info['truth_annotation']\n",
    "df_data_info['prediction'] = df_data_info['prediction']\n",
    "df_data_info['prediction_annotation'] = df_data_info['prediction_annotation']\n",
    "# df_data_info['truth'] = df_data_info['truth'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['truth_annotation'] = df_data_info['truth_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction'] = df_data_info['prediction'].str.strip('[]').str.split(',').map(np.array)\n",
    "# df_data_info['prediction_annotation'] = df_data_info['prediction_annotation'].str.strip('[]').str.split(',').map(np.array)\n",
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_vectors.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce49fe3-5dfc-49c7-8c04-8e21f390e32a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:47.830629Z",
     "iopub.status.busy": "2024-07-18T14:18:47.829781Z",
     "iopub.status.idle": "2024-07-18T14:18:48.657108Z",
     "shell.execute_reply": "2024-07-18T14:18:48.656181Z",
     "shell.execute_reply.started": "2024-07-18T14:18:47.830474Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "aux = []\n",
    "aux2 = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1 = []\n",
    "\n",
    "for index in tqdm(range(len(df_data_info))):\n",
    "    # if type(df_data_info[\"truth\"][index]) != list:\n",
    "    #     print(type(df_data_info[\"truth\"][index]))\n",
    "    #     print(type(eval(df_data_info[\"truth\"][index])))\n",
    "    #     print(eval(df_data_info[\"truth\"][index]))\n",
    "    #     df_data_info.loc[df_data_info[\"truth\"][index], \"truth\"] = eval(df_data_info[\"truth\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction\"] = eval(df_data_info[\"prediction\"][index])\n",
    "    # if type(df_data_info[\"truth_annotation\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"truth_annotation\"][index], \"truth_annotation\"] = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    # if type(df_data_info[\"prediction\"][index]) != list:\n",
    "    #     df_data_info.loc[df_data_info[\"prediction\"][index], \"prediction_annotation\"] = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "    \n",
    "    truth = eval(df_data_info[\"truth\"][index])\n",
    "    prediction = eval(df_data_info[\"prediction\"][index])\n",
    "    truth_annotation = eval(df_data_info[\"truth_annotation\"][index])\n",
    "    prediction_annotation = eval(df_data_info[\"prediction_annotation\"][index])\n",
    "\n",
    "    value = jaccard_score((truth), (prediction), average=\"micro\")\n",
    "    aux.append([value,len(truth)])\n",
    "    value2 = jaccard_score((truth_annotation), (prediction_annotation), average=\"micro\")\n",
    "    aux2.append([value2,len(truth)])\n",
    "    \n",
    "    # precision_now = precision_score(test_data3,test_data4,average='binary')\n",
    "\n",
    "    # print(df_data_info[\"prediction_annotation\"][index].split(','))\n",
    "    precision_now = precision_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    precision.append([precision_now,len(truth)])\n",
    "\n",
    "    recall_now = recall_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    recall.append([recall_now,len(truth)])\n",
    "\n",
    "    f1_now = f1_score(truth_annotation, prediction_annotation,average='binary')\n",
    "    f1.append([f1_now,len(truth)])\n",
    "\n",
    "df_data_info[\"label_score\"] = aux\n",
    "df_data_info[\"annotation_score\"] = aux2\n",
    "df_data_info[\"precision\"] = precision\n",
    "df_data_info[\"recall\"] = recall\n",
    "df_data_info[\"f1\"] = f1\n",
    "df_data_info.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0103a53d-98c4-433a-a60d-9502695a5562",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:48.659039Z",
     "iopub.status.busy": "2024-07-18T14:18:48.658495Z",
     "iopub.status.idle": "2024-07-18T14:18:48.726360Z",
     "shell.execute_reply": "2024-07-18T14:18:48.725575Z",
     "shell.execute_reply.started": "2024-07-18T14:18:48.658996Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data_info.to_csv(f'{metrics_path}/test_data_info_with_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b517a4f-2cf6-4507-b9fc-a64eed38f6e6",
   "metadata": {},
   "source": [
    "#### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c5d2a7-651c-46a2-a44c-cbfa5ff282ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:48.728186Z",
     "iopub.status.busy": "2024-07-18T14:18:48.727830Z",
     "iopub.status.idle": "2024-07-18T14:18:48.753173Z",
     "shell.execute_reply": "2024-07-18T14:18:48.752389Z",
     "shell.execute_reply.started": "2024-07-18T14:18:48.728144Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "\n",
    "label_score_weight = score/count\n",
    "\n",
    "row_metrics.append(label_score_weight)\n",
    "\n",
    "print(label_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca9c98e-e121-4b60-b556-365dd5be1309",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:48.754949Z",
     "iopub.status.busy": "2024-07-18T14:18:48.754488Z",
     "iopub.status.idle": "2024-07-18T14:18:48.841281Z",
     "shell.execute_reply": "2024-07-18T14:18:48.840236Z",
     "shell.execute_reply.started": "2024-07-18T14:18:48.754836Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"label_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "label_score_mean = score/count\n",
    "\n",
    "row_metrics.append(label_score_mean)\n",
    "\n",
    "\n",
    "print(label_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5757f1-c877-49dc-9299-a6bf80dd7cc3",
   "metadata": {},
   "source": [
    "#### Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112244ea-be72-462e-ad0e-766bb62dce76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:48.843543Z",
     "iopub.status.busy": "2024-07-18T14:18:48.842869Z",
     "iopub.status.idle": "2024-07-18T14:18:48.929625Z",
     "shell.execute_reply": "2024-07-18T14:18:48.928634Z",
     "shell.execute_reply.started": "2024-07-18T14:18:48.843499Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "annotation_score_weight = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_weight)\n",
    "\n",
    "print(annotation_score_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc53b44a-cf6d-451e-af00-a24d300883b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:48.932123Z",
     "iopub.status.busy": "2024-07-18T14:18:48.931406Z",
     "iopub.status.idle": "2024-07-18T14:18:49.016391Z",
     "shell.execute_reply": "2024-07-18T14:18:49.015374Z",
     "shell.execute_reply.started": "2024-07-18T14:18:48.932080Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"annotation_score\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "annotation_score_mean = score/count\n",
    "\n",
    "row_metrics.append(annotation_score_mean)\n",
    "print(annotation_score_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b5fece-375a-4f3e-aec8-b221efc453a1",
   "metadata": {},
   "source": [
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c8f3a-acde-43cc-946b-8b617d814d11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.018825Z",
     "iopub.status.busy": "2024-07-18T14:18:49.018139Z",
     "iopub.status.idle": "2024-07-18T14:18:49.111939Z",
     "shell.execute_reply": "2024-07-18T14:18:49.111014Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.018781Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "precision_weight = score/count\n",
    "\n",
    "row_metrics.append(precision_weight)\n",
    "print(precision_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8950f5e-8d31-4083-90dd-65022dd6c47c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.116134Z",
     "iopub.status.busy": "2024-07-18T14:18:49.114704Z",
     "iopub.status.idle": "2024-07-18T14:18:49.208785Z",
     "shell.execute_reply": "2024-07-18T14:18:49.207688Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.116085Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"precision\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "precision_mean = score/count\n",
    "\n",
    "row_metrics.append(precision_mean)\n",
    "print(precision_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa10154-c3cf-49e4-8b25-ffd3e1183e78",
   "metadata": {},
   "source": [
    "#### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fd1557-d5ce-4539-8efc-ae373a97e960",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.212296Z",
     "iopub.status.busy": "2024-07-18T14:18:49.209909Z",
     "iopub.status.idle": "2024-07-18T14:18:49.294191Z",
     "shell.execute_reply": "2024-07-18T14:18:49.291859Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.212210Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "recall_weight = score/count\n",
    "    \n",
    "row_metrics.append(recall_weight)\n",
    "\n",
    "print(recall_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add318f0-3a6e-4c79-a91e-379dfdf2d53e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.296630Z",
     "iopub.status.busy": "2024-07-18T14:18:49.295827Z",
     "iopub.status.idle": "2024-07-18T14:18:49.380804Z",
     "shell.execute_reply": "2024-07-18T14:18:49.379790Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.296508Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"recall\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "recall_mean = score/count\n",
    "\n",
    "row_metrics.append(recall_mean)\n",
    "print(recall_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b85da86-19ce-4388-bd19-cccf9802af10",
   "metadata": {},
   "source": [
    "#### F1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13982790-aab9-45f2-9f00-4ad5191e0c00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.383175Z",
     "iopub.status.busy": "2024-07-18T14:18:49.382472Z",
     "iopub.status.idle": "2024-07-18T14:18:49.473630Z",
     "shell.execute_reply": "2024-07-18T14:18:49.472386Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.383110Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Ponderada\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0] * value[1]\n",
    "    count += value[1]\n",
    "    \n",
    "f1_weight = score/count\n",
    "\n",
    "row_metrics.append(f1_weight)\n",
    "print(f1_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e769cb-a279-42bf-96d0-5bf7f59c7cf4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.482008Z",
     "iopub.status.busy": "2024-07-18T14:18:49.481282Z",
     "iopub.status.idle": "2024-07-18T14:18:49.566662Z",
     "shell.execute_reply": "2024-07-18T14:18:49.565630Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.481963Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Mean\n",
    "\n",
    "score = 0\n",
    "count = 0\n",
    "for value in df_data_info[\"f1\"]:\n",
    "    score += value[0]\n",
    "    count += 1\n",
    "\n",
    "f1_mean = score/count\n",
    "\n",
    "row_metrics.append(f1_mean)\n",
    "print(f1_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e52334-ddec-4dc7-857f-e7f0e6b3923e",
   "metadata": {},
   "source": [
    "#### Global Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e7015e-1f4f-4636-8f74-4c923f1dd6c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.569102Z",
     "iopub.status.busy": "2024-07-18T14:18:49.568368Z",
     "iopub.status.idle": "2024-07-18T14:18:49.653643Z",
     "shell.execute_reply": "2024-07-18T14:18:49.652716Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.569068Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "weight_score = annotation_score_weight * label_score_weight\n",
    "mean_score = annotation_score_mean * label_score_mean\n",
    "\n",
    "row_metrics.append(weight_score)\n",
    "row_metrics.append(mean_score)\n",
    "print(f'Mean score: {mean_score}\\nWeight score: {weight_score}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6879ef06-4bda-48ab-ba7f-92fff1faa14b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.655038Z",
     "iopub.status.busy": "2024-07-18T14:18:49.654721Z",
     "iopub.status.idle": "2024-07-18T14:18:49.745896Z",
     "shell.execute_reply": "2024-07-18T14:18:49.745106Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.655009Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report.append(row_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57357656-8f35-477d-abd8-a0fc9c9f1826",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save and compress metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8394fc04-78be-40e8-b8c0-c9cb6cf58cbe",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.747975Z",
     "iopub.status.busy": "2024-07-18T14:18:49.747396Z",
     "iopub.status.idle": "2024-07-18T14:18:49.835540Z",
     "shell.execute_reply": "2024-07-18T14:18:49.834452Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.747929Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "rows_metrics_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2448eb28-fea4-443b-a980-2bf8b01c842a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.839204Z",
     "iopub.status.busy": "2024-07-18T14:18:49.838571Z",
     "iopub.status.idle": "2024-07-18T14:18:49.923444Z",
     "shell.execute_reply": "2024-07-18T14:18:49.922646Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.839142Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "#Create a DataFrame object\n",
    "df_metrics = pd.DataFrame(rows_metrics_report,\n",
    "                  columns = ['experiment','class_weight' , 'class_mean', 'ann_weight' , 'ann_mean','precision_weight','precision_mean','recall_weight','recall_mean','f1_weight','f1_mean','global_weight','global_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36fc2dd-ad58-4dc4-b46f-25c273e2e7d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T14:18:49.932276Z",
     "iopub.status.busy": "2024-07-18T14:18:49.928936Z",
     "iopub.status.idle": "2024-07-18T14:18:50.043173Z",
     "shell.execute_reply": "2024-07-18T14:18:50.041988Z",
     "shell.execute_reply.started": "2024-07-18T14:18:49.929664Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_metrics.to_csv('df_metrics_no_short.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caecea07-52cd-4839-b909-d6e87d9ad0ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:57:03.027792Z",
     "iopub.status.busy": "2025-01-26T20:57:03.026107Z",
     "iopub.status.idle": "2025-01-26T20:57:03.046583Z",
     "shell.execute_reply": "2025-01-26T20:57:03.040690Z",
     "shell.execute_reply.started": "2025-01-26T20:57:03.027741Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20c8e37-b3a2-4764-a110-15e617bdb5a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:57:04.983762Z",
     "iopub.status.busy": "2025-01-26T20:57:04.982910Z",
     "iopub.status.idle": "2025-01-26T20:57:11.370934Z",
     "shell.execute_reply": "2025-01-26T20:57:11.369604Z",
     "shell.execute_reply.started": "2025-01-26T20:57:04.983726Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('metrics_result.tar.gz','metrics-result')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf061f2-cc6d-4e80-8fdf-cb08f3e61248",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T21:00:45.467420Z",
     "iopub.status.busy": "2025-01-26T21:00:45.466795Z",
     "iopub.status.idle": "2025-01-26T21:02:35.573323Z",
     "shell.execute_reply": "2025-01-26T21:02:35.572082Z",
     "shell.execute_reply.started": "2025-01-26T21:00:45.467385Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('llama-outputs.tar.gz','llama-outputs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ded70e4-1497-46b6-a0c0-3fae0c35d4c5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:59:35.592411Z",
     "iopub.status.busy": "2025-01-26T20:59:35.591681Z",
     "iopub.status.idle": "2025-01-26T20:59:36.216129Z",
     "shell.execute_reply": "2025-01-26T20:59:36.214258Z",
     "shell.execute_reply.started": "2025-01-26T20:59:35.592375Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('data_for_metrics.tar.gz','data_for_metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b3a0d2-d771-4d45-b5da-9df754224f7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:59:36.219674Z",
     "iopub.status.busy": "2025-01-26T20:59:36.218692Z",
     "iopub.status.idle": "2025-01-26T20:59:36.276515Z",
     "shell.execute_reply": "2025-01-26T20:59:36.274810Z",
     "shell.execute_reply.started": "2025-01-26T20:59:36.219615Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('metrics-token.tar.gz','metrics-token')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79e4913-ec9e-4b7e-a7b7-12da995be131",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-26T20:59:36.280344Z",
     "iopub.status.busy": "2025-01-26T20:59:36.279648Z",
     "iopub.status.idle": "2025-01-26T20:59:37.550270Z",
     "shell.execute_reply": "2025-01-26T20:59:37.549374Z",
     "shell.execute_reply.started": "2025-01-26T20:59:36.280302Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('teste-progresso.tar.gz','teste-progresso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79611024-eb25-4e52-be4f-e75edaedbc7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T14:41:07.403431Z",
     "iopub.status.busy": "2024-09-13T14:41:07.402770Z",
     "iopub.status.idle": "2024-09-13T14:41:09.537107Z",
     "shell.execute_reply": "2024-09-13T14:41:09.536027Z",
     "shell.execute_reply.started": "2024-09-13T14:41:07.403395Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('llama-outputs_10_shot_custom_temp_0.tar.gz','llama-outputs-augmented//no-short-data/temp-0.0/top-p-0.6/tf-idf-custom/ideas-tf-idf-10-shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78cc81d-f6b2-4a1d-b486-8160b580b8d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T14:41:26.852778Z",
     "iopub.status.busy": "2024-09-13T14:41:26.850805Z",
     "iopub.status.idle": "2024-09-13T14:41:28.955141Z",
     "shell.execute_reply": "2024-09-13T14:41:28.953988Z",
     "shell.execute_reply.started": "2024-09-13T14:41:26.852735Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('llama-outputs_10_shot_custom_temp_0.9.tar.gz','llama-outputs-augmented//no-short-data/temp-0.9/top-p-0.6/tf-idf-custom/ideas-tf-idf-10-shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adabe21c-a620-4c50-b170-f300c98cdd4d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:40:26.883973Z",
     "iopub.status.busy": "2024-09-13T15:40:26.883446Z",
     "iopub.status.idle": "2024-09-13T15:40:28.881149Z",
     "shell.execute_reply": "2024-09-13T15:40:28.880282Z",
     "shell.execute_reply.started": "2024-09-13T15:40:26.883938Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('llama-outputs_10_shot_custom_temp_0.0.tar.gz','llama-outputs-augmented//no-short-data/temp-0.0/top-p-0.6/ideas-tf-idf-10-shot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419780b6-203b-4c36-9d93-36686884de40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T14:42:02.555412Z",
     "iopub.status.busy": "2024-10-07T14:42:02.554730Z",
     "iopub.status.idle": "2024-10-07T14:42:04.125570Z",
     "shell.execute_reply": "2024-10-07T14:42:04.124640Z",
     "shell.execute_reply.started": "2024-10-07T14:42:02.555377Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('llama-outputs_0_shot_with_detail_no_explanation.tar.gz','llama-outputs-augmented/overall-organization/temp-0.0/top-p-0.6/with_detail_no_explanation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd446647-d524-4829-ab13-9583f7e98a9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-07T15:41:07.442504Z",
     "iopub.status.busy": "2024-10-07T15:41:07.441205Z",
     "iopub.status.idle": "2024-10-07T15:41:08.329608Z",
     "shell.execute_reply": "2024-10-07T15:41:08.328394Z",
     "shell.execute_reply.started": "2024-10-07T15:41:07.442467Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('llama-outputs_0_shot_with_detail_with_explanation.tar.gz','llama-outputs-augmented/overall-organization/temp-0.0/top-p-0.6/with_detail_with_explanation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "720fcd07-86f7-42d7-95c5-ce01e1297cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-09T16:44:25.472140Z",
     "iopub.status.busy": "2024-10-09T16:44:25.470763Z",
     "iopub.status.idle": "2024-10-09T16:44:27.177058Z",
     "shell.execute_reply": "2024-10-09T16:44:27.175896Z",
     "shell.execute_reply.started": "2024-10-09T16:44:25.472046Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "make_tarfile('llama-outputs_0_shot_ideas_raw_typo.tar.gz','llama-outputs-augmented/raw-typos/temp-0.9/top-p-0.6/ideas-0-shot')"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
