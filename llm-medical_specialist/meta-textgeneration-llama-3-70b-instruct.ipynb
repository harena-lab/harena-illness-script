{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f82d9ea",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart - invoke text generation endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c51bc6",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to attach a predictor to an existing endpoint name and invoke the endpoint with example payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f18dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:38:34.323581Z",
     "iopub.status.busy": "2024-07-01T13:38:34.322708Z",
     "iopub.status.idle": "2024-07-01T13:38:48.689553Z",
     "shell.execute_reply": "2024-07-01T13:38:48.688534Z",
     "shell.execute_reply.started": "2024-07-01T13:38:34.323545Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker\n",
    "%pip install jsonlines\n",
    "%pip install tdqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6267543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:53:22.055375Z",
     "iopub.status.busy": "2024-07-01T13:53:22.054980Z",
     "iopub.status.idle": "2024-07-01T13:53:24.077639Z",
     "shell.execute_reply": "2024-07-01T13:53:24.072590Z",
     "shell.execute_reply.started": "2024-07-01T13:53:22.055343Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import retrieve_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98dc72a",
   "metadata": {},
   "source": [
    "Retrieve a predictor from your deployed endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc092a63",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-07-01T13:53:24.088755Z",
     "iopub.status.busy": "2024-07-01T13:53:24.086063Z",
     "iopub.status.idle": "2024-07-01T13:53:24.915828Z",
     "shell.execute_reply": "2024-07-01T13:53:24.914745Z",
     "shell.execute_reply.started": "2024-07-01T13:53:24.088714Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"jumpstart-dft-meta-textgeneration-llama-3-70b-instruct\"\n",
    "predictor = retrieve_default(endpoint_name)\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "endpoint_name = \"jumpstart-dft-meta-textgeneration-llama-3-70b-instruct\"\n",
    "\n",
    "\n",
    "def query_endpoint(payload):\n",
    "    config = Config(\n",
    "    read_timeout=900,\n",
    "    connect_timeout=900,\n",
    "    tcp_keepalive=True,\n",
    "    retries={\"max_attempts\": 0})\n",
    "\n",
    "    client = boto3.client(\"sagemaker-runtime\",config=config)\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "    response = response[\"Body\"].read().decode(\"utf8\")\n",
    "    response = json.loads(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55229311",
   "metadata": {},
   "source": [
    "Now query your endpoint with example payloads."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d050d979-e085-496d-909a-75978a75ca96",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Prompt Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dd25f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T13:41:14.940814Z",
     "iopub.status.busy": "2024-04-25T13:41:14.940180Z",
     "iopub.status.idle": "2024-04-25T13:41:21.136933Z",
     "shell.execute_reply": "2024-04-25T13:41:21.135837Z",
     "shell.execute_reply.started": "2024-04-25T13:41:14.940781Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nwhat is the recipe of mayonnaise?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"stop\": \"<|eot_id|>\"\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83baf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\nI am going to Paris, what should I see?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nParis, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is so great about #1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"stop\": \"<|eot_id|>\"\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nAlways answer with Haiku<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nI am going to Paris, what should I see?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"stop\": \"<|eot_id|>\"\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ffe76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nAlways answer with emojis<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nHow to go from Beijing to NY?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\",\n",
    "    \"parameters\": {\n",
    "        \"max_new_tokens\": 256,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.6,\n",
    "        \"stop\": \"<|eot_id|>\"\n",
    "    }\n",
    "}\n",
    "response = predictor.predict(payload)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dfdfd4",
   "metadata": {},
   "source": [
    "This model supports the following payload parameters. You may specify any subset of these parameters when invoking an endpoint.\n",
    "\n",
    "* **do_sample:** If True, activates logits sampling. If specified, it must be boolean.\n",
    "* **max_new_tokens:** Maximum number of generated tokens. If specified, it must be a positive integer.\n",
    "* **repetition_penalty:** A penalty for repetitive generated text. 1.0 means no penalty.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **stop**: If specified, it must a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "* **seed**: Random sampling seed.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **truncate:** Truncate inputs tokens to the given size.\n",
    "* **typical_p:** Typical decoding mass, according to [Typical Decoding for Natural Language Generation](https://arxiv.org/abs/2202.00666).\n",
    "* **best_of:** Generate best_of sequences and return the one if the highest token logprobs.\n",
    "* **watermark:** Whether to perform watermarking with [A Watermark for Large Language Models](https://arxiv.org/abs/2301.10226).\n",
    "* **details:** Return generation details, to include output token logprobs and IDs.\n",
    "* **decoder_input_details:** Return decoder input token logprobs and IDs.\n",
    "* **top_n_tokens:** Return the N most likely tokens at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1130d-1026-43fa-bfa1-c698cf79cf63",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Medical Specialist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01410659-6b63-4060-a579-1d2fb40d3d64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:53:25.703297Z",
     "iopub.status.busy": "2024-07-01T13:53:25.702473Z",
     "iopub.status.idle": "2024-07-01T13:53:25.716725Z",
     "shell.execute_reply": "2024-07-01T13:53:25.715675Z",
     "shell.execute_reply.started": "2024-07-01T13:53:25.703260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format messages for Llama-3 chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    prompt: List[str] = []\n",
    "    # print(messages[0]['role'])\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        content = \"\".join([\"<|start_header_id|>system<|end_header_id|>\\n\\n\", messages[0][\"content\"], \"<|eot_id|>\", \"<|start_header_id|>user<|end_header_id|>\\n\\n\",messages[1][\"content\"],\"<|eot_id|>\"])\n",
    "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
    "\n",
    "    for user, answer in zip(messages[::2], messages[1::2]):\n",
    "        prompt.extend([\"<|start_header_id|>user<|end_header_id|>\", \"\\n\\n\", (user[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "\n",
    "    prompt.extend([\"<|begin_of_text|>\", (messages[0][\"content\"]).strip(), \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"])\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "llama_config = {\"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"max_new_tokens\": 8192\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487de77a-896b-48ce-8e6b-723c6e667532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:53:27.790538Z",
     "iopub.status.busy": "2024-07-01T13:53:27.790037Z",
     "iopub.status.idle": "2024-07-01T13:53:27.898744Z",
     "shell.execute_reply": "2024-07-01T13:53:27.896123Z",
     "shell.execute_reply.started": "2024-07-01T13:53:27.790502Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "def get_text_example(text):\n",
    "    return str({\"text\":text})\n",
    "def get_annotation_example(text):\n",
    "    return str({\"annotation\":text})\n",
    "def get_examples(file_path):\n",
    "    with jsonlines.open(file_path) as reader:\n",
    "        data = [line for line in reader]\n",
    "    return data\n",
    "\n",
    "def get_from_annotated_dataset(annotated_dataset,_id):\n",
    "    for doc in annotated_dataset:\n",
    "        if doc['doc_id'] == _id:\n",
    "            return doc\n",
    "\n",
    "system_organization_lvl = 'You are a medical assistant specialist. Your task is to evaluate the organization level of manuscripts from medical students. Follow every guideline for evaluation, if any. Answer only with \"Org: X\", where X is the score for the organization level. ONLY answer in portuguese.'\n",
    "prompt_organization_lvl = {'less':['Preciso que você avalie o seguinte texto, e de uma nota para a organização do texto. Os níveis possíveis de organização vão de 1 a 4.\\nA pergunta sobre o assunto está a seguir, começando após \"P.\". A resposta à pergunta começa logo após \"R.\".\\nP. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR. {{question}}','Preciso que você avalie o seguinte texto. Para isso é necessário que você tenha conhecimento médico e clínico sobre medicina interna, principalmente sobre a doença pulmonar obstrutiva crônica (DPOC). Além disso, preciso que você consiga diferenciar entre textos de especialistas sobre DPOC e textos de alunos.\\nAvalie a organização do texto. Os níveis possíveis de organização vão de 1 a 4.\\nA pergunta sobre o assunto está a seguir, começando após \"P.\". A resposta à pergunta começa logo após \"R.\".\\nP. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR. {{question}}']\n",
    "                           ,'semi':['Avalie a organização do texto. A organização diz respeito a como o aluno desenvolve as ideias. Os níveis possíveis de organização vão de 1 a 4. Somente responda com a nota de organização.\\nA pergunta sobre o assunto está a seguir, começando após \"P.\". A resposta à pergunta começa logo após \"R.\".\\nP. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR. {{question}}','Para avaliar o texto a seguir é necessário que você tenha conhecimento médico e clínico sobre medicina interna, principalmente sobre a doença pulmonar obstrutiva crônica (DPOC).\\nAvalie a organização do texto. Os níveis possíveis de organização vão de 1 a 4. A organização diz respeito a como o aluno desenvolve suas ideias. Essas ideias se referem a alguma dessas dimensões: fisiopatologia, etiologia, epidemiologia, história, exame físico, exames complementares, diagnóstico diferencial e tratamento.\\nA pergunta sobre o assunto está a seguir, começando após \"P.\". A resposta à pergunta começa logo após \"R.\".\\nP. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR. {{question}}'],\n",
    "                           'super':['Preciso que você avalie o seguinte texto. Para isso é necessário que você tenha conhecimento médico e clínico sobre medicina interna, principalmente sobre a doença pulmonar obstrutiva crônica (DPOC).\\nAvalie a organização do texto. Os níveis possíveis de organização vão de 1 a 4. A organização diz respeito a como o aluno desenvolve suas ideias. Essas ideias se referem a  alguma dessas dimensões: fisiopatologia, etiologia, epidemiologia, história, exame físico, exames complementares, diagnóstico diferencial e tratamento. Espera-se que, se o aluno começa expressando ideias sobre uma dessas dimensões, fale tudo sobre a dimensão antes de partir para outra dimensão. Além disso, há uma certa ordem esperada, de forma que se ele começar o texto falando sobre tratamento e terminar falando da epidemiologia, isso seria pouco organizado.\\nA pergunta sobre o assunto está a seguir, começando após \"P.\". A resposta à pergunta começa logo após \"R.\".\\nP. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR. {{question}}'],\n",
    "                           'one':[],'two':[],'three':[],'four':[],'five':[],'six':[],'seven':[],'eight':[],'nine':[],'ten':[]}\n",
    "\n",
    "\n",
    "# system_ideas_full = 'You are a helpful medical knowledge assistant for Beginning -Inside Outside (BIO) tagging of medical texts. Your task is to create Medical Named Entity Recognition (NER) annotations using the Beginning -Inside Outside (BIO) format. Label the given sentence (question) using BIO tags from these classes ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic. For tokens not belonging to these classes , do not show in the response. For each token you MUST start a new line. Punctuations should be skipped in tokens. Each line must include: (1) word or token , (2) BIO tag , and (3) entity number for B-tags and I-tags ONLY. Use ’<None >’ for O-tags. Maintain the correct format. Example: ’token ’, BIO -tag , ’entity_1 ’. All the texts will be in portuguese.'\n",
    "\n",
    "system_ideas_full = \"\"\"You are a medical assistant with expertise in document processing.\\n Your task is to tag entities related to these classes ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic. Punctuations should be skipped in tokens. Each line must include: (1) word or phrase , (2) start position and end position of the word or phrase , and (3) class or classes that the text is a part of. Maintain the correct format. Example: ’token ’, start pos and end pos, ’entity_1’. All the texts will be in portuguese. Use JSON as the output format. Example of output:\\n \"A Doença Pulmonar Obstrutiva Crônica é uma condição de insuficiência respiratória de padrão obstrutivo, que ocorre por lesão crônica do parêqnuima pulmonar, a qual culmina em diminuição da complacência pulmonar. A fisiopatogenia envolve um processo inflamatório crônico das vias aéreas e do parênquima que não permite a saída de ar dos pulmões e leva ao acúmulo de volume morto nos alvéolos, mantendo o tórax hiperinsuflado, com acúmulo de CO2 e dificultando as trocas respiratórias.\\nA causa mais comum para DPOC é o tabagismo, mas outras causas incluem a convivência com forno a lenha por longo tempo ou a deficiência genética de alfa-1-antitripsina.\\nO principal sintoma desses pacientes é a dispneia, que se incia em grandes esforços e pode chegar até ao repouso. O diagnóstico é feito pela clínica + espirometria. A principal complicação são as exacerbações de doença que pode ser associada a quadro infeccioso sobreposto.\\nEsses pacientes podem ser divididos segundo os critérios do GOLD entre pacientes muito sintomátisoc&nbsp; e com muitas exacerbações, e o tratamento utiliza LABA, SABA LAMA e CI a depender desses\\n', 'annotations': [['tabagismo', 517, 525, ['epidemiology', 'etiology'], ['forno a lenha', 572, 584, ['epidemiology', 'etiology'], ['forno a lenha + por longo tempo', 572, 600, ['epidemiology'], ['deficiência genética de alfa-1-antitripsina', 607, 649, ['etiology'], ['diagnóstico é feito pela clínica + espirometria', 768, 814, ['exams', 'history'], ['dispneia', 693, 700, ['history'], ['dispneia + incia em grandes esforços', 693, 734, ['history'], ['dispneia + incia em grandes esforços + pode chegar até ao repouso', 693, 763, ['history'], ['exacerbações de doença', 848, 869, ['history', 'pathophysiology'], ['exacerbações de doença + quadro infeccioso', 848, 912, ['history', 'pathophysiology'], ['podem ser divididos segundo os critérios do GOLD', 942, 989, ['history'], ['critérios do GOLD + muito sintomátiso + muitas exacerbações', 973, 1056, ['history'], ['lesão crônica do parêqnuima pulmonar', 119, 154, ['pathophysiology'], ['diminuição da complacência pulmonar', 175, 209, ['pathophysiology'], ['processo inflamatório crônico', 240, 268, ['pathophysiology'], ['não permite a saída de ar dos pulmões', 306, 342, ['pathophysiology'], ['acúmulo de volume morto nos alvéolos', 354, 389, ['pathophysiology'], ['tórax hiperinsuflado', 403, 422, ['pathophysiology'], ['acúmulo de CO2', 429, 442, ['pathophysiology'], ['dificultando as trocas respiratórias', 446, 481, ['pathophysiology'], ['insuficiência respiratória', 55, 80, ['pathophysiology'], ['LABA', 1082, 1085, ['therapeutic']], ['SABA', 1088, 1091, ['therapeutic']], ['LAMA', 1093, 1096, ['therapeutic']], ['CI', 1100, 1101, ['therapeutic']]]\"\"\"\n",
    "\n",
    "system_ideas_full_in_order_zero_shot = \"\"\"You are a medical assistant with expertise in medical document processing.\\n Your task is to tag entities related to these classes ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic. Each line must include: (1) word or phrase, (2) class or classes that the text is a part of. Maintain the correct format. Example: ['token', ['physical']]. All the texts will be in portuguese. ONLY use JSON as the output format, starting with 'annotations'. DO NOT write, only respond in JSON format.\\n\"\"\"\n",
    "\n",
    "system_ideas_full_in_order = \"\"\"You are a medical assistant with expertise in medical document processing.\\n Your task is to tag entities related to these classes ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic. Each line must include: (1) word or phrase, (2) class or classes that the text is a part of. Maintain the correct format. Example: ['token', ['physical']]. All the texts will be in portuguese. ONLY use JSON as the output format, starting with 'annotations'. DO NOT write, only respond in JSON format. Examples of user input and assistant output:\\n \"user input\":\\n \"A Doença Pulmonar Obstrutiva Crônica é uma condição de insuficiência respiratória de padrão obstrutivo, que ocorre por lesão crônica do parêqnuima pulmonar, a qual culmina em diminuição da complacência pulmonar. A fisiopatogenia envolve um processo inflamatório crônico das vias aéreas e do parênquima que não permite a saída de ar dos pulmões e leva ao acúmulo de volume morto nos alvéolos, mantendo o tórax hiperinsuflado, com acúmulo de CO2 e dificultando as trocas respiratórias.\\nA causa mais comum para DPOC é o tabagismo, mas outras causas incluem a convivência com forno a lenha por longo tempo ou a deficiência genética de alfa-1-antitripsina.\\nO principal sintoma desses pacientes é a dispneia, que se incia em grandes esforços e pode chegar até ao repouso. O diagnóstico é feito pela clínica + espirometria. A principal complicação são as exacerbações de doença que pode ser associada a quadro infeccioso sobreposto.\\nEsses pacientes podem ser divididos segundo os critérios do GOLD entre pacientes muito sintomátisoc&nbsp; e com muitas exacerbações, e o tratamento utiliza LABA, SABA LAMA e CI a depender desses\\n', 'assistant output':\\n \"annotations\": [['tabagismo', ['epidemiology', 'etiology'], ['forno a lenha', ['epidemiology', 'etiology'], ['forno a lenha + por longo tempo', ['epidemiology'], ['deficiência genética de alfa-1-antitripsina', ['etiology'], ['diagnóstico é feito pela clínica + espirometria', ['exams', 'history'], ['dispneia', ['history'], ['dispneia + incia em grandes esforços', 693, 734, ['history'], ['dispneia + incia em grandes esforços + pode chegar até ao repouso', 693, 763, ['history'], ['exacerbações de doença', ['history', 'pathophysiology'], ['exacerbações de doença + quadro infeccioso', ['history', 'pathophysiology'], ['podem ser divididos segundo os critérios do GOLD',['history'], ['critérios do GOLD + muito sintomátiso + muitas exacerbações', ['history'], ['lesão crônica do parêqnuima pulmonar', ['pathophysiology'], ['diminuição da complacência pulmonar', ['pathophysiology'], ['processo inflamatório crônico', ['pathophysiology'], ['não permite a saída de ar dos pulmões', ['pathophysiology'], ['acúmulo de volume morto nos alvéolos', ['pathophysiology'], ['tórax hiperinsuflado', ['pathophysiology'], ['acúmulo de CO2', ['pathophysiology'], ['dificultando as trocas respiratórias', ['pathophysiology'], ['insuficiência respiratória', ['pathophysiology'], ['LABA', ['therapeutic']], ['SABA', ['therapeutic']], ['LAMA', ['therapeutic']], ['CI', ['therapeutic']]]\\n\"\"\"\n",
    "\n",
    "system_ideas_full_in_order_2_shot = \"\"\"You are a medical assistant with expertise in medical document processing.\\n Your task is to tag entities related to these classes ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic. Each line must include: (1) word or phrase, (2) class or classes that the text is a part of. Maintain the correct format. Example: ['token', ['physical']]. All the texts will be in portuguese. ONLY use JSON as the output format, starting with 'annotations'. DO NOT write, only respond in JSON format. Examples of user input and assistant output:\\n \"user input\":\\n \"A Doença Pulmonar Obstrutiva Crônica é uma condição de insuficiência respiratória de padrão obstrutivo, que ocorre por lesão crônica do parêqnuima pulmonar, a qual culmina em diminuição da complacência pulmonar. A fisiopatogenia envolve um processo inflamatório crônico das vias aéreas e do parênquima que não permite a saída de ar dos pulmões e leva ao acúmulo de volume morto nos alvéolos, mantendo o tórax hiperinsuflado, com acúmulo de CO2 e dificultando as trocas respiratórias.\\nA causa mais comum para DPOC é o tabagismo, mas outras causas incluem a convivência com forno a lenha por longo tempo ou a deficiência genética de alfa-1-antitripsina.\\nO principal sintoma desses pacientes é a dispneia, que se incia em grandes esforços e pode chegar até ao repouso. O diagnóstico é feito pela clínica + espirometria. A principal complicação são as exacerbações de doença que pode ser associada a quadro infeccioso sobreposto.\\nEsses pacientes podem ser divididos segundo os critérios do GOLD entre pacientes muito sintomátisoc&nbsp; e com muitas exacerbações, e o tratamento utiliza LABA, SABA LAMA e CI a depender desses\\n', 'assistant output':\\n \"annotations\": [['tabagismo', ['epidemiology', 'etiology'], ['forno a lenha', ['epidemiology', 'etiology'], ['forno a lenha + por longo tempo', ['epidemiology'], ['deficiência genética de alfa-1-antitripsina', ['etiology'], ['diagnóstico é feito pela clínica + espirometria', ['exams', 'history'], ['dispneia', ['history'], ['dispneia + incia em grandes esforços', 693, 734, ['history'], ['dispneia + incia em grandes esforços + pode chegar até ao repouso', 693, 763, ['history'], ['exacerbações de doença', ['history', 'pathophysiology'], ['exacerbações de doença + quadro infeccioso', ['history', 'pathophysiology'], ['podem ser divididos segundo os critérios do GOLD',['history'], ['critérios do GOLD + muito sintomátiso + muitas exacerbações', ['history'], ['lesão crônica do parêqnuima pulmonar', ['pathophysiology'], ['diminuição da complacência pulmonar', ['pathophysiology'], ['processo inflamatório crônico', ['pathophysiology'], ['não permite a saída de ar dos pulmões', ['pathophysiology'], ['acúmulo de volume morto nos alvéolos', ['pathophysiology'], ['tórax hiperinsuflado', ['pathophysiology'], ['acúmulo de CO2', ['pathophysiology'], ['dificultando as trocas respiratórias', ['pathophysiology'], ['insuficiência respiratória', ['pathophysiology'], ['LABA', ['therapeutic']], ['SABA', ['therapeutic']], ['LAMA', ['therapeutic']], ['CI', ['therapeutic']]]\\n\"user input\":\\n \"DPOC é uma doença que costuma ocorrer em idosos e muito associada ao tabagismo e inalação de demais partículas tóxicas, com alta prevalência.\\nÉ caracterizada por enfisema e bronquite, havendo tanto o padrão clássico do paciente soprador rosado (magro, avermelhado, predomina enfisema) quanto do tossidor azul (cianótico, sobrepeso, predomina bronquite).\\nComo sintomas clássicos, a DPOC tem como sintomas tosse expectorante crônica, dispneia, infecções de repetição, edema. No exame físico, nota-se timpanismo, tórax aumentado em volume, respiração não enche plenamente a caixa torácica, por vezes uso de musculatura acessória, ruídos adventícios.\\n\",\"assistant output\":\\n\"annotations\":[['idosos', ['epidemiology']], ['muito associada ao tabagismo', ['epidemiology']], ['inalação de demais partículas tóxicas', ['epidemiology', 'etiology']], ['alta prevalência', ['epidemiology']], ['enfisema', ['physical', 'pathophysiology']], ['bronquite', ['physical', 'pathophysiology']], ['soprador rosado', ['pathophysiology', 'physical']], ['magro', ['physical']], ['avermelhado', ['physical']], ['predomina enfisema', ['pathophysiology']], ['tossidor azul', ['pathophysiology', 'physical']], ['cianótico', ['physical']], ['sobrepeso', ['physical']], ['predomina bronquite', ['pathophysiology']], ['tosse expectorante crônica', ['history']], ['dispneia', ['history']], ['infecções de repetição', ['history']], ['edema', ['physical']], ['timpanismo', ['physical']], ['tórax aumentado em volume', ['physical']], ['respiração não enche plenamente a caixa torácica', ['uso de musculatura acessória', ['physical']],['ruídos adventícios', ['physical']]]\"\"\"\n",
    "\n",
    "system_ideas_full_in_order_shot_dynamic = \"\"\"You are a medical assistant with expertise in medical document processing.\\n Your task is to tag entities related to these classes ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic. Each line must include: (1) word or phrase, (2) class or classes that the text is a part of. Maintain the correct format. Example: ['token', ['physical']]. All the texts will be in portuguese. ONLY use JSON as the output format, starting with 'annotations'. DO NOT write, only respond in JSON format. Examples of user input and assistant output:\\n {{shot}}\"\"\"\n",
    "\n",
    "system_ideas_full_bio = \"\"\"\"\n",
    "You are a helpful medical knowledge assistant for Beginning -Inside Outside (BIO) tagging of medical texts. Your task is to create Medical Named Entity Recognition (NER) annotations using the Beginning -Inside Outside (BIO) format. Label the given sentence (question) using BIO tags from the class ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic. For tokens not belonging to these classes , tag as O-tag. Punctuations should be skipped in tokens. Each line must include: (1) word or token , (2) BIO tag , (3) the class or classes that the token is part of. Follow these steps:\\n 1. Extract all the sentences that belong to one of the classes.\\n 2. Tokenize the sentences from the step 1.\\n 3. Tokenize the sentences that do not belong to any of the classes. 4. Return only the text, no additional responses, starting with \"labels:\" in JSON format.\n",
    "Example of output:\\n \"user\":\\n \"Trata-se de uma doença crôica, com processo evolutivo associado a um quadro de remodelamento anatomopatológico da estrutura brônquica e alveolar. Está relacionada com hábitos de vida com o contato direto de ar/fumaça quente, assim como substâncias tóxicas ao epitélio alveolar (e.g.: alcatrão).\\nSeu diagnóstico se dá a partir da achados clínicos (aumento anteroposterior do tórax, dispneia e dessaturação, tempo expiratório prolongado) associado à comprovação da irreversibilidade (total ou parcial) nas provas espirométricas, idealmente.\\nSeu tratamento baseia-se na retirada do agente nocivo (por vezes tabagismo) e controle sintomático (seja com broncodilatadores, corticoides, analgésicos com potencial de conforto respiratório ou mesmo oxigênio terapia).\\n\" \\n\"assistant:\\n     \"labels\": [[\"Trata-se\",\"O\",null],[\"de\",\"O\",null],[\"uma\",\"O\",null],[\"doença\",\"O\",null],[\"crôica\",\"O\",null],[\"O\",\"O\",null],[\"com\",\"O\",null],[\"processo\",\"B\",[\"pathophysiology\"]],[\"evolutivo\",\"I\",[\"pathophysiology\"]],[\"associado\",\"O\",null],[\"a\",\"O\",null],[\"um\",\"O\",null],[\"quadro\",\"O\",null],[\"de\",\"O\",null],[\"remodelamento\",\"B\",[\"pathophysiology\"]],[\"anatomopatológico\",\"I\",[\"pathophysiology\"]],[\"da\",\"I\",[\"pathophysiology\"]],[\"estrutura\",\"I\",[\"pathophysiology\"]],[\"brônquica\",\"I\",[\"pathophysiology\"]],[\"e\",\"I\",[\"pathophysiology\"]],[\"alveolar\",\"I\",[\"pathophysiology\"]],[\".\",\"O\",null],[\"Está\",\"O\",null],[\"relacionada\",\"O\",null],[\"com\",\"O\",null],[\"hábitos\",\"O\",null],[\"de\",\"O\",null],[\"vida\",\"O\",null],[\"com\",\"O\",null],[\"o\",\"O\",null],[\"contato\",\"B\",[\"epidemiology\"]],[\"direto\",\"I\",[\"epidemiology\"]],[\"de\",\"I\",[\"epidemiology\"]],[\"ar/fumaça\",\"I\",[\"epidemiology\"]],[\"quente\",\"I\",[\"epidemiology\"]],[\".\",\"O\",null],[\"assim\",\"O\",null],[\"como\",\"O\",null],[\"substâncias\",\"B\",[\"etiology\",\"pathophysiology\"]],[\"tóxicas\",\"I\",[\"etiology\",\"pathophysiology\"]],[\"ao\",\"I\",[\"etiology\",\"pathophysiology\"]],[\"epitélio\",\"I\",[\"etiology\",\"pathophysiology\"]],[\"alveolar\",\"I\",[\"etiology\",\"pathophysiology\"]],[\"(\",\"O\",null],[\"e\",\"O\",null],[\".\",\"O\",null],[\"g\",\"O\",null],[\".\",\"O\",null],[\":\",\"O\",null],[\"alcatrão\",\"O\",null],[\")\",\"O\",null],[\".\",\"O\",null],[\"Seu\",\"O\",null],[\"diagnóstico\",\"O\",null],[\"se\",\"O\",null],[\"dá\",\"O\",null],[\"a\",\"O\",null],[\"partir\",\"O\",null],[\"da\",\"O\",null],[\"achados\",\"O\",null],[\"clínicos\",\"O\",null],[\"(\",\"O\",null],[\"aumento\",\"B\",[\"physical\"]],[\"anteroposterior\",\"I\",[\"physical\"]],[\"do\",\"I\",[\"physical\"]],[\"tórax\",\"I\",[\"physical\"]],[\"dispneia\",\"B\",[\"history\"]],[\"e\",\"O\",null],[\"dessaturação\",\"B\",[\"physical\"]],[\"tempo\",\"B\",[\"physical\"]],[\"expiratório\",\"I\",[\"physical\"]],[\"prolongado\",\"I\",[\"physical\"]],[\")\",\"O\",null],[\"associado\",\"O\",null],[\"à\",\"O\",null],[\"comprovação\",\"O\",null],[\"da\",\"O\",null],[\"irreversibilidade\",\"B\",[\"exams\"]],[\"(\",\"I\",[\"exams\"]],[\"total\",\"I\",[\"exams\"]],[\"ou\",\"I\",[\"exams\"]],[\"parcial\",\"I\",[\"exams\"]],[\")\",\"I\",[\"exams\"]],[\"nas\",\"I\",[\"exams\"]],[\"provas\",\"I\",[\"exams\"]],[\"espirométricas\",\"I\",[\"exams\"]],[\"idealmente\",\"O\",null],[\".\",\"O\",null],[\"Seu\",\"O\",null],[\"tratamento\",\"O\",null],[\"baseia-se\",\"O\",null],[\"na\",\"O\",null],[\"retirada\",\"B\",[\"therapeutic\"]],[\"do\",\"I\",[\"therapeutic\"]],[\"agente\",\"I\",[\"therapeutic\"]],[\"nocivo\",\"I\",[\"therapeutic\"]],[\"(\",\"O\",null],[\"por\",\"O\",null],[\"vezes\",\"O\",null],[\"tabagismo\",\"O\",null],[\"e\",\"O\",null],[\"controle\",\"B\",[\"therapeutic\"]],[\"sintomático\",\"I\",[\"therapeutic\"]],[\"(\",\"O\",null],[\"seja\",\"O\",null],[\"com\",\"O\",null],[\"broncodilatadores\",\"B\",[\"therapeutic\"]],[\"corticoides\",\"B\",[\"therapeutic\"]],[\"analgésicos\",\"B\",[\"therapeutic\"]],[\"com\",\"I\",[\"therapeutic\"]],[\"potencial\",\"I\",[\"therapeutic\"]],[\"de\",\"I\",[\"therapeutic\"]],[\"conforto\",\"I\",[\"therapeutic\"]],[\"respiratório\",\"I\",[\"therapeutic\"]],[\"ou\",\"O\",null],[\"mesmo\",\"O\",null],[\"oxigênio\",\"B\",[\"therapeutic\"]],[\"terapia\",\"I\",[\"therapeutic\"]],[\")\",\"O\",null],[\".\",\"O\",null]]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "system_ideas_patho = 'You are a medical assistant with expertise in document processing.\\n Your task is to tag entities related to pathophysiology.\\n. For each entity correctly found, it must contain: (1) the word or phrase (text should contain the EXACT characters in the raw input, including punctuation, and typos), (2) start position of the word or phrase, (3) end position of the word or phrase, (4) pathophysiology. Use JSON as the output format. All the texts will be in portuguese.\\nExample of output: user: \"DPOC: é uma enfermidade respiratorio prevenivel e tratavel, que se caracteriza pela presença de obstrução cronica do fluxo aereo é progressiva e associada a inflamação cronica anormal.\"\\n assistant:\\n labels:[[\"obstrução\",\"pathophysiology\"],[\"cronica\",\"pathophysiology],[\"fluxo aereo\",\"pathophysiology\"],[\"é progressiva\",\"pathophysiology\"],[\"inflamação cronica anormal\",\"pathophysiology\"]]'\n",
    "\n",
    "system_ideas_patho_bio = 'You are a helpful medical knowledge assistant for Beginning -Inside Outside (BIO) tagging of medical texts. Your task is to create Medical Named Entity Recognition (NER) annotations using the Beginning -Inside Outside (BIO) format. Label the given sentence (question) using BIO tags from the class ONLY: pathophysiology. If the the token is not part of the class, do not put it into your response.\\n The order for each line must be: (1) word or phrase (text should contain the EXACT characters in the raw input, including punctuation, and typos), (2) BIO tag,(3) start and end position of the word or phrase (position 0 is the start of the text and each new character, space or pontuctuation adds one position),(4) pathophysiology. Use JSON as the output format. All the texts will be in portuguese.\\nExample of output: user: \"DPOC: é uma enfermidade respiratorio prevenivel e tratavel, que se caracteriza pela presença de obstrução cronica do fluxo aereo é progressiva e associada a inflamação cronica anormal.\"\\n assistant:\\n labels:[[\"obstrução\",96,104,\"B\",\"pathophysiology\"],[\"cronica\",106,112,\"I\",\"pathophysiology\"],[\"fluxo\",117,121,\"B\",\"pathophysiology\"],[\"aereo\",123,127,\"I\",\"pathophysiology\"],[\"é\",129,129,\"B\",\"pathophysiology\"],[\"progressiva\",131,141,\"I\",\"pathophysiology\"],[\"inflamação\",157,166,\"B\",\"pathophysiology\"],[\"cronica\",168,174,\"I\",\"pathophysiology\"],[\"anormal\",176,182,\"I\",\"pathophysiology\"]]'\n",
    "\n",
    "prompt_ideas_full = {'less':[\"Preciso que você avalie o seguinte texto de um aluno de medicina. Calcule a quantidade de ideias escritas no texto. Após calcular a quantidade, retorne as partes do texto consideradas como ideias.\\nA pergunta sobre o assunto está a seguir, começando após 'P.'. A resposta à pergunta começa logo após 'R.'.\\nP. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR. {{question}}\"],\n",
    "                     'semi':[\"Preciso que você avalie o seguinte texto de um aluno de medicina. O texto pode ser dividido em unidades de ideias. Essas unidades de ideias podem ser referente a uma ou mais das categorias a seguir: fisiopatologia, epidemiologia, etiologia, história, exame físico, exames complementares, diagnóstico diferencial, tratamento. Caso alguma ideia não fizer parte de alguma dessas categorias, não inclua a ideia.\\nApós dividir o texto em unidades de ideias, retorne as partes do texto consideradas como ideias no formato a seguir:\\n<<{’annotations’:[[‘ideia1’,[‘categoria1’,’categoriaN’]],[‘ideia2’,[‘categoria1’,’categoriaN’]],[‘ideiaN’,[‘categoria1’,’categoriaN’]]]}>>\\nA pergunta sobre o assunto está a seguir, começando após 'P.'. A resposta à pergunta começa logo após 'R.'.\\nP. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR. {{question}}\"],\n",
    "                     'super':[],\n",
    "                     'one-few':[\"Preciso que você avalie o seguinte texto de um aluno de medicina. O texto pode ser dividido em unidades de ideias. Essas unidades de ideias podem ser referente a uma ou mais das categorias a seguir: fisiopatologia, epidemiologia, etiologia, história, exame físico, exames complementares, diagnóstico diferencial, tratamento.\\nApós dividir o texto em unidades de ideias, retorne as partes do texto consideradas como ideias no formato a seguir:\\n<<{’annotations’:[[‘ideia1’,[‘categoria1’,’categoriaN’]],[‘ideia2’,[‘categoria1’,’categoriaN’]],[‘ideiaN’,[‘categoria1’,’categoriaN’]]]}>>\\nA pergunta sobre o assunto está a seguir, começando após 'P.'. A resposta à pergunta começa logo após 'R.'. A seguir, estão textos anotados como exemplo:\\n{{example}} \\nAgora separe e classifique as ideias, utilizando o formato dos exemplos, mas a partir da resposta a seguir:\\nP. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR. {{question}}\"],\n",
    "                    'one-few-test':['Preciso que você avalie o seguinte texto de um aluno de medicina. O texto pode ser dividido em unidades de ideias. Essas unidades de ideias podem ser referente a uma ou mais das categorias a seguir: fisiopatologia, epidemiologia, etiologia, história, exame físico, exames complementares, diagnóstico diferencial, tratamento.\\nApós dividir o texto em unidades de ideias, retorne as partes do texto consideradas como ideias no formato a seguir:\\n<<{\"annotations\":[[‘ideia1\",[‘categoria1\",\"categoriaN\"]],[‘ideia2\",[‘categoria1\",\"categoriaN\"]],[‘ideiaN\",[‘categoria1\",\"categoriaN\"]]]}>>\\nSepare e classifique as ideias, utilizando o formato dos exemplos, a partir da resposta a seguir:\\nP. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR. {{question}}']}\n",
    "\n",
    "file_path = 'prompt-shots.jsonl'\n",
    "examples = get_examples(file_path)\n",
    "annotated_dataset = get_examples('teste-progresso/annotations-medical_specialist-dpoc-json.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2465d6-2ea4-4989-b2c4-fdefd24cb8e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:53:30.466011Z",
     "iopub.status.busy": "2024-07-01T13:53:30.465610Z",
     "iopub.status.idle": "2024-07-01T13:53:30.506789Z",
     "shell.execute_reply": "2024-07-01T13:53:30.501700Z",
     "shell.execute_reply.started": "2024-07-01T13:53:30.465981Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "import sys\n",
    "def set_prompt(prompt_type,prompt_category,prompt_guide_level, prompt_pos, prompt_shot_amount,question, sys_prompt_name='default'):\n",
    "    # user_prompt = prompt_type[prompt_guide_level][prompt_pos]\n",
    "    ideas_shot_template = \"P. Fale TUDO que você sabe sobre doença pulmonar obstrutiva crônica. TODAS as informações que você souber são importantes.\\nR.<<{{studentResponse}}>>\\nSeparação e classificação de ideias:\\n<<{{annotationsResponse}}>>\"\n",
    "    \n",
    "    system_prompt = \"Você é um médico especialista com conhecimento médico e clínico sobre medicina interna, principalmente sobre as doenças doença pulmonar obstrutiva crônica (DPOC). Além disso, você consegue diferenciar entre textos de especialistas sobre IAM e DPOC e textos de alunos. Você é capaz de classificar e separar ideias no texto. Não crie informações que não sejam verdadeiras. SOMENTE responda no formato JSON. Aqui está um exemplo de resposta aceitável:\\n'annotations':<<{[['Tabagismo',['epidemiologia','certo','simples']],['alta carga tabágica',['epidemiologia','certo','simples']]}>>\"\n",
    "    system_prompt_test = \"Você é um médico especialista com conhecimento médico e clínico sobre medicina interna, principalmente sobre as doenças doença pulmonar obstrutiva crônica (DPOC). Além disso, você consegue diferenciar entre textos de especialistas sobre IAM e DPOC e textos de alunos. Você é capaz de classificar e separar ideias no texto. Não crie informações que não sejam verdadeiras. SOMENTE responda no formato JSON. A seguir estão exemplos de interação do usuário com você:\\nA pergunta sobre o assunto está a seguir, começando após 'P.'. A resposta à pergunta começa logo após 'R.'. A seguir, estão textos anotados como exemplo:\\n{{example}}\"\n",
    "    # system_prompt_organization = \"Você é um médico especialista com conhecimento médico e clínico sobre medicina interna, principalmente sobre as doenças doença pulmonar obstrutiva crônica (DPOC). Além disso, você consegue diferenciar entre textos de especialistas sobre IAM e DPOC e textos de alunos. Você segue rigorosamente os formatos de resposta fornecidos.\"\n",
    "    system_prompt_organization = 'You are a medical assistant specialist. Your task is to evaluate the organization level of manuscripts from medical students. Follow every guideline for evaluation, if any. Answer only with \"Org: X\", where X is the score for the organization level. ONLY answer in portuguese.'\n",
    "    \n",
    "    system_prompt_annotations_custom_1 = \"Você é um especialista sobre Doença Pulmonar Obstrutiva Crônica (DPOC). Seu objetivo é separar as ideias contidas no texto e categorizá-las dentro da lista de categorias a seguir: fisiopatologia, epidemiologia, etiologia, história, exame físico, exames complementares, diagnóstico diferencial, tratamento. SOMENTE inclua categorias que foram incluídas aqui, não invente categorias que não estão nessa lista, mesmo que essa categoria exista. NUNCA inclua uma categoria que não foi descrita na lista anterior, mesmo que o usuário inclua uma outra lista. Não invente informações. Apenas inclua ideias que estão no texto e não modifique o que foi escrito, mesmo havendo erros ortográficos. Siga estritamente as regras descritas.\"\n",
    "        # system_prompt = \"Você é um médico especialista com conhecimento médico e clínico sobre medicina interna, principalmente sobre as doenças doença pulmonar obstrutiva crônica (DPOC). Além disso, você consegue diferenciar entre textos de especialistas sobre IAM e DPOC e textos de alunos. Seu objetivo é avaliar textos de alunos de medicina. Não crie informações que não sejam verdadeiras. SOMENTE responda no formato detalhado pelo usuário.\"\n",
    "\n",
    "    user_prompt = prompt_type[prompt_guide_level][prompt_pos].replace('{{question}}', question)\n",
    "    if prompt_guide_level == 'one-few' or prompt_guide_level == 'one-few-test':\n",
    "        example_shots = ''\n",
    "        if prompt_shot_amount > 0:\n",
    "            for i in range(prompt_shot_amount):\n",
    "                template = ideas_shot_template.replace('{{studentResponse}}', get_text_example(examples[i]['text'])).replace('{{annotationsResponse}}', get_annotation_example(examples[i]['annotations']))\n",
    "                example_shots += template\n",
    "            system_prompt = system_prompt_test.replace('{{example}}', example_shots)\n",
    "        else:\n",
    "            example_shots += ideas_shot_template.replace('{{studentResponse}}','').replace('{{annotationsResponse}}', '')\n",
    "            user_prompt = user_prompt.replace('{{example}}', '')\n",
    "    # elif prompt_type == 'organization':\n",
    "    #     example_shots += ideas_shot_template.replace('{{studentResponse}}','').replace('{{annotationsResponse}}', '')\n",
    "    #     user_prompt = user_prompt.replace('{{example}}', '')\n",
    "    # else:\n",
    "        # example_shots = ideas_shot_template.join(get_text_example(question))\n",
    "        # example_shots = ideas_shot_template.replace('{{studentResponse}}', get_text_example(question)).replace('Separação e classificação de ideias:\\n<<{{annotationsResponse}}>>', '')\n",
    "    # print(prompt_type[prompt_guide_level][prompt_pos])\n",
    "    # user_prompt = user_prompt.replace('\\\\n', '\\n')\n",
    "    if prompt_category == 'organization':\n",
    "        dialog = [\n",
    "            { \"role\": \"system\",\"content\": system_prompt_organization},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    elif sys_prompt_name == 'annotations_custom_1':\n",
    "        dialog = [\n",
    "            { \"role\": \"system\",\"content\": system_prompt_annotations_custom_1},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    elif prompt_category == 'custom':\n",
    "        dialog = [\n",
    "            { \"role\": \"system\",\"content\": sys_prompt_name},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    else:\n",
    "        dialog = [\n",
    "            { \"role\": \"system\",\"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    input_prompt = format_messages(dialog)\n",
    "    return input_prompt\n",
    "\n",
    "def set_custom_prompt(system_prompt, user_prompt):\n",
    "    dialog = [\n",
    "            { \"role\": \"system\",\"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    input_prompt = format_messages(dialog)\n",
    "    return input_prompt\n",
    "\n",
    "def prompt_routine(input_prompt, num_replicas, top_p,temp,top_k,max_new_tokens):\n",
    "    prompt_list = []\n",
    "    llama_config[\"top_p\"] = top_p\n",
    "    llama_config[\"temperature\"] = temp\n",
    "    llama_config[\"top_k\"] = top_k\n",
    "    payload = {\n",
    "    \"inputs\":  input_prompt,\n",
    "       \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\":max_new_tokens,\n",
    "        \"top_p\": llama_config[\"top_p\"],\n",
    "        \"temperature\": llama_config[\"temperature\"],\n",
    "        # \"repetition_penalty\":llama_config['repetition_penalty'],\n",
    "        \"top_k\": llama_config[\"top_k\"],\n",
    "        \"stop\": \"<|eot_id|>\"}\n",
    "    }\n",
    "    for i in range(num_replicas):\n",
    "        predictior_output = query_endpoint(payload)\n",
    "        # sys.stdout.write('\\r')\n",
    "        # # the exact output you're looking for:\n",
    "        # sys.stdout.write(\"[%-20s] %d%%\" % ('='*num_replicas, 5*num_replicas))\n",
    "        # sys.stdout.flush()\n",
    "        # sleep(0.25)\n",
    "        prompt_list.append({\"generated_text\":predictior_output[\"generated_text\"] ,\"input\":input_prompt})\n",
    "        \n",
    "    return prompt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78222b5e-e5f0-409f-b8db-b07ffd92fa07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:53:31.669540Z",
     "iopub.status.busy": "2024-07-01T13:53:31.668401Z",
     "iopub.status.idle": "2024-07-01T13:53:31.712708Z",
     "shell.execute_reply": "2024-07-01T13:53:31.707118Z",
     "shell.execute_reply.started": "2024-07-01T13:53:31.669492Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/'\n",
    "file_names = os.listdir(path)\n",
    "used_data = [\"f6cb9773-9a81-499c-85c7-3cebe935b930\",\"207c237f-3bcd-4010-bcf1-c1e8b32de6be\"]\n",
    "exisiting_ids = []\n",
    "for name in file_names:\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6') and doc_id not in used_data:\n",
    "        used_data.append(doc_id)\n",
    "        exisiting_ids.append(doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c8a1f-5885-4392-ae3b-0bfb0e5cbe3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:53:35.397595Z",
     "iopub.status.busy": "2024-07-01T13:53:35.397260Z",
     "iopub.status.idle": "2024-07-01T13:53:35.416712Z",
     "shell.execute_reply": "2024-07-01T13:53:35.415819Z",
     "shell.execute_reply.started": "2024-07-01T13:53:35.397567Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_new_examples (guided_lvl,prompt_category,shot_n):\n",
    "    random_annotated = random.choice(annotated_dataset)\n",
    "    while random_annotated['doc_id'] in used_data:\n",
    "        random_annotated = random.choice(annotated_dataset)\n",
    "    if prompt_category == 'organization':\n",
    "        #prompt_type,prompt_category,prompt_guide_level, prompt_pos, prompt_shot_amount,question\n",
    "            input_prompt = set_prompt(prompt_organization_lvl,'organization',guided_lvl, 0, shot_n,random_annotated['text'],'system_organization_lvl')\n",
    "    else:\n",
    "        input_prompt = set_prompt(prompt_ideas_full, guided_lvl, 0, shot_n,random_annotated['text'])\n",
    "    # prompt_category = 'ideas'\n",
    "    # guided_lvl = 'less'\n",
    "    output_batch = prompt_routine(input_prompt, 20,0.6,0.9,llama_config['top_k'],10)\n",
    "    dict_to_json = {\"doc_id\": random_annotated['doc_id'],\"text\": random_annotated['text'], \"response\": output_batch}\n",
    "    with open(f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}-{guided_lvl}-{random_annotated[\"doc_id\"]}.json', 'w') as file:\n",
    "        json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "    print('Complete',random_annotated['doc_id'])\n",
    "def run_examples_from_list (list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/'\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        requested_doc = get_from_annotated_dataset(_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_prompt(prompt_ideas_full, guided_lvl, 0, shot_n,requested_doc['text'])\n",
    "            output_batch = prompt_routine(input_prompt, 40,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'])\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'], \"response\": output_batch}\n",
    "            with open(f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def run_examples_from_list_all_guide_lvl (list_id,prompt_obj,guided_lvl_max,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}'\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        requested_doc = get_from_annotated_dataset(_id)\n",
    "        max_guided = False\n",
    "        for guided_lvl in prompt_obj:\n",
    "           \n",
    "            if max_guided == False:\n",
    "                for prompt_pos in range(len(prompt_obj[guided_lvl])):\n",
    "                    if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "                        input_prompt = set_prompt(prompt_obj, prompt_category, guided_lvl, prompt_pos, shot_n,requested_doc['text'])\n",
    "                        output_batch = prompt_routine(input_prompt,20,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],llama_config['max_new_tokens'])\n",
    "                        dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'], \"response\": output_batch}\n",
    "                        with open(f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                            json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "                        print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)*3}')\n",
    "                    else:\n",
    "                        print('Already exists',requested_doc['doc_id'])\n",
    "                    i += 1\n",
    "            if guided_lvl == guided_lvl_max:\n",
    "                max_guided = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a460ce08-c12c-4b40-8d5b-5ab6e0c0d7c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:53:47.088361Z",
     "iopub.status.busy": "2024-07-01T13:53:47.087575Z",
     "iopub.status.idle": "2024-07-01T13:53:47.095347Z",
     "shell.execute_reply": "2024-07-01T13:53:47.094017Z",
     "shell.execute_reply.started": "2024-07-01T13:53:47.088327Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\" RUN SINGLE TESTS \"\"\"\n",
    "random_annotated = random.choice(annotated_dataset)\n",
    "print(random_annotated['doc_id'])\n",
    "print(random_annotated['text'])\n",
    "\n",
    "input_prompt = set_prompt(prompt_organization_lvl,'organization','semi', 0, 0,random_annotated['text'],'system_organization_lvl')\n",
    "print(input_prompt)\n",
    "output_batch_grabber = []\n",
    "dict_to_json = {\"doc_id\": random_annotated['doc_id'],\"text\": random_annotated['text']}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a8a7f2-66d5-4577-9878-dbdede3fa5c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T17:19:20.072309Z",
     "iopub.status.busy": "2024-05-16T17:19:20.071220Z",
     "iopub.status.idle": "2024-05-16T17:19:20.082727Z",
     "shell.execute_reply": "2024-05-16T17:19:20.079905Z",
     "shell.execute_reply.started": "2024-05-16T17:19:20.072266Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "requested_case = get_from_annotated_dataset(annotated_dataset,'2e287874-1e15-4f2b-aa6d-3f167bef25fd')\n",
    "input_prompt = set_prompt(prompt_ideas_full,'ideas','semi', 0, 0,requested_case['text'],'annotations_custom_1')\n",
    "input_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54afc4d7-5838-40cd-89a9-d9a38c0e5445",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-16T17:22:43.589824Z",
     "iopub.status.busy": "2024-05-16T17:22:43.588999Z",
     "iopub.status.idle": "2024-05-16T17:22:52.363465Z",
     "shell.execute_reply": "2024-05-16T17:22:52.359350Z",
     "shell.execute_reply.started": "2024-05-16T17:22:43.589701Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_routine(input_prompt,1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4617f9ea-5588-4a10-8bc6-66717c258b11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-04-25T16:22:31.486784Z",
     "iopub.status.busy": "2024-04-25T16:22:31.485431Z",
     "iopub.status.idle": "2024-04-25T16:22:33.900907Z",
     "shell.execute_reply": "2024-04-25T16:22:33.899121Z",
     "shell.execute_reply.started": "2024-04-25T16:22:31.486619Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "random_annotated = random.choice(annotated_dataset)\n",
    "\n",
    "input_prompt = set_prompt(prompt_organization_lvl,'organization','semi', 0, 0,random_annotated['text'],'system_organization_lvl')\n",
    "prompt_category = 'organization'\n",
    "guided_lvl = 'semi'\n",
    "\n",
    "# requested_case = get_from_annotated_dataset(annotated_dataset,'dc680cf6-0bb8-4cd4-b090-36b51ea9ef2f')\n",
    "# print(requested_case)\n",
    "# input_prompt = set_prompt(prompt_organization_lvl,'organization','semi', 0, 0,requested_case['text'],'system_organization_lvl')\n",
    "\n",
    "# input_prompt = set_prompt(prompt_ideas_full, guided_lvl, 0, 0,random.choice(annotated_dataset)['text'])\n",
    "output_batch = prompt_routine(input_prompt,10,llama_config['top_p'],1,llama_config['top_k'],100)\n",
    "output_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d145cd8-87b8-454b-9ed2-a85aa5e00488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    run_new_examples('super','organization',0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ec6b9e-1280-4c1d-b714-4b47a28f2327",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Testing idea tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024c29f-abd3-43b2-921b-f29bb1920b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Annotating all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e6c119-0830-4b7c-b56a-817c48f55f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "annotated_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94fef9-b22e-48ed-9428-b994abe28589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T15:56:32.896137Z",
     "iopub.status.busy": "2024-05-06T15:56:32.894757Z",
     "iopub.status.idle": "2024-05-06T15:56:53.918436Z",
     "shell.execute_reply": "2024-05-06T15:56:53.917417Z",
     "shell.execute_reply.started": "2024-05-06T15:56:32.895759Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# requested_case = random.choice(annotated_dataset)\n",
    "requested_case = get_from_annotated_dataset(annotated_dataset,'3e663aa5-b913-4115-a0ad-38c8f3e24cae')\n",
    "#3e663aa5-b913-4115-a0ad-38c8f3e24cae\n",
    "print(requested_case,'\\n\\n')\n",
    "input_prompt = set_custom_prompt(system_ideas_full_bio,requested_case['text'])\n",
    "prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44f20a-1846-446d-81dd-d69aeeeabe9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T16:53:21.662559Z",
     "iopub.status.busy": "2024-05-07T16:53:21.661517Z",
     "iopub.status.idle": "2024-05-07T16:53:21.667459Z",
     "shell.execute_reply": "2024-05-07T16:53:21.666135Z",
     "shell.execute_reply.started": "2024-05-07T16:53:21.662521Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment 1 - One prompt for all Categories (Same texts as Gabriel's Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e382b-d716-4032-b2d9-4a76e29266d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:06:48.626656Z",
     "iopub.status.busy": "2024-05-21T19:06:48.625063Z",
     "iopub.status.idle": "2024-05-21T19:06:48.663134Z",
     "shell.execute_reply": "2024-05-21T19:06:48.662181Z",
     "shell.execute_reply.started": "2024-05-21T19:06:48.626525Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test_ids = list(pd.read_csv('test_data_info.csv')['doc_id'])\n",
    "test_ids_no_short = list(pd.read_csv('test_data_info_no_short.csv')['doc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc38866a-e607-43e8-99e5-a67a5133ef72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T20:08:05.968097Z",
     "iopub.status.busy": "2024-05-21T20:08:05.967252Z",
     "iopub.status.idle": "2024-05-21T20:08:05.973239Z",
     "shell.execute_reply": "2024-05-21T20:08:05.972291Z",
     "shell.execute_reply.started": "2024-05-21T20:08:05.968056Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "diff = list(set(test_ids) - set(test_ids_no_short))\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6c556-bfb8-4c38-a091-63a930c8fbc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:06:26.440609Z",
     "iopub.status.busy": "2024-05-21T19:06:26.437582Z",
     "iopub.status.idle": "2024-05-21T19:06:26.481841Z",
     "shell.execute_reply": "2024-05-21T19:06:26.480767Z",
     "shell.execute_reply.started": "2024-05-21T19:06:26.440559Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_annotation_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def run_annotation_processed_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dddae93-f551-4eb9-9aef-b745f22bbbc9",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1-shot (arbitrary examples) temp 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5728ff91-80ca-4bba-b5ab-6f2e7f57de92",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:07:14.497951Z",
     "iopub.status.busy": "2024-05-21T19:07:14.497500Z",
     "iopub.status.idle": "2024-05-21T19:11:57.624246Z",
     "shell.execute_reply": "2024-05-21T19:11:57.618578Z",
     "shell.execute_reply.started": "2024-05-21T19:07:14.497915Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_full_in_order,test_ids_no_short,'full','ideas',1)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3081929-342d-4098-b512-c728571095bd",
   "metadata": {},
   "source": [
    "#### 2-shot (arbitrary examples) temp 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039f07b0-5cba-4f82-a39b-8827b638340d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:11:57.639290Z",
     "iopub.status.busy": "2024-05-21T19:11:57.630505Z",
     "iopub.status.idle": "2024-05-21T19:16:50.583335Z",
     "shell.execute_reply": "2024-05-21T19:16:50.582373Z",
     "shell.execute_reply.started": "2024-05-21T19:11:57.639241Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_full_in_order_2_shot,test_ids_no_short,'full','ideas',2)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23292a83-3218-4da0-9902-b126ffcda9a4",
   "metadata": {},
   "source": [
    "#### 1-shot (arbitrary examples) temp 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1dfe04-abf8-4f56-8fe3-b1cb909c215c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:16:50.586715Z",
     "iopub.status.busy": "2024-05-21T19:16:50.585677Z",
     "iopub.status.idle": "2024-05-21T19:21:29.404519Z",
     "shell.execute_reply": "2024-05-21T19:21:29.403489Z",
     "shell.execute_reply.started": "2024-05-21T19:16:50.586636Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_full_in_order,test_ids_no_short,'full','ideas',1)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0450fc4c-da70-4399-85ba-1c05fcbc0a39",
   "metadata": {},
   "source": [
    "#### 2-shot (arbitrary examples) temp 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9540c1e-54e7-4554-86ad-a755699224f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:21:29.413529Z",
     "iopub.status.busy": "2024-05-21T19:21:29.412895Z",
     "iopub.status.idle": "2024-05-21T19:26:20.968700Z",
     "shell.execute_reply": "2024-05-21T19:26:20.960054Z",
     "shell.execute_reply.started": "2024-05-21T19:21:29.413493Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_full_in_order_2_shot,test_ids_no_short,'full','ideas',2)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f1ec3-6ebe-42a7-a0f1-6b0ded742755",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment 2 - One prompt for all Categories (Information Retrieval for best shot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3828fb2-ed20-425c-9a3a-2ddd7aa9ef87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:26:20.984805Z",
     "iopub.status.busy": "2024-05-21T19:26:20.980143Z",
     "iopub.status.idle": "2024-05-21T19:26:21.523795Z",
     "shell.execute_reply": "2024-05-21T19:26:21.522880Z",
     "shell.execute_reply.started": "2024-05-21T19:26:20.984745Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "df_data_info = pd.read_csv('test_data_info.csv')\n",
    "df_no_short_data_info = pd.read_csv('test_data_info_no_short.csv')\n",
    "def minimize_labels(label):\n",
    "    n_label = []\n",
    "    if type(label) == str:\n",
    "        label = eval(label)\n",
    "    for l in label:\n",
    "        if l[4] != None:\n",
    "            n_label.append([l[0],list(l[4].keys())])\n",
    "        else:\n",
    "            n_label.append([l[0],l[4]])\n",
    "    return n_label\n",
    "def get_only_text(label):\n",
    "    n_label = []\n",
    "    if type(label) == str:\n",
    "        label = eval(label)\n",
    "    for l in label:\n",
    "        n_label.append([l[0]])\n",
    "    return n_label\n",
    "\n",
    "def bio_to_cluster_annotation (bio_annotation):\n",
    "    main_annotation = []\n",
    "    for i in range(len(bio_annotation)):\n",
    "        list_ann = eval(bio_annotation[i])\n",
    "        \n",
    "        sub_annotation = []\n",
    "        phrase = []\n",
    "        for z in range(len(list_ann)):\n",
    "            if list_ann[z][3] == 'B':\n",
    "                if len(phrase) > 0:\n",
    "                    sub_annotation.append([' '.join(phrase),list(list_ann[z][4].keys())])\n",
    "                    phrase = []\n",
    "                phrase.append(list_ann[z][0])\n",
    "            elif list_ann[z][3] == 'I':\n",
    "                phrase.append(list_ann[z][0])\n",
    "        main_annotation.append(sub_annotation)\n",
    "    return main_annotation\n",
    "\n",
    "def extract_example_shot_from_row(_row, output='string'):\n",
    "    if output == 'list':\n",
    "        shot_text = {'user_input':_row['text'],'assistant_output':_row['cluster_labels']}\n",
    "    elif output == 'string':\n",
    "        shot_text = f\"\"\"'user_input':{_row['text']}\\n 'assistant_output':\"annotations\":{_row['cluster_labels']}\"\"\"\n",
    "    return shot_text\n",
    "\n",
    "## Retrieving similar examples from complete text (TF-IDF)\n",
    "def find_top_matches_from_text(query, df, top_n):\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the text data in the dataframe\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Transform the query string\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Calculate cosine similarity between the query vector and all documents\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "    # Get the indices of the top 3 matches\n",
    "    top_indices = similarity_scores.argsort()[0][-1*top_n:][::-1]\n",
    "    # Get the corresponding documents from the dataframe\n",
    "    top_matches = df.iloc[top_indices]\n",
    "    # print(type(top_matches))\n",
    "    \n",
    "    return top_matches\n",
    "\n",
    "## Retrieving similar examples from complete text (TF-IDF)\n",
    "def find_top_matches_from_annotation(query, df, top_n):\n",
    "    # for i in range(len(df['cluster_labels'])):\n",
    "    #     df['cluster_labels'][i] = minimize_labels(df['cluster_labels'][i])\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the text data in the dataframe\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['cluster_labels'])\n",
    "\n",
    "    # Transform the query string\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Calculate cosine similarity between the query vector and all documents\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "    # Get the indices of the top 3 matches\n",
    "    top_indices = similarity_scores.argsort()[0][-1*top_n:][::-1]\n",
    "    # Get the corresponding documents from the dataframe\n",
    "    top_matches = df.iloc[top_indices]\n",
    "    # print(type(top_matches))\n",
    "    \n",
    "    return top_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471e82cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "demo_df = pd.read_csv('test_data_info_no_short.csv')\n",
    "demo_txt = demo_df.iloc[0]\n",
    "def annotation_token_to_text(df_text):\n",
    "    annotation_token_to_text = ''\n",
    "    # for id,row in df_text.iterrows():\n",
    "        # print(row['cluster_labels'])\n",
    "        # print(eval(row['cluster_labels'])[0])\n",
    "        # print('============list_row==')\n",
    "    list_row = eval(df_text['cluster_labels'])\n",
    "    for i in range(len(list_row)):\n",
    "        annotation_token_to_text += list_row[i][0] + ' '\n",
    "    return annotation_token_to_text\n",
    "\n",
    "ata = find_top_matches_from_annotation(annotation_token_to_text(demo_df.iloc[0]),demo_df[demo_df.loc[:, 'text'] != demo_txt[\"text\"]],5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b25cb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(demo_df.iloc[0]['text'])\n",
    "print('=====================')\n",
    "print(ata['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe7c9d-647c-48fa-8c86-d5e084cd959c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:26:21.527067Z",
     "iopub.status.busy": "2024-05-21T19:26:21.526434Z",
     "iopub.status.idle": "2024-05-21T19:26:21.547297Z",
     "shell.execute_reply": "2024-05-21T19:26:21.546178Z",
     "shell.execute_reply.started": "2024-05-21T19:26:21.526953Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_annotations_tf_idf_shots (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-tf-idf-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            df_top_shots = find_top_matches_from_text(requested_doc[\"text\"],df_data_info[df_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "            shots_text = ''\n",
    "            for j in range(shot_n):\n",
    "                top_shot = df_top_shots.iloc[j]\n",
    "                # print('top_shot',top_shot)\n",
    "                txt = extract_example_shot_from_row(top_shot)\n",
    "                shots_text += '\\n'+txt\n",
    "            # print('bundle of shots', shots_text)\n",
    "            replace_sys_instruction = system_ideas_full_in_order_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "            # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "            input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "            report_path = path\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def predict_annotations_tf_idf_shots_from_preprocessed (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-tf-idf-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            df_top_shots = find_top_matches_from_text(requested_doc[\"text\"],df_no_short_data_info[df_no_short_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "            shots_text = ''\n",
    "            for j in range(shot_n):\n",
    "                top_shot = df_top_shots.iloc[j]\n",
    "                # print('top_shot',top_shot)\n",
    "                txt = extract_example_shot_from_row(top_shot)\n",
    "                shots_text += '\\n'+txt\n",
    "            # print('bundle of shots', shots_text)\n",
    "            replace_sys_instruction = system_ideas_full_in_order_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "            # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "            input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "            report_path = path\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da9a03-32dc-41b4-a3d5-c3ec3349a2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffa1bb-a481-435f-a040-8b7e1779a2ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:26:21.552217Z",
     "iopub.status.busy": "2024-05-21T19:26:21.551570Z",
     "iopub.status.idle": "2024-05-21T19:30:42.891614Z",
     "shell.execute_reply": "2024-05-21T19:30:42.886426Z",
     "shell.execute_reply.started": "2024-05-21T19:26:21.552181Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0f76f-e009-4dd7-a594-0989c10864bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:39:41.067226Z",
     "iopub.status.busy": "2024-05-21T19:39:41.066687Z",
     "iopub.status.idle": "2024-05-21T19:44:07.263674Z",
     "shell.execute_reply": "2024-05-21T19:44:07.261706Z",
     "shell.execute_reply.started": "2024-05-21T19:39:41.067190Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3d2173-8eef-48db-9853-697b9989209c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff351f9-c8db-44b5-852a-bce9c75071b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:30:42.896038Z",
     "iopub.status.busy": "2024-05-21T19:30:42.895657Z",
     "iopub.status.idle": "2024-05-21T19:35:09.263001Z",
     "shell.execute_reply": "2024-05-21T19:35:09.261691Z",
     "shell.execute_reply.started": "2024-05-21T19:30:42.895946Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e6ecf-2cfb-43de-9beb-b832d408231c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:44:07.272487Z",
     "iopub.status.busy": "2024-05-21T19:44:07.269333Z",
     "iopub.status.idle": "2024-05-21T19:48:36.109075Z",
     "shell.execute_reply": "2024-05-21T19:48:36.105704Z",
     "shell.execute_reply.started": "2024-05-21T19:44:07.272425Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af528022-b5be-4657-9543-c82b09e645a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639e664-41ce-41ff-b592-9db8e795d626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:35:09.266398Z",
     "iopub.status.busy": "2024-05-21T19:35:09.265579Z",
     "iopub.status.idle": "2024-05-21T19:39:41.063297Z",
     "shell.execute_reply": "2024-05-21T19:39:41.062393Z",
     "shell.execute_reply.started": "2024-05-21T19:35:09.266348Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10662be8-cc9a-4ac1-8c8a-ad244953174a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:48:36.114073Z",
     "iopub.status.busy": "2024-05-21T19:48:36.113279Z",
     "iopub.status.idle": "2024-05-21T19:53:12.408260Z",
     "shell.execute_reply": "2024-05-21T19:53:12.405105Z",
     "shell.execute_reply.started": "2024-05-21T19:48:36.114021Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513852be-d7b8-446b-a618-778ffab9e78e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9065a-b6f1-43ea-8bee-5f99f9100e78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:53:12.411840Z",
     "iopub.status.busy": "2024-05-21T19:53:12.410353Z",
     "iopub.status.idle": "2024-05-21T19:57:48.570406Z",
     "shell.execute_reply": "2024-05-21T19:57:48.568477Z",
     "shell.execute_reply.started": "2024-05-21T19:53:12.411787Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e30f16-4abb-4760-960c-ef6a90268f69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T19:57:48.573199Z",
     "iopub.status.busy": "2024-05-21T19:57:48.572798Z",
     "iopub.status.idle": "2024-05-21T20:02:31.650657Z",
     "shell.execute_reply": "2024-05-21T20:02:31.648996Z",
     "shell.execute_reply.started": "2024-05-21T19:57:48.573154Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6ae66-8245-4bc5-907f-166eeeaed990",
   "metadata": {},
   "source": [
    "### Experiment 3 - Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9683345e-6950-4584-a415-59d8f58510e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:53:59.095483Z",
     "iopub.status.busy": "2024-07-01T13:53:59.094830Z",
     "iopub.status.idle": "2024-07-01T13:53:59.168444Z",
     "shell.execute_reply": "2024-07-01T13:53:59.166720Z",
     "shell.execute_reply.started": "2024-07-01T13:53:59.095448Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test_ids = list(pd.read_csv('test_data_info.csv')['doc_id'])\n",
    "test_ids_no_short = list(pd.read_csv('test_data_info_no_short.csv')['doc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c71261-de0a-4463-a82d-8afca82d7557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:53:59.783429Z",
     "iopub.status.busy": "2024-07-01T13:53:59.781539Z",
     "iopub.status.idle": "2024-07-01T13:53:59.800782Z",
     "shell.execute_reply": "2024-07-01T13:53:59.799723Z",
     "shell.execute_reply.started": "2024-07-01T13:53:59.783389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_annotation_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def run_annotation_processed_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f407b76-fc56-4ff8-a22b-b619f1a54bc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### zero shot temp 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751a72f-7089-412a-ae84-536c4767f290",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T13:54:02.040915Z",
     "iopub.status.busy": "2024-07-01T13:54:02.039974Z",
     "iopub.status.idle": "2024-07-01T13:58:43.208456Z",
     "shell.execute_reply": "2024-07-01T13:58:43.207301Z",
     "shell.execute_reply.started": "2024-07-01T13:54:02.040872Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_full_in_order_zero_shot,test_ids_no_short,'full','ideas',0)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf378d-970d-4669-8ee7-5a9494041580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-01T17:32:24.630313Z",
     "iopub.status.busy": "2024-07-01T17:32:24.628981Z",
     "iopub.status.idle": "2024-07-01T17:32:30.654801Z",
     "shell.execute_reply": "2024-07-01T17:32:30.653887Z",
     "shell.execute_reply.started": "2024-07-01T17:32:24.630273Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "make_tarfile('llama_outputs.tar.gz','llama-outputs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59a845-3029-45a8-b271-6a55dae88613",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Delete endpoint (stop billing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2f16d-7638-4807-9b60-68f511efd03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T20:02:31.654251Z",
     "iopub.status.busy": "2024-05-21T20:02:31.653133Z",
     "iopub.status.idle": "2024-05-21T20:02:32.203764Z",
     "shell.execute_reply": "2024-05-21T20:02:32.202995Z",
     "shell.execute_reply.started": "2024-05-21T20:02:31.654201Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify your AWS Region\n",
    "aws_region='us-east-1'\n",
    "\n",
    "# Create a low-level SageMaker service client.\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
    "\n",
    "# Delete endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5e535e-ec24-40c7-bee6-1b1dbc79a303",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
