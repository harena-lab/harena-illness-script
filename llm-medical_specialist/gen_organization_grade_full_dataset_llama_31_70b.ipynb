{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f82d9ea",
   "metadata": {},
   "source": [
    "# SageMaker JumpStart - invoke text generation endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c51bc6",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to attach a predictor to an existing endpoint name and invoke the endpoint with example payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94f18dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-22T20:38:43.441817Z",
     "iopub.status.busy": "2024-09-22T20:38:43.440829Z",
     "iopub.status.idle": "2024-09-22T20:39:03.844885Z",
     "shell.execute_reply": "2024-09-22T20:39:03.843382Z",
     "shell.execute_reply.started": "2024-09-22T20:38:43.441782Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U sagemaker\n",
    "%pip install jsonlines\n",
    "%pip install tdqm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6267543",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T15:26:22.019566Z",
     "iopub.status.busy": "2024-09-09T15:26:22.019177Z",
     "iopub.status.idle": "2024-09-09T15:26:23.595801Z",
     "shell.execute_reply": "2024-09-09T15:26:23.594618Z",
     "shell.execute_reply.started": "2024-09-09T15:26:22.019535Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.predictor import retrieve_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98dc72a",
   "metadata": {},
   "source": [
    "Retrieve a predictor from your deployed endpoint name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc092a63",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2024-09-09T20:05:05.737691Z",
     "iopub.status.busy": "2024-09-09T20:05:05.737211Z",
     "iopub.status.idle": "2024-09-09T20:05:06.109307Z",
     "shell.execute_reply": "2024-09-09T20:05:06.108270Z",
     "shell.execute_reply.started": "2024-09-09T20:05:05.737660Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = \"jumpstart-dft-llama-3-1-70b-instruct\"\n",
    "predictor = retrieve_default(endpoint_name)\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "from botocore.config import Config\n",
    "\n",
    "# endpoint_name = \"jumpstart-dft-meta-textgeneration-llama-3-70b-instruct\"\n",
    "\n",
    "\n",
    "def query_endpoint(payload):\n",
    "    config = Config(\n",
    "    read_timeout=900,\n",
    "    connect_timeout=900,\n",
    "    tcp_keepalive=True,\n",
    "    retries={\"max_attempts\": 0})\n",
    "\n",
    "    client = boto3.client(\"sagemaker-runtime\",config=config)\n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"application/json\",\n",
    "        Body=json.dumps(payload),\n",
    "    )\n",
    "    response = response[\"Body\"].read().decode(\"utf8\")\n",
    "    response = json.loads(response)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a1130d-1026-43fa-bfa1-c698cf79cf63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Medical Specialist - Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01410659-6b63-4060-a579-1d2fb40d3d64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T20:05:06.434035Z",
     "iopub.status.busy": "2024-09-09T20:05:06.433375Z",
     "iopub.status.idle": "2024-09-09T20:05:06.445383Z",
     "shell.execute_reply": "2024-09-09T20:05:06.444138Z",
     "shell.execute_reply.started": "2024-09-09T20:05:06.433999Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format messages for Llama-3 chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    prompt: List[str] = []\n",
    "    # print(messages[0]['role'])\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        content = \"\".join([\"<|start_header_id|>system<|end_header_id|>\\n\\n\", messages[0][\"content\"], \"<|eot_id|>\", \"<|start_header_id|>user<|end_header_id|>\\n\\n\",messages[1][\"content\"],\"<|eot_id|>\"])\n",
    "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
    "\n",
    "    for user, answer in zip(messages[::2], messages[1::2]):\n",
    "        prompt.extend([\"<|start_header_id|>user<|end_header_id|>\", \"\\n\\n\", (user[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "\n",
    "    prompt.extend([\"<|begin_of_text|>\", (messages[0][\"content\"]).strip(), \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"])\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "llama_config = {\"top_p\": 0.6,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 30,\n",
    "    \"max_new_tokens\": 8192\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1ad602-5c13-4a62-9e93-47d2ac0479f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T20:05:07.204083Z",
     "iopub.status.busy": "2024-09-09T20:05:07.203136Z",
     "iopub.status.idle": "2024-09-09T20:05:07.226907Z",
     "shell.execute_reply": "2024-09-09T20:05:07.225401Z",
     "shell.execute_reply.started": "2024-09-09T20:05:07.204047Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "def get_static_shots():\n",
    "    file_path_example = 'static_examples.jsonl'\n",
    "    with jsonlines.open(file_path_example) as reader:\n",
    "        data = [line for line in reader]\n",
    "    return data\n",
    "\n",
    "static_shots = get_static_shots()\n",
    "print(static_shots[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487de77a-896b-48ce-8e6b-723c6e667532",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:17:34.454141Z",
     "iopub.status.busy": "2024-09-12T19:17:34.452835Z",
     "iopub.status.idle": "2024-09-12T19:17:34.535048Z",
     "shell.execute_reply": "2024-09-12T19:17:34.533984Z",
     "shell.execute_reply.started": "2024-09-12T19:17:34.454103Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jsonlines\n",
    "import pandas as pd\n",
    "def get_text_example(text):\n",
    "    return str({\"text\":text})\n",
    "def get_annotation_example(text):\n",
    "    return str({\"annotation\":text})\n",
    "def get_examples(file_path):\n",
    "    full_df =  pd.read_csv(file_path)\n",
    "    filtered_df = full_df[[\"annotation id\", \"year or semester\", \"temporality\", \"objective test score\", \"organization level\", \"global score\", \"student year\"]]\n",
    "    filtered_df.columns = [\"doc_id\", \"year_semester\", \"temporality\", \"objective_test_score\", \"organization_lvl\", \"global_score\", \"predict_student_year\"]\n",
    "    return filtered_df\n",
    "        \n",
    "\n",
    "def get_from_annotated_dataset(annotated_dataset,_id):\n",
    "    for doc in annotated_dataset:\n",
    "        if doc['doc_id'] == _id:\n",
    "            return doc\n",
    "\n",
    "system_organization_lvl_zero_shot_no_detail_no_explanation = 'You are a medical assistant with expertise in evaluating the organization level of manuscripts from medical students. The scores range from 1 to 4, with 1 being the lowest score and 4 being the heighest score. Answer only with \"Org: X\", where X is the score for the organization level. ONLY answer in portuguese.'\n",
    "system_organization_lvl_zero_shot_no_detail_with_explanation = 'You are a medical assistant with expertise in evaluating the organization level of manuscripts from medical students. The scores range from 1 to 4, with 1 being the lowest score and 4 being the heighest score. Think step by step and write the score at the end as \"Org: X\", where X is the score for the organization level. ONLY answer in portuguese.'\n",
    "system_organization_lvl_zero_shot_with_detail_no_exaplanation = 'You are a medical assistant with expertise in evaluating the organization level of manuscripts from medical students. The scores range from 1 to 4, with 1 being the lowest score and 4 being the heighest score. Organization score refers to how the student develops their ideas. These ideas refer to some of these dimensions: pathophysiology, etiology, epidemiology, history, physical examination, complementary tests, differential diagnosis and treatment. Answer only with \"Org: X\", where X is the score for the organization level. ONLY answer in portuguese.'\n",
    "system_organization_lvl_zero_shot_with_detail_with_explanation = 'You are a medical assistant with expertise in evaluating the organization level of manuscripts from medical students. The scores range from 1 to 4, with 1 being the lowest score and 4 being the heighest score. Think step by step and write the score at the end as \"Org: X\", where X is the score for the organization level. ONLY answer in portuguese.'\n",
    "\n",
    "# system_ideas_full_in_order_shot_dynamic = \"\"\"You are a medical assistant with expertise in medical document processing.\\n Your task is to tag entities related to these classes ONLY: pathophysiology, etiology, epidemiology, history, physical, exams, differential e therapeutic. Each line must include: (1) word or phrase, (2) class or classes that the text is a part of. Maintain the correct format. Example: ['token', ['physical']]. All the texts will be in portuguese. ONLY use JSON as the output format, starting with 'annotations'. DO NOT write, only respond in JSON format. Examples of user input and assistant output:\\n {{shot}}\"\"\"\n",
    "\n",
    "# annotated_dataset = get_examples('teste-progresso/annotations-medical_specialist-dpoc-json.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a713e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = get_examples('teste-progresso/resultados_anotacoes_teste_progresso_dpoc.csv')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2465d6-2ea4-4989-b2c4-fdefd24cb8e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:17:36.692897Z",
     "iopub.status.busy": "2024-09-12T19:17:36.692455Z",
     "iopub.status.idle": "2024-09-12T19:17:36.731240Z",
     "shell.execute_reply": "2024-09-12T19:17:36.727483Z",
     "shell.execute_reply.started": "2024-09-12T19:17:36.692862Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "from time import sleep\n",
    "import sys\n",
    "\n",
    "def set_custom_prompt(system_prompt, user_prompt):\n",
    "    dialog = [\n",
    "            { \"role\": \"system\",\"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    input_prompt = format_messages(dialog)\n",
    "    return input_prompt\n",
    "\n",
    "def prompt_routine(input_prompt, num_replicas, top_p,temp,top_k,max_new_tokens):\n",
    "    prompt_list = []\n",
    "    llama_config[\"top_p\"] = top_p\n",
    "    llama_config[\"temperature\"] = temp\n",
    "    llama_config[\"top_k\"] = top_k\n",
    "    payload = {\n",
    "    \"inputs\":  input_prompt,\n",
    "       \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"max_new_tokens\":max_new_tokens,\n",
    "        \"top_p\": llama_config[\"top_p\"],\n",
    "        \"temperature\": llama_config[\"temperature\"],\n",
    "        # \"repetition_penalty\":llama_config['repetition_penalty'],\n",
    "        \"top_k\": llama_config[\"top_k\"],\n",
    "        \"stop\": \"<|eot_id|>\"}\n",
    "    }\n",
    "    for i in range(num_replicas):\n",
    "        predictior_output = query_endpoint(payload)\n",
    "        # sys.stdout.write('\\r')\n",
    "        # # the exact output you're looking for:\n",
    "        # sys.stdout.write(\"[%-20s] %d%%\" % ('='*num_replicas, 5*num_replicas))\n",
    "        # sys.stdout.flush()\n",
    "        # sleep(0.25)\n",
    "        prompt_list.append({\"generated_text\":predictior_output[\"generated_text\"] ,\"input\":input_prompt})\n",
    "        \n",
    "    return prompt_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78222b5e-e5f0-409f-b8db-b07ffd92fa07",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:17:38.028188Z",
     "iopub.status.busy": "2024-09-12T19:17:38.026192Z",
     "iopub.status.idle": "2024-09-12T19:17:38.057940Z",
     "shell.execute_reply": "2024-09-12T19:17:38.051649Z",
     "shell.execute_reply.started": "2024-09-12T19:17:38.028137Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/'\n",
    "file_names = os.listdir(path)\n",
    "used_data = [\"f6cb9773-9a81-499c-85c7-3cebe935b930\",\"207c237f-3bcd-4010-bcf1-c1e8b32de6be\"]\n",
    "exisiting_ids = []\n",
    "for name in file_names:\n",
    "    doc_id = name[-41:-5]\n",
    "    if len(doc_id) == len('f98e69ee-fda6-4b1c-a8a9-c20b92630cb6') and doc_id not in used_data:\n",
    "        used_data.append(doc_id)\n",
    "        exisiting_ids.append(doc_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941c8a1f-5885-4392-ae3b-0bfb0e5cbe3b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:17:38.319252Z",
     "iopub.status.busy": "2024-09-12T19:17:38.318618Z",
     "iopub.status.idle": "2024-09-12T19:17:38.364556Z",
     "shell.execute_reply": "2024-09-12T19:17:38.360764Z",
     "shell.execute_reply.started": "2024-09-12T19:17:38.319218Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def run_new_examples (guided_lvl,prompt_category,shot_n):\n",
    "    random_annotated = random.choice(annotated_dataset)\n",
    "    while random_annotated['doc_id'] in used_data:\n",
    "        random_annotated = random.choice(annotated_dataset)\n",
    "    if prompt_category == 'organization':\n",
    "        #prompt_type,prompt_category,prompt_guide_level, prompt_pos, prompt_shot_amount,question\n",
    "            input_prompt = set_prompt(prompt_organization_lvl,'organization',guided_lvl, 0, shot_n,random_annotated['text'],'system_organization_lvl')\n",
    "    else:\n",
    "        input_prompt = set_prompt(prompt_ideas_full, guided_lvl, 0, shot_n,random_annotated['text'])\n",
    "    # prompt_category = 'ideas'\n",
    "    # guided_lvl = 'less'\n",
    "    output_batch = prompt_routine(input_prompt, 20,0.6,0.9,llama_config['top_k'],10)\n",
    "    dict_to_json = {\"doc_id\": random_annotated['doc_id'],\"text\": random_annotated['text'], \"response\": output_batch}\n",
    "    with open(f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}-{guided_lvl}-{random_annotated[\"doc_id\"]}.json', 'w') as file:\n",
    "        json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "    print('Complete',random_annotated['doc_id'])\n",
    "def run_examples_from_list (list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/'\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        requested_doc = get_from_annotated_dataset(_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_prompt(prompt_ideas_full, guided_lvl, 0, shot_n,requested_doc['text'])\n",
    "            output_batch = prompt_routine(input_prompt, 40,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'])\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'], \"response\": output_batch}\n",
    "            with open(f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def run_examples_from_list_all_guide_lvl (list_id,prompt_obj,guided_lvl_max,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}'\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        requested_doc = get_from_annotated_dataset(_id)\n",
    "        max_guided = False\n",
    "        for guided_lvl in prompt_obj:\n",
    "           \n",
    "            if max_guided == False:\n",
    "                for prompt_pos in range(len(prompt_obj[guided_lvl])):\n",
    "                    if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "                        input_prompt = set_prompt(prompt_obj, prompt_category, guided_lvl, prompt_pos, shot_n,requested_doc['text'])\n",
    "                        output_batch = prompt_routine(input_prompt,20,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],llama_config['max_new_tokens'])\n",
    "                        dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'], \"response\": output_batch}\n",
    "                        with open(f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/{prompt_category}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                            json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "                        print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)*3}')\n",
    "                    else:\n",
    "                        print('Already exists',requested_doc['doc_id'])\n",
    "                    i += 1\n",
    "            if guided_lvl == guided_lvl_max:\n",
    "                max_guided = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f024c29f-abd3-43b2-921b-f29bb1920b8f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Running Singular text - Overall Organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f94fef9-b22e-48ed-9428-b994abe28589",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T20:07:07.645004Z",
     "iopub.status.busy": "2024-09-09T20:07:07.643398Z",
     "iopub.status.idle": "2024-09-09T20:07:26.482756Z",
     "shell.execute_reply": "2024-09-09T20:07:26.478186Z",
     "shell.execute_reply.started": "2024-09-09T20:07:07.644950Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# requested_case = random.choice(annotated_dataset)\n",
    "requested_case = get_from_annotated_dataset(annotated_dataset,'3e663aa5-b913-4115-a0ad-38c8f3e24cae')\n",
    "#3e663aa5-b913-4115-a0ad-38c8f3e24cae\n",
    "print(requested_case,'\\n\\n')\n",
    "input_prompt = set_custom_prompt(system_ideas_full_bio,requested_case['text'])\n",
    "prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c44f20a-1846-446d-81dd-d69aeeeabe9f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-07T16:53:21.662559Z",
     "iopub.status.busy": "2024-05-07T16:53:21.661517Z",
     "iopub.status.idle": "2024-05-07T16:53:21.667459Z",
     "shell.execute_reply": "2024-05-07T16:53:21.666135Z",
     "shell.execute_reply.started": "2024-05-07T16:53:21.662521Z"
    },
    "tags": []
   },
   "source": [
    "### Experiment 1 - Overall Organization - Zero-Shot without detailing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625e382b-d716-4032-b2d9-4a76e29266d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T20:07:26.487169Z",
     "iopub.status.busy": "2024-09-09T20:07:26.486348Z",
     "iopub.status.idle": "2024-09-09T20:07:26.612945Z",
     "shell.execute_reply": "2024-09-09T20:07:26.608313Z",
     "shell.execute_reply.started": "2024-09-09T20:07:26.487122Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# test_ids = list(pd.read_csv('test_data_info.csv')['doc_id'])\n",
    "test_ids_no_short = list(pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')['doc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6c556-bfb8-4c38-a091-63a930c8fbc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:17:41.460188Z",
     "iopub.status.busy": "2024-09-12T19:17:41.459463Z",
     "iopub.status.idle": "2024-09-12T19:17:41.487530Z",
     "shell.execute_reply": "2024-09-12T19:17:41.484507Z",
     "shell.execute_reply.started": "2024-09-12T19:17:41.460145Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_organization_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def run_annotation_processed_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d243ab0-945e-486d-8d8b-094d2dd61f4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### zero shot temp 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedab20e-0b7c-4686-916f-c3a29696e30e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T02:03:19.973144Z",
     "iopub.status.busy": "2024-09-10T02:03:19.972306Z",
     "iopub.status.idle": "2024-09-10T02:32:16.748417Z",
     "shell.execute_reply": "2024-09-10T02:32:16.746327Z",
     "shell.execute_reply.started": "2024-09-10T02:03:19.973079Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "run_annotation_examples_from_list(annotated_dataset,system_ideas_full_in_order_zero_shot,test_ids_no_short,'full','organization',0)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54f1ec3-6ebe-42a7-a0f1-6b0ded742755",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment 2 - Overall Organization - Zero-Shot with detailing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3828fb2-ed20-425c-9a3a-2ddd7aa9ef87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T22:08:31.043752Z",
     "iopub.status.busy": "2024-09-09T22:08:31.043269Z",
     "iopub.status.idle": "2024-09-09T22:08:31.170511Z",
     "shell.execute_reply": "2024-09-09T22:08:31.169698Z",
     "shell.execute_reply.started": "2024-09-09T22:08:31.043708Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "df_data_info = pd.read_csv('test_data_info.csv')\n",
    "df_no_short_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "def minimize_labels(label):\n",
    "    n_label = []\n",
    "    if type(label) == str:\n",
    "        label = eval(label)\n",
    "    for l in label:\n",
    "        if l[4] != None:\n",
    "            n_label.append([l[0],list(l[4].keys())])\n",
    "        else:\n",
    "            n_label.append([l[0],l[4]])\n",
    "    return n_label\n",
    "def get_only_text(label):\n",
    "    n_label = []\n",
    "    if type(label) == str:\n",
    "        label = eval(label)\n",
    "    for l in label:\n",
    "        n_label.append([l[0]])\n",
    "    return n_label\n",
    "\n",
    "def bio_to_cluster_annotation (bio_annotation):\n",
    "    main_annotation = []\n",
    "    for i in range(len(bio_annotation)):\n",
    "        list_ann = eval(bio_annotation[i])\n",
    "        \n",
    "        sub_annotation = []\n",
    "        phrase = []\n",
    "        for z in range(len(list_ann)):\n",
    "            if list_ann[z][3] == 'B':\n",
    "                if len(phrase) > 0:\n",
    "                    sub_annotation.append([' '.join(phrase),list(list_ann[z][4].keys())])\n",
    "                    phrase = []\n",
    "                phrase.append(list_ann[z][0])\n",
    "            elif list_ann[z][3] == 'I':\n",
    "                phrase.append(list_ann[z][0])\n",
    "        main_annotation.append(sub_annotation)\n",
    "    return main_annotation\n",
    "\n",
    "def extract_example_shot_from_row(_row, output='string'):\n",
    "    if output == 'list':\n",
    "        shot_text = {'user_input':_row['text'],'assistant_output':_row['cluster_labels']}\n",
    "    elif output == 'string':\n",
    "        shot_text = f\"\"\"'user_input':{_row['text']}\\n 'assistant_output':\"annotations\":{_row['cluster_labels']}\"\"\"\n",
    "    return shot_text\n",
    "\n",
    "def find_top_matches(query, df, top_n):\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the text data in the dataframe\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "    # Transform the query string\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Calculate cosine similarity between the query vector and all documents\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "    # Get the indices of the top 3 matches\n",
    "    top_indices = similarity_scores.argsort()[0][-1*top_n:][::-1]\n",
    "    # Get the corresponding documents from the dataframe\n",
    "    top_matches = df.iloc[top_indices]\n",
    "    # print(type(top_matches))\n",
    "    \n",
    "    return top_matches\n",
    "\n",
    "## Transform list of annotated tokens into continuous string for TF-IDF\n",
    "def annotation_token_to_text(df_text):\n",
    "    annotation_token_to_text = ''\n",
    "    # for id,row in df_text.iterrows():\n",
    "        # print(row['cluster_labels'])\n",
    "        # print(eval(row['cluster_labels'])[0])\n",
    "        # print('============list_row==')\n",
    "    list_row = eval(df_text['cluster_labels'])\n",
    "    for i in range(len(list_row)):\n",
    "        annotation_token_to_text += list_row[i][0] + ' '\n",
    "    return annotation_token_to_text\n",
    "\n",
    "## Retrieving similar examples from complete text (TF-IDF)\n",
    "def find_top_matches_from_annotation(query, df, top_n):\n",
    "    # for i in range(len(df['cluster_labels'])):\n",
    "    #     df['cluster_labels'][i] = minimize_labels(df['cluster_labels'][i])\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the text data in the dataframe\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['cluster_labels'])\n",
    "\n",
    "    # Transform the query string\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Calculate cosine similarity between the query vector and all documents\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "    # Get the indices of the top 3 matches\n",
    "    top_indices = similarity_scores.argsort()[0][-1*top_n:][::-1]\n",
    "    # Get the corresponding documents from the dataframe\n",
    "    top_matches = df.iloc[top_indices]\n",
    "    # print(type(top_matches))\n",
    "    \n",
    "    return top_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfe7c9d-647c-48fa-8c86-d5e084cd959c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T22:08:31.175419Z",
     "iopub.status.busy": "2024-09-09T22:08:31.175033Z",
     "iopub.status.idle": "2024-09-09T22:08:31.192448Z",
     "shell.execute_reply": "2024-09-09T22:08:31.191164Z",
     "shell.execute_reply.started": "2024-09-09T22:08:31.175389Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_annotations_tf_idf_shots (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-tf-idf-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            df_top_shots = find_top_matches(requested_doc[\"text\"],df_data_info[df_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "            shots_text = ''\n",
    "            for j in range(shot_n):\n",
    "                top_shot = df_top_shots.iloc[j]\n",
    "                # print('top_shot',top_shot)\n",
    "                txt = extract_example_shot_from_row(top_shot)\n",
    "                shots_text += '\\n'+txt\n",
    "            # print('bundle of shots', shots_text)\n",
    "            replace_sys_instruction = system_ideas_full_in_order_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "            # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "            input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "            report_path = path\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def predict_annotations_tf_idf_shots_from_preprocessed (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-tf-idf-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            df_top_shots = find_top_matches(requested_doc[\"text\"],df_no_short_data_info[df_no_short_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "            shots_text = ''\n",
    "            for j in range(shot_n):\n",
    "                top_shot = df_top_shots.iloc[j]\n",
    "                # print('top_shot',top_shot)\n",
    "                txt = extract_example_shot_from_row(top_shot)\n",
    "                shots_text += '\\n'+txt\n",
    "            # print('bundle of shots', shots_text)\n",
    "            replace_sys_instruction = system_ideas_full_in_order_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "            # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "            input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "            report_path = path\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8da9a03-32dc-41b4-a3d5-c3ec3349a2d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ffa1bb-a481-435f-a040-8b7e1779a2ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T22:08:31.196157Z",
     "iopub.status.busy": "2024-09-09T22:08:31.194314Z",
     "iopub.status.idle": "2024-09-09T22:36:44.128895Z",
     "shell.execute_reply": "2024-09-09T22:36:44.127639Z",
     "shell.execute_reply.started": "2024-09-09T22:08:31.196121Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0f76f-e009-4dd7-a594-0989c10864bd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T22:36:44.133052Z",
     "iopub.status.busy": "2024-09-09T22:36:44.132552Z",
     "iopub.status.idle": "2024-09-09T23:05:23.700592Z",
     "shell.execute_reply": "2024-09-09T23:05:23.698503Z",
     "shell.execute_reply.started": "2024-09-09T22:36:44.133018Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3d2173-8eef-48db-9853-697b9989209c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff351f9-c8db-44b5-852a-bce9c75071b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T23:05:23.704743Z",
     "iopub.status.busy": "2024-09-09T23:05:23.704236Z",
     "iopub.status.idle": "2024-09-09T23:34:04.336519Z",
     "shell.execute_reply": "2024-09-09T23:34:04.335297Z",
     "shell.execute_reply.started": "2024-09-09T23:05:23.704697Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e6ecf-2cfb-43de-9beb-b832d408231c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-09T23:34:04.338462Z",
     "iopub.status.busy": "2024-09-09T23:34:04.337903Z",
     "iopub.status.idle": "2024-09-10T00:03:12.158922Z",
     "shell.execute_reply": "2024-09-10T00:03:12.157648Z",
     "shell.execute_reply.started": "2024-09-09T23:34:04.338418Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af528022-b5be-4657-9543-c82b09e645a3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2639e664-41ce-41ff-b592-9db8e795d626",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T00:03:12.164447Z",
     "iopub.status.busy": "2024-09-10T00:03:12.163175Z",
     "iopub.status.idle": "2024-09-10T00:32:46.628093Z",
     "shell.execute_reply": "2024-09-10T00:32:46.626930Z",
     "shell.execute_reply.started": "2024-09-10T00:03:12.164411Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10662be8-cc9a-4ac1-8c8a-ad244953174a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T00:32:46.630959Z",
     "iopub.status.busy": "2024-09-10T00:32:46.630432Z",
     "iopub.status.idle": "2024-09-10T01:02:43.499995Z",
     "shell.execute_reply": "2024-09-10T01:02:43.498874Z",
     "shell.execute_reply.started": "2024-09-10T00:32:46.630913Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513852be-d7b8-446b-a618-778ffab9e78e",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a9065a-b6f1-43ea-8bee-5f99f9100e78",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T01:02:43.502515Z",
     "iopub.status.busy": "2024-09-10T01:02:43.502100Z",
     "iopub.status.idle": "2024-09-10T01:32:49.707723Z",
     "shell.execute_reply": "2024-09-10T01:32:49.706725Z",
     "shell.execute_reply.started": "2024-09-10T01:02:43.502472Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e30f16-4abb-4760-960c-ef6a90268f69",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T01:32:49.710212Z",
     "iopub.status.busy": "2024-09-10T01:32:49.709336Z",
     "iopub.status.idle": "2024-09-10T02:03:19.834171Z",
     "shell.execute_reply": "2024-09-10T02:03:19.831770Z",
     "shell.execute_reply.started": "2024-09-10T01:32:49.710167Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c98480a-1695-4bb8-837a-c991c9aab698",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 10-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607664e7-c38e-41ba-ad95-1b7ffb723584",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:03:22.277015Z",
     "iopub.status.busy": "2024-09-13T15:03:22.276164Z",
     "iopub.status.idle": "2024-09-13T15:36:40.852369Z",
     "shell.execute_reply": "2024-09-13T15:36:40.851161Z",
     "shell.execute_reply.started": "2024-09-13T15:03:22.276965Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8025c7-d11a-4b86-9471-94cb83d62f8c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T15:36:40.855908Z",
     "iopub.status.busy": "2024-09-13T15:36:40.855261Z",
     "iopub.status.idle": "2024-09-13T16:10:28.068840Z",
     "shell.execute_reply": "2024-09-13T16:10:28.067627Z",
     "shell.execute_reply.started": "2024-09-13T15:36:40.855870Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c6ae66-8245-4bc5-907f-166eeeaed990",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment 3 - Overall Organization - Few-shot without detailing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9683345e-6950-4584-a415-59d8f58510e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T02:03:19.842456Z",
     "iopub.status.busy": "2024-09-10T02:03:19.841761Z",
     "iopub.status.idle": "2024-09-10T02:03:19.948490Z",
     "shell.execute_reply": "2024-09-10T02:03:19.947010Z",
     "shell.execute_reply.started": "2024-09-10T02:03:19.842407Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "test_ids = list(pd.read_csv('test_data_info.csv')['doc_id'])\n",
    "test_ids_no_short = list(pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')['doc_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c71261-de0a-4463-a82d-8afca82d7557",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T02:03:19.950970Z",
     "iopub.status.busy": "2024-09-10T02:03:19.950534Z",
     "iopub.status.idle": "2024-09-10T02:03:19.970804Z",
     "shell.execute_reply": "2024-09-10T02:03:19.969943Z",
     "shell.execute_reply.started": "2024-09-10T02:03:19.950909Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_annotation_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1\n",
    "\n",
    "def run_annotation_processed_examples_from_list (full_dataset,sys_prompt,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            input_prompt = set_custom_prompt(sys_prompt,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'] ,\"response\": output_batch[0]['generated_text']}\n",
    "            report_path = f'llama-outputs/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot'\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'llama-outputs/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-{shot_n}-shot/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'])\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f407b76-fc56-4ff8-a22b-b619f1a54bc5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### zero shot temp 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6751a72f-7089-412a-ae84-536c4767f290",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T02:03:19.973144Z",
     "iopub.status.busy": "2024-09-10T02:03:19.972306Z",
     "iopub.status.idle": "2024-09-10T02:32:16.748417Z",
     "shell.execute_reply": "2024-09-10T02:32:16.746327Z",
     "shell.execute_reply.started": "2024-09-10T02:03:19.973079Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "run_annotation_processed_examples_from_list(annotated_dataset,system_ideas_full_in_order_zero_shot,test_ids_no_short,'full','ideas',0)\n",
    "# llama_config['temperature'] = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0cf378d-970d-4669-8ee7-5a9494041580",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T02:32:16.757923Z",
     "iopub.status.busy": "2024-09-10T02:32:16.756608Z",
     "iopub.status.idle": "2024-09-10T02:32:53.694234Z",
     "shell.execute_reply": "2024-09-10T02:32:53.692980Z",
     "shell.execute_reply.started": "2024-09-10T02:32:16.757886Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "def make_tarfile(output_filename, source_dir):\n",
    "    with tarfile.open(output_filename, \"w:gz\") as tar:\n",
    "        tar.add(source_dir, arcname=os.path.basename(source_dir))\n",
    "make_tarfile('llama_outputs.tar.gz','llama-outputs')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf6233a-cd60-4094-abfd-0b8c080865cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-26T15:04:45.727520Z",
     "iopub.status.busy": "2024-08-26T15:04:45.725971Z",
     "iopub.status.idle": "2024-08-26T15:04:45.732644Z",
     "shell.execute_reply": "2024-08-26T15:04:45.731899Z",
     "shell.execute_reply.started": "2024-08-26T15:04:45.727477Z"
    },
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Experiment 4 - Overall Organization - Few-shot with detailing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55f6be2-459c-414e-9c4d-0aa1e5b12d7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T02:32:53.696861Z",
     "iopub.status.busy": "2024-09-10T02:32:53.696362Z",
     "iopub.status.idle": "2024-09-10T02:32:53.899329Z",
     "shell.execute_reply": "2024-09-10T02:32:53.898319Z",
     "shell.execute_reply.started": "2024-09-10T02:32:53.696815Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "df_data_info = pd.read_csv('test_data_info.csv')\n",
    "df_no_short_data_info = pd.read_csv('annotations_medical_specialist_pre_processed_no_short.csv')\n",
    "df_medical_specialist_pre_processed = pd.read_csv('annotations_medical_specialist_pre_processed.csv')\n",
    "def minimize_labels(label):\n",
    "    n_label = []\n",
    "    if type(label) == str:\n",
    "        label = eval(label)\n",
    "    for l in label:\n",
    "        if l[4] != None:\n",
    "            n_label.append([l[0],list(l[4].keys())])\n",
    "        else:\n",
    "            n_label.append([l[0],l[4]])\n",
    "    return n_label\n",
    "def get_only_text(label):\n",
    "    n_label = []\n",
    "    if type(label) == str:\n",
    "        label = eval(label)\n",
    "    for l in label:\n",
    "        n_label.append([l[0]])\n",
    "    return n_label\n",
    "\n",
    "def bio_to_cluster_annotation (bio_annotation):\n",
    "    main_annotation = []\n",
    "    for i in range(len(bio_annotation)):\n",
    "        list_ann = eval(bio_annotation[i])\n",
    "        \n",
    "        sub_annotation = []\n",
    "        phrase = []\n",
    "        for z in range(len(list_ann)):\n",
    "            if list_ann[z][3] == 'B':\n",
    "                if len(phrase) > 0:\n",
    "                    sub_annotation.append([' '.join(phrase),list(list_ann[z][4].keys())])\n",
    "                    phrase = []\n",
    "                phrase.append(list_ann[z][0])\n",
    "            elif list_ann[z][3] == 'I':\n",
    "                phrase.append(list_ann[z][0])\n",
    "        main_annotation.append(sub_annotation)\n",
    "    return main_annotation\n",
    "\n",
    "def extract_example_shot_from_row(_row, output='string'):\n",
    "    if output == 'list':\n",
    "        shot_text = {'user_input':_row['text'],'assistant_output':_row['cluster_labels']}\n",
    "    elif output == 'string':\n",
    "        shot_text = f\"\"\"'user_input':{_row['text']}\\n 'assistant_output':\"annotations\":{_row['cluster_labels']}\"\"\"\n",
    "    return shot_text\n",
    "\n",
    "## Transform list of annotated tokens into continuous string for TF-IDF \n",
    "def annotation_token_to_text(df_text):\n",
    "    annotation_token_to_text = ''\n",
    "    # for id,row in df_text.iterrows():\n",
    "        # print(row['cluster_labels'])\n",
    "        # print(eval(row['cluster_labels'])[0])\n",
    "        # print('============list_row==')\n",
    "    list_row = eval(df_text['cluster_labels'])\n",
    "    for i in range(len(list_row)):\n",
    "        annotation_token_to_text += list_row[i][0] + ' '\n",
    "    return annotation_token_to_text\n",
    "\n",
    "## Retrieving similar examples from complete text (TF-IDF)  ##\n",
    "def find_top_matches_from_annotation(query, df, top_n):\n",
    "    # for i in range(len(df['cluster_labels'])):\n",
    "    #     df['cluster_labels'][i] = minimize_labels(df['cluster_labels'][i])\n",
    "\n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "\n",
    "    # Fit and transform the text data in the dataframe\n",
    "    tfidf_matrix = vectorizer.fit_transform(df['cluster_labels'])\n",
    "\n",
    "    # Transform the query string\n",
    "    query_vector = vectorizer.transform([query])\n",
    "\n",
    "    # Calculate cosine similarity between the query vector and all documents\n",
    "    similarity_scores = cosine_similarity(query_vector, tfidf_matrix)\n",
    "\n",
    "    # Get the indices of the top 3 matches\n",
    "    top_indices = similarity_scores.argsort()[0][-1*top_n:][::-1]\n",
    "    # Get the corresponding documents from the dataframe\n",
    "    top_matches = df.iloc[top_indices]\n",
    "    # print(type(top_matches))\n",
    "    \n",
    "    return top_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf79f6b9-f873-4746-a440-4c7a2570e19f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T02:32:53.901224Z",
     "iopub.status.busy": "2024-09-10T02:32:53.900871Z",
     "iopub.status.idle": "2024-09-10T02:32:53.923929Z",
     "shell.execute_reply": "2024-09-10T02:32:53.922789Z",
     "shell.execute_reply.started": "2024-09-10T02:32:53.901185Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_annotations_tf_idf_custom_shots (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/full-dataset/overall-organization/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/ideas-tf-idf-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            df_top_shots = find_top_matches_from_annotation(requested_doc[\"text\"],df_data_info[df_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "            shots_text = ''\n",
    "            for j in range(shot_n):\n",
    "                top_shot = df_top_shots.iloc[j]\n",
    "                # print('top_shot',top_shot)\n",
    "                txt = extract_example_shot_from_row(top_shot)\n",
    "                shots_text += '\\n'+txt\n",
    "            # print('bundle of shots', shots_text)\n",
    "            replace_sys_instruction = system_ideas_full_in_order_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "            # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "            input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "            report_path = path\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}', end=\"\\r\")\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'], end=\"\\r\")\n",
    "        i += 1\n",
    "\n",
    "def predict_annotations_tf_idf_custom_shots_from_preprocessed (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "    path = f'llama-outputs/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/tf-idf-custom/ideas-tf-idf-{shot_n}-shot'\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    file_names = os.listdir(path)\n",
    "    i = 1\n",
    "    for _id in list_id:\n",
    "        # print(_id)\n",
    "        requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "        if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "            df_top_shots = find_top_matches_from_annotation(requested_doc[\"text\"],df_no_short_data_info[df_no_short_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "            shots_text = ''\n",
    "            for j in range(shot_n):\n",
    "                top_shot = df_top_shots.iloc[j]\n",
    "                # print('top_shot',top_shot)\n",
    "                txt = extract_example_shot_from_row(top_shot)\n",
    "                shots_text += '\\n'+txt\n",
    "            # print('bundle of shots', shots_text)\n",
    "            replace_sys_instruction = system_ideas_full_in_order_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "            # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "            input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "            # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "            output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "            report_path = path\n",
    "            if not os.path.exists(report_path):\n",
    "                os.makedirs(report_path)\n",
    "            with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "                json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "            print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}', end=\"\\r\")\n",
    "        else:\n",
    "            print('Already exists',requested_doc['doc_id'], end=\"\\r\")\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9931590-50f0-4f15-981f-481cde1bde61",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1-Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af09e4e3-3ffa-41c8-bf87-e0ceb566259f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:09:03.992476Z",
     "iopub.status.busy": "2024-09-10T14:09:03.990383Z",
     "iopub.status.idle": "2024-09-10T14:09:55.853459Z",
     "shell.execute_reply": "2024-09-10T14:09:55.851808Z",
     "shell.execute_reply.started": "2024-09-10T14:09:03.992433Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdfdbc2-7232-413b-96e7-75c281e42e5c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:09:55.856922Z",
     "iopub.status.busy": "2024-09-10T14:09:55.856340Z",
     "iopub.status.idle": "2024-09-10T14:38:21.218789Z",
     "shell.execute_reply": "2024-09-10T14:38:21.211858Z",
     "shell.execute_reply.started": "2024-09-10T14:09:55.856873Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c052d75-7231-4fc1-9d6f-af2eadb0a4ae",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d919518e-9640-4ef9-b2d4-4241f58be8ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T14:38:21.221014Z",
     "iopub.status.busy": "2024-09-10T14:38:21.220274Z",
     "iopub.status.idle": "2024-09-10T15:06:49.863030Z",
     "shell.execute_reply": "2024-09-10T15:06:49.861722Z",
     "shell.execute_reply.started": "2024-09-10T14:38:21.220965Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c9569-5612-469a-819d-130090cfd01a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T15:06:49.866425Z",
     "iopub.status.busy": "2024-09-10T15:06:49.866122Z",
     "iopub.status.idle": "2024-09-10T15:35:45.172045Z",
     "shell.execute_reply": "2024-09-10T15:35:45.170855Z",
     "shell.execute_reply.started": "2024-09-10T15:06:49.866397Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2626c4-2079-4580-93cc-0ef99f43d3db",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 3-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4607c58-772c-44d8-927c-766174e70c8a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T15:35:45.174559Z",
     "iopub.status.busy": "2024-09-10T15:35:45.173828Z",
     "iopub.status.idle": "2024-09-10T16:05:04.075986Z",
     "shell.execute_reply": "2024-09-10T16:05:04.071333Z",
     "shell.execute_reply.started": "2024-09-10T15:35:45.174351Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9add6f01-0b1d-4ba6-8e76-81e395b5eaec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T16:05:04.082967Z",
     "iopub.status.busy": "2024-09-10T16:05:04.079959Z",
     "iopub.status.idle": "2024-09-10T16:34:33.539905Z",
     "shell.execute_reply": "2024-09-10T16:34:33.536629Z",
     "shell.execute_reply.started": "2024-09-10T16:05:04.082911Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3221dbaa-0478-4e71-9208-48caf5d5df0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 4-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ccadb2-34f4-454c-95ca-53e053a18927",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 10-shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e086d742-af3f-45b1-a25b-68ac54f8dde2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:17:50.052136Z",
     "iopub.status.busy": "2024-09-12T19:17:50.051664Z",
     "iopub.status.idle": "2024-09-12T19:50:57.672632Z",
     "shell.execute_reply": "2024-09-12T19:50:57.664100Z",
     "shell.execute_reply.started": "2024-09-12T19:17:50.052099Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.0\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a86e4b-1ca8-4d68-ae99-b36ce138ba2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-12T19:50:57.680665Z",
     "iopub.status.busy": "2024-09-12T19:50:57.679556Z",
     "iopub.status.idle": "2024-09-12T20:24:35.808825Z",
     "shell.execute_reply": "2024-09-12T20:24:35.807581Z",
     "shell.execute_reply.started": "2024-09-12T19:50:57.680616Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "llama_config['temperature'] = 0.9\n",
    "predict_annotations_tf_idf_custom_shots_from_preprocessed(annotated_dataset,test_ids_no_short,'full','ideas',10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeec7fdd-6c97-4925-a6ae-f8c83cc2016e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Score Assessment by annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4eab3d-ffe7-49ef-8ef8-e35e701e82fb",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-09T20:04:43.221035Z",
     "iopub.status.idle": "2024-09-09T20:04:43.221989Z",
     "shell.execute_reply": "2024-09-09T20:04:43.221773Z",
     "shell.execute_reply.started": "2024-09-09T20:04:43.221741Z"
    }
   },
   "outputs": [],
   "source": [
    "# def predict_score_by_annotation_custom_shot (full_dataset,list_id,guided_lvl,prompt_category,shot_n):\n",
    "#     path = f'llama-outputs/full-dataset/no-short-data/temp-{str(llama_config[\"temperature\"])}/top-p-{str(llama_config[\"top_p\"])}/tf-idf-custom/scoring-tf-idf-{shot_n}-shot'\n",
    "#     if not os.path.exists(path):\n",
    "#         os.makedirs(path)\n",
    "#     file_names = os.listdir(path)\n",
    "#     i = 1\n",
    "#     for _id in list_id:\n",
    "#         # print(_id)\n",
    "#         requested_doc = get_from_annotated_dataset(full_dataset,_id)\n",
    "#         if f'{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json' not in file_names:\n",
    "#             df_top_shots = find_top_matches_from_annotation(requested_doc[\"text\"],df_no_short_data_info[df_no_short_data_info.loc[:, 'text'] != requested_doc[\"text\"]],shot_n)\n",
    "            \n",
    "#             shots_text = ''\n",
    "#             for j in range(shot_n):\n",
    "#                 top_shot = df_top_shots.iloc[j]\n",
    "#                 # print('top_shot',top_shot)\n",
    "#                 txt = extract_example_shot_from_row(top_shot)\n",
    "#                 shots_text += '\\n'+txt\n",
    "#             # print('bundle of shots', shots_text)\n",
    "#             replace_sys_instruction = system_ideas_full_in_order_shot_dynamic.replace(\"{{shot}}\", shots_text)\n",
    "#             # print('SYSTEM PROMPT ===========\\n',replace_sys_instruction,'#################### END SYSTEM PROMPT #####################')\n",
    "#             input_prompt = set_custom_prompt(replace_sys_instruction,requested_doc['text'])\n",
    "#             # prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "            \n",
    "#             output_batch = prompt_routine(input_prompt, 1,llama_config['top_p'],llama_config['temperature'],llama_config['top_k'],1000)\n",
    "#             dict_to_json = {\"doc_id\": requested_doc['doc_id'],\"text\": requested_doc['text'],\"input\":output_batch[0]['input'], \"response\": output_batch[0]['generated_text']}\n",
    "#             report_path = path\n",
    "#             if not os.path.exists(report_path):\n",
    "#                 os.makedirs(report_path)\n",
    "#             with open(f'{report_path}/{prompt_category}-{guided_lvl}-{requested_doc[\"doc_id\"]}.json', 'w') as file:\n",
    "#                 json.dump(dict_to_json,file,ensure_ascii=False)\n",
    "#             print('Complete',requested_doc['doc_id'],f'{i}/{len(list_id)}')\n",
    "#         else:\n",
    "#             print('Already exists',requested_doc['doc_id'])\n",
    "#         i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e3842d-8632-4c10-b02c-8a46bc856069",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-09-09T20:04:43.223244Z",
     "iopub.status.idle": "2024-09-09T20:04:43.224268Z",
     "shell.execute_reply": "2024-09-09T20:04:43.223804Z",
     "shell.execute_reply.started": "2024-09-09T20:04:43.223778Z"
    }
   },
   "outputs": [],
   "source": [
    "# system_ideas_full_in_order_2_shot = \"\"\"You are a medical assistant with expertise for education evaluation of students.\\n Your task is to evaluate the organization medical student texts. After the text, an annotation of . All the texts will be in portuguese. ONLY use JSON as the output format, starting with 'grade'. DO NOT write, only respond in JSON format. \n",
    "# Examples of user input and assistant output:\\n \"user input\":\\n \"A Doena Pulmonar Obstrutiva Crnica  uma condio de insuficincia respiratria de padro obstrutivo, que ocorre por leso crnica do parqnuima pulmonar, a qual culmina em diminuio da complacncia pulmonar. A fisiopatogenia envolve um processo inflamatrio crnico das vias areas e do parnquima que no permite a sada de ar dos pulmes e leva ao acmulo de volume morto nos alvolos, mantendo o trax hiperinsuflado, com acmulo de CO2 e dificultando as trocas respiratrias.\\nA causa mais comum para DPOC  o tabagismo, mas outras causas incluem a convivncia com forno a lenha por longo tempo ou a deficincia gentica de alfa-1-antitripsina.\\nO principal sintoma desses pacientes  a dispneia, que se incia em grandes esforos e pode chegar at ao repouso. O diagnstico  feito pela clnica + espirometria. A principal complicao so as exacerbaes de doena que pode ser associada a quadro infeccioso sobreposto.\\nEsses pacientes podem ser divididos segundo os critrios do GOLD entre pacientes muito sintomtisoc&nbsp; e com muitas exacerbaes, e o tratamento utiliza LABA, SABA LAMA e CI a depender desses\\n', 'assistant output':\\n \"annotations\": [['tabagismo', ['epidemiology', 'etiology'], ['forno a lenha', ['epidemiology', 'etiology'], ['forno a lenha + por longo tempo', ['epidemiology'], ['deficincia gentica de alfa-1-antitripsina', ['etiology'], ['diagnstico  feito pela clnica + espirometria', ['exams', 'history'], ['dispneia', ['history'], ['dispneia + incia em grandes esforos', 693, 734, ['history'], ['dispneia + incia em grandes esforos + pode chegar at ao repouso', 693, 763, ['history'], ['exacerbaes de doena', ['history', 'pathophysiology'], ['exacerbaes de doena + quadro infeccioso', ['history', 'pathophysiology'], ['podem ser divididos segundo os critrios do GOLD',['history'], ['critrios do GOLD + muito sintomtiso + muitas exacerbaes', ['history'], ['leso crnica do parqnuima pulmonar', ['pathophysiology'], ['diminuio da complacncia pulmonar', ['pathophysiology'], ['processo inflamatrio crnico', ['pathophysiology'], ['no permite a sada de ar dos pulmes', ['pathophysiology'], ['acmulo de volume morto nos alvolos', ['pathophysiology'], ['trax hiperinsuflado', ['pathophysiology'], ['acmulo de CO2', ['pathophysiology'], ['dificultando as trocas respiratrias', ['pathophysiology'], ['insuficincia respiratria', ['pathophysiology'], ['LABA', ['therapeutic']], ['SABA', ['therapeutic']], ['LAMA', ['therapeutic']], ['CI', ['therapeutic']]]\\n\"user input\":\\n \"DPOC  uma doena que costuma ocorrer em idosos e muito associada ao tabagismo e inalao de demais partculas txicas, com alta prevalncia.\\n caracterizada por enfisema e bronquite, havendo tanto o padro clssico do paciente soprador rosado (magro, avermelhado, predomina enfisema) quanto do tossidor azul (ciantico, sobrepeso, predomina bronquite).\\nComo sintomas clssicos, a DPOC tem como sintomas tosse expectorante crnica, dispneia, infeces de repetio, edema. No exame fsico, nota-se timpanismo, trax aumentado em volume, respirao no enche plenamente a caixa torcica, por vezes uso de musculatura acessria, rudos adventcios.\\n\",\"assistant output\":\\n\"annotations\":[['idosos', ['epidemiology']], ['muito associada ao tabagismo', ['epidemiology']], ['inalao de demais partculas txicas', ['epidemiology', 'etiology']], ['alta prevalncia', ['epidemiology']], ['enfisema', ['physical', 'pathophysiology']], ['bronquite', ['physical', 'pathophysiology']], ['soprador rosado', ['pathophysiology', 'physical']], ['magro', ['physical']], ['avermelhado', ['physical']], ['predomina enfisema', ['pathophysiology']], ['tossidor azul', ['pathophysiology', 'physical']], ['ciantico', ['physical']], ['sobrepeso', ['physical']], ['predomina bronquite', ['pathophysiology']], ['tosse expectorante crnica', ['history']], ['dispneia', ['history']], ['infeces de repetio', ['history']], ['edema', ['physical']], ['timpanismo', ['physical']], ['trax aumentado em volume', ['physical']], ['respirao no enche plenamente a caixa torcica', ['uso de musculatura acessria', ['physical']],['rudos adventcios', ['physical']]]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59a845-3029-45a8-b271-6a55dae88613",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Delete endpoint (stop billing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d2f16d-7638-4807-9b60-68f511efd03f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-13T16:10:28.073381Z",
     "iopub.status.busy": "2024-09-13T16:10:28.072208Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify your AWS Region\n",
    "aws_region='us-east-1'\n",
    "\n",
    "# Create a low-level SageMaker service client.\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=aws_region)\n",
    "\n",
    "# Delete endpoint\n",
    "sagemaker_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sagemaker_client.delete_endpoint_config(EndpointConfigName=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18769f0-048c-4402-8e48-7c56d17314c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
