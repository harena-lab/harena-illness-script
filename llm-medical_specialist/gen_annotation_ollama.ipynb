{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8268f31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ollama\n",
    "import ollama\n",
    "import json\n",
    "from typing import Dict, List\n",
    "\n",
    "# --- Step 1: Control parameters and generate a response ---\n",
    "# The `generate` function sends a request to the Ollama server.\n",
    "# You can control model parameters through the `options` dictionary.\n",
    "# A full list of parameters can be found in the Ollama documentation.\n",
    "\n",
    "# Define the model and prompt\n",
    "model_name = 'llama3.1'\n",
    "\n",
    "# Format prompt for Llama3.1\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format messages for Llama-3 chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    prompt: List[str] = []\n",
    "    # print(messages[0]['role'])\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        content = \"\".join([\"<|start_header_id|>system<|end_header_id|>\\n\\n\", messages[0][\"content\"], \"<|eot_id|>\", \"<|start_header_id|>user<|end_header_id|>\\n\\n\",messages[1][\"content\"],\"<|eot_id|>\"])\n",
    "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
    "\n",
    "    for user, answer in zip(messages[::2], messages[1::2]):\n",
    "        prompt.extend([\"<|start_header_id|>user<|end_header_id|>\", \"\\n\\n\", (user[\"content\"]).strip(), \" <|eot_id|>\"])\n",
    "\n",
    "    prompt.extend([\"<|begin_of_text|>\", (messages[0][\"content\"]).strip(), \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"])\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "# Define the generation parameters\n",
    "params = {\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"top_k\": 50,\n",
    "    \"seed\": 7103,\n",
    "    \"max_new_tokens\": 8192\n",
    "}\n",
    "\n",
    "# Generate the response\n",
    "print(f\"Generating a response with the following parameters: {params}\\n\")\n",
    "response = ollama.generate(\n",
    "    model=model_name,\n",
    "    prompt=prompt_text,\n",
    "    options=params,\n",
    "    stream=False, # Set to True for streaming responses\n",
    "    format=\"json\" # Forces the response to be in JSON format\n",
    ")\n",
    "\n",
    "# The generated text is in the 'response' key\n",
    "generated_text = response['response']\n",
    "print(f\"Generated Story:\\n{generated_text}\\n\")\n",
    "\n",
    "\n",
    "# --- Step 2: Save the answer to a file ---\n",
    "# You can save the response and other metadata to a file,\n",
    "# for example, a JSON file to keep the data structured.\n",
    "\n",
    "# Create a dictionary to hold the data\n",
    "data_to_save = {\n",
    "    'model': model_name,\n",
    "    'prompt': prompt_text,\n",
    "    'parameters': params,\n",
    "    'response_text': generated_text\n",
    "}\n",
    "\n",
    "# Define a filename\n",
    "filename = 'ollama_story_output.json'\n",
    "\n",
    "# Save the data to the JSON file\n",
    "with open(filename, 'w') as f:\n",
    "    json.dump(data_to_save, f, indent=4)\n",
    "\n",
    "print(f\"Response and parameters saved to '{filename}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a807c27",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
